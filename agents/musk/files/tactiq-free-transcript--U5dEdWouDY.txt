# tactiq.io free youtube transcript
# MIND BLOWING WORK ETHIC - Elon Musk Motivational Video
# https://www.youtube.com/watch/-U5dEdWouDY

00:00:00.080 no one should put this many hours into
00:00:04.080 work this is not good
00:00:08.800 people should not work this hard i'm not
00:00:10.719 they should not do this this is
00:00:12.400 very painful painful in what sense
00:00:15.839 it's because my earth's my brain and my
00:00:18.400 heart
00:00:21.920 particularly if you're starting a
00:00:22.960 company you need to work super hard
00:00:25.119 so what what does super heart mean well
00:00:28.560 when my brother and i were starting our
00:00:30.160 first company
00:00:31.679 instead of getting an apartment we just
00:00:33.600 rented us a small office
00:00:35.200 and we slept on the couch and we
00:00:38.239 we showered at the ymca and uh we're so
00:00:41.840 hot up we had just
00:00:43.040 one computer so the the website was up
00:00:46.239 during the day
00:00:47.600 and i was coding at night seven days a
00:00:50.079 week all the time
00:00:52.640 and i i briefly had a girlfriend that
00:00:55.760 period and in order to
00:00:57.600 be with me she has to sleep in the
00:00:58.719 office so
00:01:00.719 i work hard like
00:01:04.559 i mean every waking hour that's that's
00:01:06.640 the the thing i would i would say if you
00:01:08.880 particularly if you're starting a
00:01:09.840 company um
00:01:12.240 and i mean if you do simple math say
00:01:13.840 like okay if somebody else is working
00:01:15.520 50 hours and you're working 100 you'll
00:01:18.000 get twice as done
00:01:18.880 as much done in the course of a year as
00:01:21.200 the other company
00:01:22.159 just work like hell i mean you just have
00:01:24.159 to put in
00:01:25.600 you know 80 hour 80 to 100 hour weeks
00:01:28.960 every week
00:01:30.720 and then a lot of work that all those
00:01:32.560 things improve the odds of success
00:01:34.799 um i mean if if other people are putting
00:01:37.840 in 40 hour work weeks and you're putting
00:01:39.680 in
00:01:40.079 100 hour work weeks then even if
00:01:44.000 you're doing the same thing you know
00:01:45.680 that in in one year you will achieve
00:01:48.079 what they achieve
00:01:49.280 you will achieve in four months what it
00:01:52.320 takes them
00:01:53.200 a year to achieve what was your biggest
00:01:55.840 failure
00:01:56.640 and how did it change you we almost did
00:01:58.719 diet spacex actually so
00:02:00.240 we i'd budgeted for or three flights
00:02:04.399 i mean technically i didn't have a plan
00:02:05.920 where i had
00:02:07.520 had the money from paypal i had like 180
00:02:09.280 million from paypal
00:02:10.639 i thought you know i'll i'll allocate
00:02:14.239 half of that to spacex and tesla and
00:02:16.560 solarcity
00:02:17.680 and um that should be fine i'll have 90
00:02:20.080 million likes just
00:02:20.879 lots you know uh but
00:02:23.920 but then what happened is um things cost
00:02:26.480 more and took longer than
00:02:27.760 than i thought so i had a choice of
00:02:30.400 either
00:02:30.959 put the rest of the money in or the
00:02:32.959 companies are going to die
00:02:35.599 and it's like so i ended up putting all
00:02:38.080 the money in and
00:02:39.360 borrowing money for rent from france um
00:02:43.760 2008 was brutal
00:02:45.900 [Music]
00:02:47.440 yeah 2008 we had the third consecutive
00:02:49.920 failure
00:02:50.720 of the falcon rocket for spacex um
00:02:54.879 tesla almost went bankrupt we closed our
00:02:58.239 financing round
00:03:00.080 6 pm christmas eve 2008. it was the last
00:03:03.040 hour of the last day that it was
00:03:04.319 possible
00:03:04.879 we would have gone bankrupt two days
00:03:06.080 after christmas otherwise spacex is
00:03:08.159 alive by the
00:03:09.280 skin of its teeth so is tesla um
00:03:13.599 if things just gone a little bit the
00:03:15.440 other way both companies would be dead
00:03:17.519 and i had like one of the most difficult
00:03:19.040 choices i ever faced
00:03:21.040 in life was was in 2008
00:03:26.000 and i think i had
00:03:30.480 like maybe 30 million dollars left
00:03:33.519 or 30 or 40 million left in 2008 i had
00:03:35.599 two choices
00:03:37.360 i could put it all into one company
00:03:40.480 and then the other company would
00:03:41.599 definitely die um or split it
00:03:44.799 between the two companies and but if i
00:03:47.120 split up between two companies then both
00:03:48.720 might die
00:03:50.400 um and you know when you put your
00:03:53.680 blood sweat and tears into creating
00:03:55.360 something you're building something it's
00:03:57.280 like a child
00:04:00.799 and so it's like which one am i gonna
00:04:03.599 let one starve to death
00:04:05.840 i can bring myself to do it so i put i
00:04:08.400 split the money between the two
00:04:10.640 fortunately thank goodness uh they both
00:04:12.480 came through tesla really faced the
00:04:14.319 severe
00:04:15.360 uh threat threat of death due to the
00:04:18.000 model 3 production ramp
00:04:20.160 essentially the company was bleeding
00:04:21.839 money like crazy and and just
00:04:24.400 if if we didn't solve these problems in
00:04:25.919 a very short period of time uh we would
00:04:27.759 die
00:04:28.320 uh and was extremely difficult to solve
00:04:30.800 them how close to death did you come
00:04:33.040 we're within single budget weeks
00:04:36.800 22 hours a day or like what how many
00:04:38.160 hours working yeah so seven days a week
00:04:40.320 sleeping in the factory uh i worked
00:04:42.479 everywhere from the i worked in the oaks
00:04:43.759 in the paint shop
00:04:44.800 general assembly body shop you ever
00:04:46.880 worry about yourself imploding like just
00:04:48.800 too much absolutely i think failure is
00:04:51.520 bad
00:04:52.160 um i don't think it's good but
00:04:55.280 if if something's important enough then
00:04:57.600 you you do it even though the risk of
00:04:59.280 failure is high
00:05:01.120 were you a little naive when you thought
00:05:02.720 i'll just i can easily build
00:05:04.240 build an electric car and a rocket i
00:05:07.199 didn't think it would be easy
00:05:08.840 um like i said i thought they would
00:05:11.680 probably fail
00:05:13.280 um but you know like creating a company
00:05:16.400 is almost like having a child
00:05:17.919 so it's sort of like how do you say your
00:05:20.560 child should not have food
00:05:23.280 so one once you have the company you
00:05:25.039 have to feed it and
00:05:26.479 announce it and take care of it
00:05:30.080 even if it it ruins you yeah
00:05:34.080 [Music]
00:05:37.600 but uh supposing there wasn't tough
00:05:39.520 times in
00:05:42.120 2008 end of 2008
00:05:46.160 how did you get through that period of
00:05:48.160 crisis
00:05:52.960 can we just break for a second
00:05:57.360 you wanna wait a little while
00:06:02.720 yeah sure if it was worth it let me sure
00:06:05.919 hope it was worth it
00:06:07.600 well there's a ton of failures along the
00:06:08.960 way that's for sure except for
00:06:10.800 as i said for spacex the first three
00:06:13.360 launches failed
00:06:15.840 and uh we were just barely able to
00:06:18.960 scrape
00:06:19.520 together enough parts and money to do
00:06:22.319 the
00:06:22.800 the fourth launch that fourth launch had
00:06:24.800 failed we would have been dead
00:06:26.479 so multiple failures along the way um
00:06:30.639 i tried very hard to to get the right
00:06:32.479 expertise in for for spacex
00:06:35.120 i tried hard to to find a great chief
00:06:37.520 engineer for the rocket
00:06:38.800 but the good chief engineers wouldn't
00:06:41.120 join
00:06:42.000 and the bad ones well there was no point
00:06:43.759 in hiring them so i ended up being chief
00:06:45.039 engineer of the rocket
00:06:47.120 so if i could have found somebody better
00:06:48.960 than we would have maybe had
00:06:50.400 less than three failures when you had
00:06:53.759 that third failure in a row
00:06:56.720 did you think i need to pack this in
00:06:59.520 never
00:07:01.360 why not i don't ever give up
00:07:04.639 i mean i'd have to be dead or completely
00:07:07.919 incapacitated
00:07:09.840 you know there are american heroes who
00:07:12.479 don't like this idea
00:07:14.400 neil armstrong gene cernan have both
00:07:17.120 testified
00:07:18.160 against commercial space flight and the
00:07:20.240 way that you're developing it
00:07:21.759 and i wonder what you think of that
00:07:24.800 i was very sad to see that because those
00:07:27.360 guys are
00:07:30.720 you know those guys are heroes of mine
00:07:32.160 so it's really tough
00:07:33.919 you know i i wish they would come and
00:07:35.280 visit and and see the hardware that
00:07:36.880 we're doing here
00:07:38.639 and i think that would change their mind
00:07:41.440 they inspired you to do this didn't they
00:07:43.759 yes and to see them casting stones in
00:07:48.400 your direction
00:07:51.130 [Music]
00:07:52.879 it's difficult did you expect them to
00:07:55.840 cheer you on
00:07:58.879 so they're hoping they would something
00:08:00.960 that can be helpful is fatalism
00:08:02.560 uh to some degree um if you just if you
00:08:04.960 just accept the probabilities
00:08:06.639 um then that diminishes fear
00:08:10.000 uh so um starting spacex i thought the
00:08:13.759 odds of success were less than 10
00:08:16.560 um and i just accepted that actually
00:08:19.440 probably
00:08:20.080 i would just lose lose everything um
00:08:24.000 but that maybe would make some progress
00:08:26.160 if we could just move the ball forward
00:08:27.919 even if we died maybe some other company
00:08:30.800 could pick up the baton and move
00:08:32.320 and keep moving it forward um so that
00:08:34.640 we'll still do some good
00:08:37.120 um yeah same with tesla i thought the
00:08:39.599 odds of a car company succeeding were
00:08:41.519 extremely low in creating these
00:08:43.360 companies we thought that we would be
00:08:45.360 successful
00:08:46.880 i thought that the most likely outcome
00:08:48.240 was failure
00:08:50.000 but but it was still worth doing even
00:08:52.640 though the
00:08:53.279 odds of success were low in fact even
00:08:55.440 for for sport spacex
00:08:57.279 the originally what i started doing was
00:08:59.440 not creating a rocket company but
00:09:01.279 but actually was going to do a small
00:09:04.480 mission to mars which was just a
00:09:05.760 philanthropic mission where you
00:09:07.440 would send a small greenhouse with seeds
00:09:09.680 and dehydrated gel
00:09:10.720 in the wood upon landing hydrate the gel
00:09:13.839 and you'd have this cool picture of
00:09:16.240 green plants on a red background
00:09:18.080 and the public tends to respond to
00:09:19.519 precedence and superlatives so this will
00:09:20.880 be the first life on mars
00:09:22.080 furthest the life's ever traveled and
00:09:24.320 you'd have this great money shot of
00:09:25.600 green plants on a red background
00:09:27.600 so um i thought that would get people's
00:09:30.720 attention so
00:09:32.080 um but but the expectation for that was
00:09:34.959 was no return so
00:09:36.240 i thought we wouldn't get any uh you
00:09:38.959 know just
00:09:39.600 spend the money on that and it wouldn't
00:09:41.440 wouldn't happen
00:09:43.440 if you're creating a company or if
00:09:44.800 you're joining company
00:09:46.800 the most important thing is to attract
00:09:50.240 is to
00:09:50.720 attract great people so either you would
00:09:52.480 join a group that's amazing that you
00:09:54.399 really respect or if you're building a
00:09:57.040 company you've got to gather great
00:09:58.080 people i mean all the company is
00:09:59.600 is a group of people that have gathered
00:10:01.279 together to create a product or service
00:10:03.600 and so depending upon how talented and
00:10:05.600 hard-working that group is
00:10:07.200 and degree to which they are focused uh
00:10:09.279 cohesively
00:10:10.399 in a good direction that will determine
00:10:12.640 the success of the company
00:10:14.320 so do everything you can to to gather
00:10:16.800 great people uh if you're creating a
00:10:19.200 company
00:10:21.360 then i'd say focus on on signal over
00:10:24.720 noise a lot of companies get get
00:10:27.600 confused
00:10:28.320 they spend money on things that don't
00:10:30.560 actually make the product better
00:10:32.880 so for example at tesla
00:10:36.480 we've we've never spent any money on
00:10:38.160 advertising
00:10:39.680 we put all the money into r d
00:10:42.800 and manufacturing and design to try to
00:10:45.200 make the car as good as possible
00:10:48.640 and i think that's that's that's the way
00:10:51.680 to go
00:10:52.240 so if for any given company just
00:10:56.000 can keep thinking about are these
00:10:58.720 efforts that
00:10:59.600 people are expending are they resulting
00:11:01.680 in a better product or service
00:11:03.440 and if they're not stop those efforts
00:11:05.920 starting a business
00:11:06.800 i'd say number one is have a high paying
00:11:09.360 threshold
00:11:10.880 that's there's a friend of mine who's
00:11:13.519 got a good saying which is that
00:11:14.959 starting a company is like eating glass
00:11:16.640 and stirring into the abyss
00:11:18.240 okay that's um that's generally what
00:11:20.800 happens
00:11:21.600 because um when you first start a
00:11:24.160 company there's lots of optimism
00:11:25.760 and things that things are great and
00:11:27.040 then so happiness at first is high
00:11:29.920 then you encounter all sorts of issues
00:11:32.160 uh and happiness will steadily decline
00:11:34.959 and then you'll go through a whole world
00:11:36.320 of hurt
00:11:38.160 that's and then eventually you'll if you
00:11:41.120 succeed
00:11:41.760 and in most cases you will not succeed
00:11:44.480 um
00:11:45.839 and tesla almost didn't succeed came
00:11:48.320 very close to failure
00:11:49.440 um then if you succeed then
00:11:52.800 after a long time you will finally get
00:11:55.040 back to happiness
00:11:56.079 you've got to make sure that
00:11:59.360 that you that whatever you're doing is a
00:12:01.600 great product or service it has to be
00:12:04.000 really great and i go back to what i was
00:12:05.440 saying earlier where
00:12:08.240 if you're a new company i mean unless
00:12:10.720 it's like some new industry or
00:12:12.639 or new market that if it's an untapped
00:12:15.200 market or
00:12:16.000 then then uh
00:12:19.120 you have more ability to you there's
00:12:21.839 this
00:12:23.360 the standard is lower for your product
00:12:25.120 service but if you're entering anything
00:12:26.720 where there's
00:12:27.760 an existing marketplace against large
00:12:30.160 entrenched competitors
00:12:32.000 then your product or service needs to be
00:12:34.480 much better than theirs it can't be a
00:12:36.320 little bit better because then you put
00:12:37.680 yourself
00:12:38.000 in the shoes of the consumer and they
00:12:40.240 say why would you buy it as a consumer
00:12:41.920 you're always going to buy the trusted
00:12:43.200 brand unless there's a big difference
00:12:45.680 so a lot of times uh you know
00:12:49.279 entrepreneur will come up with something
00:12:51.120 which is only slightly better
00:12:53.440 and it's it's not it can't just be
00:12:55.040 slightly better it's got to be a lot
00:12:56.320 better
00:12:57.279 a well thought out critique of whatever
00:12:59.920 you're doing
00:13:00.639 is as valuable as gold
00:13:02.590 [Music]
00:13:04.320 and you should seek that from
00:13:07.519 everyone you can but particularly your
00:13:09.120 friends um
00:13:11.440 usually your friends know what's wrong
00:13:14.720 but they don't want to tell you because
00:13:16.079 they don't want to hurt you it doesn't
00:13:17.680 mean your friends are right
00:13:19.279 but very often they are right
00:13:22.320 and you at least want to listen very
00:13:23.600 carefully to what they say and to
00:13:25.279 everyone
00:13:25.920 if you're looking for basically
00:13:29.120 [Music]
00:13:31.600 you should take the approach that
00:13:34.800 that you're wrong you know that
00:13:38.480 that you the entrepreneur are wrong your
00:13:41.040 goal is to be less wrong
00:13:42.959 advice i'd give to people starting
00:13:44.720 company to entrepreneurs in general is
00:13:47.600 really focus on making a product that
00:13:49.279 your customers love
00:13:51.680 and it's so rare that you can buy a
00:13:53.519 product and and you love the product
00:13:55.440 when you bought it this is this is there
00:13:57.920 are very few
00:13:59.360 uh things that fit into that category
00:14:01.120 and if you if you can come up with
00:14:02.240 something like that
00:14:03.199 your business will be successful for
00:14:04.639 sure i think uh really
00:14:06.880 an obsessive uh nature with respect to
00:14:10.560 the quality of the product
00:14:13.440 it is very important uh yeah so you know
00:14:15.839 being obsessive compulsive
00:14:17.040 is a good thing in this context
00:14:21.440 really really liking what you do
00:14:23.120 whatever area that you get into
00:14:25.600 um given that you know even if you're if
00:14:28.160 you're
00:14:28.959 the best the best there's always a
00:14:30.160 chance of failure so i think it's
00:14:32.000 important that you really like whatever
00:14:33.440 you're doing
00:14:34.480 if you don't like it life is too short
00:14:37.360 um
00:14:38.560 you know i'd say if and also if you
00:14:42.000 if you like what you do and you think
00:14:43.440 about it even when you're not working
00:14:45.519 i mean it'll just it's it's something
00:14:47.920 that your mind is drawn to
00:14:50.160 and and if you don't like it you just
00:14:52.720 really can't make it work i think
00:14:55.279 when i was young i i didn't really know
00:14:58.240 what i was going to do
00:14:59.360 when i got older um people kept asking
00:15:01.600 me and and um
00:15:03.199 but but then eventually i thought that
00:15:04.639 the idea of inventing things
00:15:06.240 would be would be really cool and
00:15:09.839 the reason i thought that was because i
00:15:12.880 i read a quote from
00:15:14.240 author c clock which said that a
00:15:17.120 sufficiently
00:15:18.079 advanced technology is indistinguishable
00:15:20.240 from magic
00:15:21.440 and and that's really true if you think
00:15:24.240 if you go back
00:15:25.440 say 300 years the things that we take a
00:15:29.040 sufficiently advanced technology is
00:15:31.360 indistinguishable from magic
00:15:33.360 and and that's really true uh being able
00:15:36.320 to see over long distances
00:15:38.000 being able to communicate being able to
00:15:40.800 see over long distances
00:15:42.160 being able to communicate having
00:15:44.880 effectively
00:15:47.279 with with the internet
00:15:50.639 in times past in fact i think it
00:15:53.120 actually goes beyond that because there
00:15:54.639 are many things that we take for granted
00:15:56.480 today
00:15:57.120 that weren't even imagined in times past
00:16:00.079 they weren't even in the realm of magic
00:16:02.160 so that it actually goes goes beyond
00:16:03.839 that so i thought well
00:16:06.399 you know if if i can do some of those
00:16:08.800 things basically if
00:16:10.079 i can advance technology then that
00:16:11.759 that's like magic and that would be
00:16:12.959 really cool
00:16:13.920 um and the the
00:16:17.279 i i always had sort of a slight
00:16:18.480 existential crisis because i was trying
00:16:20.000 to figure out
00:16:20.880 what does it all mean like what's the
00:16:22.160 purpose of things and
00:16:24.560 um i came to the conclusion that if if
00:16:27.199 we can advance the
00:16:29.279 this the knowledge of the world if we
00:16:30.880 can do things that expand the scope and
00:16:33.120 and scale of consciousness then we're
00:16:35.759 better able to
00:16:36.800 ask the right questions and become more
00:16:39.120 enlightened and
00:16:40.399 and that's really the only way forward
00:16:42.639 so
00:16:44.240 uh so so i i studied
00:16:48.000 physics and business because i figured
00:16:49.920 in order to do a lot of these things
00:16:51.920 you need to know how the universe works
00:16:54.399 and you need to know how
00:16:56.160 how the economy works and you also need
00:16:59.600 to be able to bring a lot of people
00:17:00.880 together to work with you to create
00:17:02.160 something because it's very difficult to
00:17:03.519 do something
00:17:04.079 as as an individual if it's if it's a
00:17:06.240 significant technology
00:17:09.039 so i i originally came out to to
00:17:12.240 california
00:17:12.959 to try to figure out how to
00:17:16.319 improve the energy density of of
00:17:21.039 of electric vehicles basically to try to
00:17:23.039 figure out if there was an advanced
00:17:24.160 capacitor that that could serve as
00:17:26.319 an alternative to batteries and
00:17:29.360 that was in 95 and
00:17:32.960 that's also when the internet started to
00:17:35.360 happen
00:17:36.080 and and i i thought well i can either
00:17:40.320 uh pursue this tech this technology
00:17:43.280 where
00:17:44.080 success maybe may not be one of the
00:17:45.919 possible outcomes
00:17:47.200 which is always tricky um or
00:17:50.799 uh participate in the internet and and
00:17:53.840 be part of it and and i think maybe it's
00:17:55.840 helpful to
00:17:57.120 say one of the things that was important
00:17:58.960 then in the creation of paypal
00:18:00.960 was was was kind of how it started
00:18:02.880 because initially
00:18:04.960 the initial thought was with paypal was
00:18:07.039 to create an agglomeration of
00:18:08.799 financial services so you have one place
00:18:11.280 where
00:18:12.160 all your financial services needs would
00:18:13.679 be seamlessly integrated
00:18:15.520 and um and and work smoothly and then we
00:18:18.320 had like a little feature which was to
00:18:19.840 do email payments
00:18:21.600 um and whenever we showed show the
00:18:24.240 system off to someone
00:18:25.520 uh we'd show the hard part which was the
00:18:27.600 um the agglomeration of financial
00:18:29.840 services which was quite difficult to
00:18:31.360 put together
00:18:32.320 nobody was interested um then we'd show
00:18:34.880 people email payments which was actually
00:18:36.400 quite easy and everybody was interested
00:18:38.559 so we focused on email payments and
00:18:40.320 really try to make that work
00:18:41.679 and and that's what really got things to
00:18:43.440 take off um
00:18:45.760 but but if we hadn't if we hadn't
00:18:47.039 responded to what people said then we
00:18:49.760 probably would not have been successful
00:18:51.840 so it's important to look for things
00:18:53.360 like that
00:18:54.240 and and focus on them when when you when
00:18:56.160 you see them and you correct
00:18:57.840 your prior assumptions going from paypal
00:19:01.039 i thought it will what what are some of
00:19:03.520 the
00:19:04.240 the other problems that are likely to
00:19:06.960 most affect the future of humanity
00:19:09.360 um it really wasn't from the perspective
00:19:11.760 of what what's the
00:19:13.120 rank ordered best way to to make money
00:19:15.520 um which which is
00:19:16.559 which is okay but um
00:19:19.679 it was really what i think is going to
00:19:21.919 most affect the future humanity so
00:19:24.240 the i think the the biggest terrestrial
00:19:27.120 problem we've got
00:19:28.240 is uh sustainable energy but the
00:19:30.720 production
00:19:31.440 and consumption of energy in a
00:19:32.640 sustainable manner if we don't solve
00:19:34.240 that this the sensory is
00:19:35.679 the century we're we're in deep trouble
00:19:38.559 um
00:19:38.960 and then the the other one being the
00:19:40.960 extension of life beyond earth to make
00:19:42.640 life multi-planetary
00:19:44.720 when i started spacex i it actually
00:19:48.080 initially i thought that well there's
00:19:51.360 there's no way one could possibly start
00:19:52.799 a rocket company
00:19:53.840 i i wasn't that crazy um but but then
00:19:58.000 uh i thought well what is a way to
00:20:01.039 um increase nasa's budget that was
00:20:03.360 actually my initial goal
00:20:05.039 so so obviously the financial outcome
00:20:07.280 from such a mission would probably be
00:20:08.559 zero
00:20:09.280 um so anything better than that was on
00:20:11.840 the upside
00:20:13.760 so i actually went to i went to russia
00:20:15.919 three times to look at buying
00:20:17.760 um a refurbished icbm
00:20:21.039 and uh i can tell you it was very weird
00:20:23.039 going there in in 2000
00:20:24.640 late 2001 2002 going to the russian
00:20:27.679 rocket forces and saying i'd like to buy
00:20:30.080 two of your biggest rockets but you can
00:20:32.720 keep the nuke
00:20:34.159 and aft after making several trips to to
00:20:36.799 russia
00:20:37.360 i came to conclusion that that actually
00:20:40.320 uh
00:20:41.039 my initial impression was was wrong
00:20:43.600 about
00:20:44.640 uh because my initial thought was well
00:20:48.000 that that there's not enough will to
00:20:50.080 explore and expand beyond
00:20:51.840 earth and have a mars base and that kind
00:20:53.440 of thing but i can't conclusion that
00:20:55.679 that was wrong
00:20:56.720 um in fact there's plenty of will
00:20:58.640 particularly in the united states
00:21:00.799 because the united states is a nation of
00:21:02.960 explorers of people who came here from
00:21:04.960 from other parts of the world i think
00:21:07.360 the united states really
00:21:08.720 a distillation of the spirit of human
00:21:11.679 exploration
00:21:13.200 so after my third trip i said okay
00:21:16.880 what we really need to do here is try to
00:21:18.480 solve the space transport problem
00:21:21.440 and uh and started spacex um
00:21:25.039 and this was against the advice of
00:21:27.039 pretty much everyone i talked to
00:21:29.520 um one friend made me sit down and watch
00:21:32.000 a bunch of videos rockets blowing up let
00:21:33.840 me tell you he wasn't far wrong
00:21:36.000 it was tough going there in the
00:21:37.039 beginning because i'd never built
00:21:38.960 anything
00:21:39.600 physical i mean i built like little
00:21:41.200 model rockets as a kid and that kind of
00:21:42.720 thing but
00:21:43.600 um i'd never had a company that built
00:21:45.760 any physical started
00:21:47.039 to figure out how to how to do all these
00:21:48.400 things and and bring together the right
00:21:50.640 team of people
00:21:51.760 we did all that and and then failed
00:21:53.520 three times um
00:21:55.360 it was tough tough going um
00:21:58.799 because thing about a rocket is that the
00:22:00.720 the passing grade is 100
00:22:02.559 you don't get to actually test the
00:22:05.039 rocket in the real environment that it's
00:22:06.720 going to be in
00:22:08.080 so i think so the best analogy for for
00:22:10.559 rocket engineering is
00:22:12.080 it's like if you want to create a really
00:22:14.320 complicated bit of software
00:22:16.320 um you could you can't run the software
00:22:18.080 as an integrated hole and you can't run
00:22:19.840 it on the computer it's intended to run
00:22:21.440 on
00:22:22.000 but the first time you put it all
00:22:23.120 together and write it on that computer
00:22:24.240 it must run with no bugs
00:22:25.600 that the first launch i was picking up
00:22:27.280 bits of rocket near the
00:22:29.200 launch site it was a bit sad we we
00:22:31.360 learned with with each successive flight
00:22:33.760 and and were able to with uh eventually
00:22:36.559 with the fourth flight in 2008
00:22:38.640 uh reached orbit and that was also with
00:22:41.840 the last bit of money that we had
00:22:43.440 so that's we got the falcon one two
00:22:45.120 orbit and then
00:22:46.480 uh began to scale that up to to the
00:22:48.400 falcon 9 which is
00:22:50.240 about an order of magnitude more a
00:22:52.159 thrust it's
00:22:53.280 around a million pounds of thrust and
00:22:56.799 we managed to get that to orbit and then
00:22:59.200 uh developed a dragon spacecraft
00:23:01.520 uh which um recently was able to
00:23:04.720 dock and return to earth from the space
00:23:06.480 station
00:23:07.840 so it's a huge relief i still can't
00:23:09.679 quite believe it actually happened
00:23:11.760 um but there's a lot more that that that
00:23:14.400 must happen beyond this
00:23:15.600 in order for humanity to be to become a
00:23:17.600 space faring civilization
00:23:18.960 ultimately a multi-planet species
00:23:22.080 um and that's something i think it's
00:23:23.679 it's it's vitally important and and i
00:23:25.520 hope um
00:23:26.720 that that some of you will will
00:23:28.080 participate in in that either at spacex
00:23:30.320 or at other companies because
00:23:31.919 it's just really one of the the most
00:23:33.760 important things for the preservation
00:23:35.200 and extension of consciousness
00:23:37.280 um it's worth noting as i'm sure people
00:23:40.240 are aware
00:23:40.799 that the earth has been around for four
00:23:43.120 billion years
00:23:44.400 and uh civilization at least in terms of
00:23:47.520 having
00:23:48.840 um writing has been around for 10 000
00:23:51.600 years and that's been generous
00:23:53.919 um so uh it's it's really
00:23:58.000 uh somewhat of a tenuous existence that
00:24:00.720 that
00:24:02.159 civilization and and consciousness as we
00:24:04.799 know it has been on earth
00:24:06.559 and i think um i'm actually i'm actually
00:24:08.880 fairly optimistic about
00:24:10.240 the future of earth so i don't want to i
00:24:12.159 don't want to sort of people to have the
00:24:13.600 wrong impression that i think we're all
00:24:14.799 about to die
00:24:17.440 i think i think we'll i think things
00:24:19.200 will most likely be okay for a lo for a
00:24:21.039 long time on earth
00:24:22.400 but not not for sure but most likely um
00:24:26.320 um but but even if it's if it's sort of
00:24:28.559 99
00:24:29.440 likely one a one percent chance it's
00:24:31.279 still it's still worth
00:24:32.799 uh spending a fair bit of effort to
00:24:34.559 ensure that we have um
00:24:36.080 we've backed up the biosphere you know
00:24:37.760 planetary redundancy if you will
00:24:40.159 um and uh and so i think i think it's
00:24:43.440 really
00:24:43.760 really quite important and in order to
00:24:47.039 do that
00:24:47.840 there's a breakthrough that needs to
00:24:48.880 occur which is to create a a rapidly and
00:24:51.279 completely
00:24:52.159 reusable um transport system to mars
00:24:55.919 um which which is one of those things
00:24:57.600 that's right on the borderline
00:24:59.600 of of of of of impossible
00:25:03.039 um but that that's sort of the the thing
00:25:04.720 that we're we're going to try to achieve
00:25:06.799 that with
00:25:07.679 with with spacex when i was a kid i was
00:25:10.960 wondering
00:25:12.159 kind of what's the meaning of life like
00:25:14.480 why are we here
00:25:15.520 what's it all about and um
00:25:19.200 i came to the conclusion that uh
00:25:22.320 what what really matters is trying to
00:25:25.120 understand the right questions to ask
00:25:27.520 and the more that we can increase
00:25:30.559 the scope and scale of human
00:25:33.520 consciousness
00:25:34.799 the better we are able to ask these
00:25:36.320 questions so i think that there's
00:25:38.559 certain things that are necessary to
00:25:40.559 ensure that the future is good
00:25:44.159 and some of those things are
00:25:47.520 in the long term having long-term
00:25:49.440 sustainable
00:25:50.559 transport and sustainable energy
00:25:52.080 generation
00:25:54.880 and uh to be a space-bearing
00:25:58.799 civilization
00:26:00.240 and for humanity to be out there among
00:26:02.000 the stars and
00:26:03.440 be a multi-planetary uh species
00:26:08.000 i mean i think being a multi-planet
00:26:09.679 species and being out there among the
00:26:10.799 stars is important for
00:26:12.880 uh the long-term survival of humanity
00:26:16.960 and uh that's one reason kind of like
00:26:19.840 life insurance for life collectively
00:26:22.000 life as we know it but then the
00:26:25.200 part that i find personally most
00:26:27.600 motivating
00:26:28.400 is that it creates a sense of adventure
00:26:32.480 and it makes people excited about the
00:26:34.000 future
00:26:36.080 and if you consider two futures one
00:26:38.000 where we are forever confined to earth
00:26:40.960 until eventually something terrible
00:26:43.440 happens
00:26:44.559 or another future where we are out there
00:26:47.520 on
00:26:47.840 many planets maybe even going beyond the
00:26:50.000 solar system
00:26:51.679 i think that second version is
00:26:53.360 incredibly exciting and inspiring
00:26:56.159 and there need to be reasons to get up
00:26:58.480 in the morning
00:26:59.360 you know life can't just be about
00:27:00.640 solving problems otherwise what's the
00:27:02.720 point
00:27:03.679 there's got to be things that people
00:27:05.120 find inspiring
00:27:06.880 and make life worth living you're 47
00:27:10.559 what is the likelihood that you
00:27:11.760 personally will go to mars 70
00:27:15.120 we've recently made a number of
00:27:16.159 breakthroughs that i that i'm just
00:27:18.080 really fired up about
00:27:19.200 and when does that happen in our
00:27:20.880 lifetimes yeah yeah
00:27:22.240 i'm talking about moving there so it's
00:27:23.679 like so if you get the price per ticket
00:27:25.520 maybe around
00:27:26.159 a couple hundred thousand dollars this
00:27:28.000 could be an escape hatch for
00:27:29.679 rich people no if your probability of
00:27:32.559 dying moz is much higher than
00:27:34.000 earth really the africa going to mars
00:27:36.000 would be like shackleton's after going
00:27:37.200 to the antarctic
00:27:38.320 it's going to be hard there's a good
00:27:40.320 chance of death
00:27:42.159 going in a little can through deep space
00:27:45.039 you might land successfully
00:27:46.880 once you land successfully there will be
00:27:48.720 a map you'll be working
00:27:49.919 non-stop to build the base series you're
00:27:52.640 not not much time for leisure
00:27:54.399 and once you get there even after all
00:27:56.320 this uh there's a very harsh environment
00:27:58.399 to use a good chance you die there
00:28:00.960 we think you can come back but we're not
00:28:02.640 sure now does that sound like an escape
00:28:04.720 patch for rich people
00:28:05.919 and yet you would unhesitating like you
00:28:08.320 know there's lots of people like climb
00:28:09.760 mountains you know why they climb
00:28:11.120 mountains because people die on
00:28:12.960 endeavors all the time they like doing
00:28:14.880 it for the challenge
00:28:16.640 i think that the probable probable
00:28:19.679 outcome for civilization is
00:28:20.880 on earth is quite quite good for a long
00:28:23.039 time um
00:28:25.440 but i still think that we should try to
00:28:27.840 extend life beyond earth and have a and
00:28:29.679 the thing to do is to establish a base
00:28:31.120 on mars
00:28:32.240 and ultimate and try to make that a
00:28:33.840 self-sustaining base as soon as possible
00:28:36.399 um so uh i don't expect that spacex is
00:28:39.919 going to do that sort of single-handedly
00:28:41.360 but i think we're
00:28:42.480 we're gonna try to advance the
00:28:44.480 technology of
00:28:45.919 space travel to the point where
00:28:48.960 we can at least send some number of
00:28:50.880 people to mars which is not
00:28:52.240 currently possible on the tesla front
00:28:56.000 the goal with tesla was really to try to
00:28:58.159 show that what electric cars
00:29:00.159 can do because people had the wrong
00:29:01.520 impression we had to
00:29:03.600 change people's perception of an
00:29:05.279 electric vehicle because they
00:29:06.640 used to think of it as something that
00:29:08.080 was slow and
00:29:10.159 ugly and had low range kind of like a
00:29:11.679 golf cart um and and
00:29:14.080 so that's why we created the tesla
00:29:15.520 roadster to show that you can be fast
00:29:17.360 um attractive and and long range um
00:29:20.960 and it's amazing how even though you can
00:29:23.440 show that something works on paper
00:29:25.679 you know and the calculations are very
00:29:27.200 clear until you actually have the
00:29:29.039 physical
00:29:29.600 object and they can they can drive it it
00:29:31.520 doesn't really sink in for people
00:29:33.679 um and so that that i think is is
00:29:35.520 something worth noting if you're going
00:29:37.039 to create a company the first thing you
00:29:38.399 should try to do is create a working
00:29:39.760 prototype
00:29:41.200 um you know everything everything looks
00:29:43.200 great on powerpoint
00:29:45.039 you can you can make anything work on
00:29:46.720 powerpoint but if you have
00:29:48.799 if you have an actual demonstration
00:29:50.559 article even if it's in primitive form
00:29:52.960 that's much much more effective for
00:29:54.480 convincing people now is the time to
00:29:57.120 overrule this administration's pledge
00:30:00.399 to mediocrity listen tesla's to sell
00:30:03.760 sell sell
00:30:04.640 you don't want to own this stock you
00:30:05.840 shouldn't even rent the dorn thing
00:30:08.159 why because beyond the hype there's just
00:30:10.640 not much going on here
00:30:12.080 tesla still has yet to turn a profit
00:30:13.520 there'll be a 1.5 billion dollar company
00:30:15.039 with no profit his most recent quarter
00:30:16.799 actually lost more money than it did the
00:30:18.320 year before 1.5 billion
00:30:19.919 losing more money than you before this
00:30:22.080 is a company with limited visibility
00:30:24.240 you put 90 billion dollars like 50 years
00:30:27.760 worth
00:30:28.240 of breaks into into solar and wind to
00:30:31.360 to solyndra and fisker and tesla and
00:30:34.240 enter one i mean i had a friend who said
00:30:36.000 you don't just pick the winners and
00:30:36.960 losers you pick the losers
00:30:38.559 private enterprise will not ever
00:30:41.919 lead a space frontier not because i
00:30:44.000 don't want them to
00:30:45.120 but my read of history history tells me
00:30:47.600 they can't
00:30:48.480 it's not possible one of the biggest
00:30:51.360 mistakes people generally make and i'm
00:30:52.880 guilty of it too
00:30:53.840 is wishful thinking you know like you
00:30:56.720 want something to be true
00:30:58.399 even if it isn't true um and so you
00:31:01.360 ignore the things that
00:31:04.000 you ignore the real truth because of
00:31:06.000 what you want to be true
00:31:09.120 this is a very difficult trap to avoid
00:31:12.159 and like i said certainly one that i
00:31:14.640 find myself
00:31:15.360 in having problems with
00:31:18.480 but if you just take that approach of
00:31:20.960 you're always to some degree wrong
00:31:22.960 and your goal is to be less wrong and
00:31:25.919 and solicit critical feedback
00:31:27.840 particularly from friends like friends
00:31:30.720 particularly friends
00:31:31.679 if somebody loves you they want the best
00:31:33.840 for you they don't want to tell you the
00:31:35.519 bad things
00:31:37.039 um so you have to ask them
00:31:40.080 okay you know and said really i really
00:31:41.919 do want to know
00:31:43.120 um if you were 22 today what with the
00:31:45.279 five problems that you would think about
00:31:46.720 working on b um well first of all i
00:31:49.120 think um
00:31:50.320 if somebody is doing something that is
00:31:52.240 useful to
00:31:53.519 the rest of society i think that's a
00:31:55.200 good thing like it doesn't have to
00:31:56.640 change the world like
00:31:58.000 you know if you're doing something that
00:32:00.399 has high value
00:32:01.360 to people um and frankly even if it's
00:32:04.240 something
00:32:04.720 if it's like um just a little game um
00:32:08.000 or you know the
00:32:11.360 some improvement in photo sharing or
00:32:12.960 something if it if it has a small amount
00:32:15.519 of good uh for a large number of people
00:32:17.919 um that's
00:32:19.200 i mean i think that's that's fine like
00:32:21.519 stuff doesn't need to be changed the
00:32:22.720 world just to be good
00:32:24.240 um uh but you know in terms of things
00:32:27.360 that i think
00:32:28.000 are most likely to affect the the future
00:32:30.159 of humanity i think
00:32:31.679 um ai is probably the single biggest
00:32:34.480 item in the near term that's likely to
00:32:35.919 affect
00:32:36.880 uh humanity so it's very important that
00:32:39.360 we have
00:32:40.159 the advent of ai uh in
00:32:43.360 a good way that that is something that
00:32:46.159 um
00:32:48.320 if you if you could look into the
00:32:49.519 crucible and see the future you would
00:32:51.200 like you would like that outcome
00:32:52.960 um because it is something that could go
00:32:56.399 um could go wrong um as we've talked
00:32:59.120 about many times
00:33:00.399 um and so we really need to make sure it
00:33:02.559 goes right
00:33:03.760 um that's that's i think ai
00:33:07.679 working on ai and making sure it's a
00:33:10.399 great future that's that's the most
00:33:11.679 important thing i think
00:33:12.640 right now um the most pressing item sec
00:33:16.000 uh then um obviously anything to do with
00:33:19.120 with genetics
00:33:20.399 um if you can actually solve
00:33:24.000 genetic diseases if you can
00:33:27.120 prevent dementia or alzheimer's or
00:33:29.919 something like that that
00:33:31.200 with genetic reprogramming that would be
00:33:32.960 wonderful so i think this
00:33:35.760 genetics it might be the sort of
00:33:38.880 second most important item i think
00:33:42.240 um having a high bandwidth interface to
00:33:45.919 the brain like um we're currently
00:33:48.159 bandwidth limited
00:33:48.960 we have a digital tertiary self in the
00:33:51.440 form of our email
00:33:52.399 capabilities like computers phones
00:33:54.640 applications
00:33:55.760 uh we're effectively superhuman but
00:33:57.840 we're extremely bound with constrained
00:33:59.760 in that interface between the cortex and
00:34:01.760 your sort of
00:34:03.440 uh that tertiary digital form of
00:34:06.240 yourself and
00:34:07.360 helping solve that bandwidth constraint
00:34:09.679 would would be
00:34:10.560 i think very important for the future as
00:34:12.079 well
00:34:13.918 what have you done or what did you do
00:34:15.599 when you were younger that
00:34:17.280 you think sort of set you up to have a
00:34:19.040 big impact
00:34:20.719 well i think first of all i should say
00:34:21.918 that i do not expect to be involved in
00:34:23.440 all these things
00:34:24.159 so the the the the five things that i
00:34:27.679 thought about
00:34:28.320 the time in in college quite a long time
00:34:30.800 ago uh
00:34:31.760 25 years ago
00:34:35.280 you know being you know making life
00:34:36.719 multi-planetary
00:34:38.399 um selling accelerating the transition
00:34:40.719 to sustainable energy
00:34:42.399 um the the internet broadly speaking
00:34:45.520 um and and then genetics and ai i think
00:34:49.520 um
00:34:50.239 i didn't expect to be involved in in in
00:34:53.040 all of those things i actually
00:34:54.399 at the time in college i sort of thought
00:34:56.399 um helping with electrification of
00:34:59.200 cars was how i would start out and
00:35:01.200 that's uh that's actually what i worked
00:35:03.040 on as an intern was
00:35:04.320 um advanced uh ultra capacitors with
00:35:07.359 to see if there would be a breakthrough
00:35:09.200 relative to batteries
00:35:10.800 for energy storage and cars and then
00:35:13.839 when i came out to go to stanford um
00:35:15.839 that's what i was going to be doing my
00:35:17.040 grad studies on is
00:35:18.480 this was working on advanced energy
00:35:21.359 storage
00:35:22.079 technologies for electric cars and i put
00:35:24.320 that on hold to
00:35:26.320 start an internet company in 95 because
00:35:29.920 um
00:35:31.359 there does seem to be like a time for
00:35:33.040 particular technologies uh when they're
00:35:35.440 at a steep point in the inflection code
00:35:38.960 and um and i didn't want to you know do
00:35:42.079 a phd at stanford and then
00:35:43.520 and watch it all happen um and then and
00:35:46.640 i wasn't entirely certain that
00:35:48.079 the technology i'd be working on would
00:35:49.520 actually succeed
00:35:51.280 um i can get you can get a you know
00:35:54.000 doctrine on many things that ultimately
00:35:55.680 are not
00:35:56.400 do not have a practical bearing on the
00:35:58.640 world
00:35:59.520 um and i wanted to you know just i
00:36:02.000 really was just trying to be useful
00:36:03.520 that's the optimization it's like what
00:36:06.000 what
00:36:06.560 what can i do that would actually be
00:36:07.839 useful how should someone figure out how
00:36:10.079 they can be most useful whatever this
00:36:12.240 thing is that you're trying to
00:36:13.440 create what would what would be the
00:36:17.040 utility delta compared to the current
00:36:19.119 state of the art
00:36:20.160 times how many people it would affect so
00:36:22.560 that's why i think
00:36:24.079 having something that has that that has
00:36:26.000 a makes makes a big difference but
00:36:27.839 affects a
00:36:28.960 sort of small to moderate number of
00:36:30.160 people is great as is something that
00:36:32.079 makes
00:36:32.800 even a small difference but it but
00:36:34.480 affects a vast number of people
00:36:36.320 when you're trying to estimate
00:36:37.920 probability of success so
00:36:39.520 this thing will be really useful good
00:36:40.880 area under the curve i guess to use the
00:36:42.640 example of spacex
00:36:44.640 when you made the go decision that you
00:36:46.000 were actually going to do that this was
00:36:47.119 kind of a very crazy thing at the time
00:36:49.040 very crazy for sure yeah i'm not sure
00:36:51.920 about saying that
00:36:53.200 but i kind of agree i agreed with them
00:36:54.560 that it was quite crazy crazy if
00:36:57.119 um if the objective was um to
00:37:00.480 achieve the um best risk adjusted return
00:37:04.000 um starting our company is insane um but
00:37:07.280 that was not that was not my objective i
00:37:09.119 i
00:37:09.599 i'd simply come to the conclusion um
00:37:12.160 that if something didn't
00:37:13.760 happen to improve rocket technology
00:37:15.359 would be stuck on earth forever
00:37:17.599 and um and the big aerospace companies
00:37:20.640 had just had no interest in radical
00:37:22.240 innovation um all they wanted to do was
00:37:24.960 try to make their old technology
00:37:28.160 slightly better every year and in fact
00:37:30.000 um sometimes it would actually get worse
00:37:32.640 um and particularly in rockets is pretty
00:37:34.880 bad like the
00:37:36.240 in in 69 we were able to go to the moon
00:37:39.119 with a saturn v
00:37:40.079 and then the space shuttle could only
00:37:41.280 take people to low earth orbit and then
00:37:42.560 the space shuttle retired and that that
00:37:44.079 trend is basically trends to zero
00:37:46.720 um if you also think technology just
00:37:49.839 automatically gets better every year but
00:37:51.440 it actually doesn't it only gets better
00:37:52.880 if smart people work
00:37:54.640 work like crazy to make it better that's
00:37:57.200 how any technology actually gets better
00:37:59.760 and by itself technology if people don't
00:38:03.520 work in it actually will decline
00:38:05.200 um i mean you can look at the history of
00:38:07.599 civilizations many civilizations
00:38:09.520 and look at say um ancient egypt where
00:38:12.000 they were able to build these incredible
00:38:13.119 pyramids and then they
00:38:14.320 basically forgot how to build pyramids
00:38:16.839 um
00:38:18.000 and and then even hieroglyphics they've
00:38:21.040 forgotten how to read hieroglyphics so
00:38:22.720 we look at rome and how they're able to
00:38:24.480 look
00:38:24.800 to build these incredible roadways and
00:38:26.560 aqueducts and indoor planning they've
00:38:28.320 got how to do all of those things
00:38:30.640 and um there are many such examples in
00:38:32.960 in history
00:38:34.160 um so i i think um
00:38:37.359 should always bear in mind that
00:38:40.800 you know entropy is not on your side
00:38:44.880 you may have heard me say that it's good
00:38:46.800 to think in terms of
00:38:48.160 the physics approach or first principles
00:38:50.720 uh which is
00:38:51.760 [Music]
00:38:52.960 rather than reasoning by analogy you
00:38:54.880 boil things down to
00:38:56.560 the most fundamental truths you can
00:38:57.839 imagine and you reason up from there
00:39:00.320 and this is a good way to figure out if
00:39:04.079 if something really makes sense or if
00:39:06.480 it's just
00:39:07.599 what everybody else is doing
00:39:10.800 it's hard to think that way you can't
00:39:12.880 think that way about everything
00:39:14.320 it takes a lot of effort but if you're
00:39:17.200 trying to do something new
00:39:18.400 it's the best way to think and that
00:39:20.640 framework was developed
00:39:21.920 by by physicists to figure out
00:39:24.800 counterintuitive things
00:39:27.119 like quantum mechanics so it's really a
00:39:30.000 powerful
00:39:30.960 powerful method how do you think about
00:39:33.440 making a decision when everyone tells
00:39:34.880 you this is a crazy idea or where do you
00:39:36.480 get the internal strength to do that
00:39:39.040 well first of all i'd say i actually
00:39:40.480 think i think i feel feel fear quite
00:39:42.800 strongly
00:39:43.920 um so it's not as though i just have the
00:39:47.119 absence of fear i've
00:39:48.320 i feel it quite strongly um but
00:39:51.359 there are just times when something is
00:39:53.520 important enough
00:39:54.400 you believe in it enough that you you do
00:39:57.040 it in spite of
00:39:57.680 fear people should think well i feel
00:40:00.240 fear about this and therefore i
00:40:01.920 shouldn't do it
00:40:02.800 um it's normal to be to feel fear like
00:40:05.599 you'd have to
00:40:06.560 definitely something mentally wrong if
00:40:08.000 you didn't feel fear if you have an
00:40:10.720 advice to them
00:40:12.160 young people globally who want to be
00:40:13.839 like elon musk
00:40:15.599 what's your advice to them i think that
00:40:19.920 probably they shouldn't want to be
00:40:23.200 you it i think it sounds better than it
00:40:25.680 is okay
00:40:27.040 yeah it's uh not as much fun being me as
00:40:30.880 you'd think
00:40:31.680 i don't know you don't think so yeah
00:40:34.319 there's definitely it could be worse for
00:40:35.760 sure
00:40:36.480 but it's um i
00:40:39.520 i'm not sure i would i'm not sure i want
00:40:41.520 to be me
00:40:43.440 so when everybody leaves it's just elon
00:40:45.760 sitting at home brushing his teeth
00:40:47.680 just bunch ideas bouncing around your
00:40:49.440 head when did you realize that that's
00:40:50.880 not the case with most people
00:40:53.760 i think when i was i don't know five or
00:40:55.520 six or something i thought i was insane
00:40:59.599 it was just strange because it was clear
00:41:03.280 that other people do not what
00:41:06.240 their mind wasn't exploding with ideas i
00:41:08.800 was like
00:41:10.960 hmm i'm strange
00:41:14.720 i don't think i don't think you'd
00:41:16.800 necessarily want to be me
00:41:18.720 people would like it that much it's very
00:41:21.200 hard to turn it off
00:41:24.240 it's like a neverending explosion all
00:41:27.359 the time
00:41:28.800 what do you think the odds of the mars
00:41:30.160 colony are at this point today
00:41:34.240 um oddly enough i actually think
00:41:37.839 they're pretty good at this point i am
00:41:40.160 certain there is a way
00:41:41.760 i'm certain that success is one of the
00:41:43.280 possible outcomes for establishing
00:41:45.440 a self-sustaining mars colony in fact
00:41:47.760 growing mars colony
00:41:49.119 i'm certain that that is possible
00:41:51.280 whereas
00:41:52.319 until maybe a few years ago i was not
00:41:54.800 sure that success was even one of the
00:41:56.319 possible outcomes
00:41:58.160 it's a meaningful number of people going
00:41:59.599 to mars i think this is
00:42:02.240 potentially something that can be
00:42:03.839 accomplished in about 10 years
00:42:06.880 maybe sooner maybe nine years
00:42:11.280 i need to make sure that spacex doesn't
00:42:13.440 die between now and then
00:42:14.560 and that i don't die or if i do die that
00:42:17.040 someone takes over who will continue
00:42:18.560 that
00:42:19.760 you shouldn't go on the first launch
00:42:21.760 yeah exactly
00:42:23.119 the best of the available alternatives
00:42:25.839 that i can come up with and maybe
00:42:27.040 somebody else can come up with a better
00:42:29.040 approach or better outcome is
00:42:32.480 that we achieve democratization of ai
00:42:35.280 technology meaning that
00:42:37.280 no one company or a
00:42:40.319 small set of individuals has control
00:42:42.000 over advanced ai technology
00:42:44.000 i think that that's very dangerous
00:42:48.400 it could also get stolen by somebody bad
00:42:50.880 you know like some evil dictator
00:42:52.720 or country could send their intelligence
00:42:55.200 agency to go steal it and gain control
00:42:57.599 it just becomes a very unstable
00:42:59.040 situation i think if you've got any
00:43:01.200 um any incredibly powerful ai
00:43:06.319 you just don't know who's who's going to
00:43:07.839 control that so it's not as i think that
00:43:10.000 the risk is that the ai would develop a
00:43:11.599 will of its own right off the bat i
00:43:13.040 think
00:43:13.520 it's more it's the consumers that some
00:43:16.560 someone um may use it in a way that is
00:43:19.839 bad
00:43:20.800 um or and even if they weren't going to
00:43:22.720 use it in a way that's bad that somebody
00:43:24.079 could take it from them and use it in a
00:43:25.680 way that's bad
00:43:27.200 that that i think is quite a big danger
00:43:29.200 so i think we must have democratization
00:43:30.720 of ai technology and make it widely
00:43:32.240 available
00:43:33.440 um and that's you know the reason that
00:43:35.760 obviously uh
00:43:36.880 the rest of the team uh you know created
00:43:38.400 open ai
00:43:40.079 was to help uh with the democracy
00:43:43.839 help help spread out ai technology so
00:43:47.359 it doesn't get concentrated in the hands
00:43:49.040 of a few
00:43:50.560 and but then of course that needs to be
00:43:53.599 combined with solving the high bandwidth
00:43:57.040 interface to the cortex
00:44:00.480 um humans are so slow humans are so slow
00:44:04.319 yes exactly but you know we already have
00:44:06.800 a situation in our brain where we've got
00:44:08.400 the cortex and the limbic system
00:44:10.800 and the limbic system is kind of i mean
00:44:14.000 that's that's the primitive brain it's
00:44:16.000 kind of like the urine
00:44:17.040 your instincts and um whatnot
00:44:20.960 and then the cortex is the thinking
00:44:22.640 upper part of the brain
00:44:24.079 those two seem to work together quite
00:44:26.240 well um
00:44:27.599 occasionally your cortex and limbic
00:44:29.200 system may disagree generally works
00:44:30.880 pretty well and
00:44:31.760 it's like rare to find someone who i've
00:44:34.160 not found someone who wishes to either
00:44:35.920 get rid of their cortex or get rid of
00:44:37.280 their living system
00:44:38.400 so i think if if we can effectively
00:44:41.599 uh um merge with uh ai
00:44:45.119 by um improving that
00:44:48.319 the the neural link between your cortex
00:44:51.119 and the
00:44:52.640 the your digital extension yourself
00:44:54.079 which already likes it already exists
00:44:56.079 just has a bandwidth issue um
00:44:59.280 and then then effectively um
00:45:02.319 you become an ai human symbiote
00:45:05.680 um and and if that then is widespread
00:45:08.640 with
00:45:09.599 anyone who wants it can have it uh then
00:45:12.240 we solve the control problem as well
00:45:14.400 um we don't have to worry about um some
00:45:17.200 sort of evil dictator ai
00:45:18.720 um because kind of we are the ai um
00:45:20.960 collectively
00:45:22.480 that seems like the best outcome i can
00:45:24.319 think of i think we've got
00:45:25.599 a really talented group with opening eye
00:45:27.280 yeah really really talented
00:45:28.640 team and they're working hard open a is
00:45:31.920 structured as
00:45:32.800 uh see a 51c3 nonprofit
00:45:36.240 um but you know many non-profits uh
00:45:39.359 do not have a sense of urgency it's fine
00:45:41.520 they don't have to have a sense of
00:45:42.480 urgency
00:45:43.599 um but open ai does um
00:45:47.040 because i think people really believe in
00:45:48.400 the mission i think it's important
00:45:50.480 um and it's it's about minimizing
00:45:54.880 the risk of existential harm in the
00:45:58.839 future and uh so i think it's going well
00:46:01.760 i'm pretty
00:46:02.720 impressed with what people are doing and
00:46:05.040 the talent level
00:46:06.720 and obviously we're always looking for
00:46:10.000 great people to join when i interview
00:46:11.839 somebody i really just ask them to tell
00:46:13.920 me the story of their career
00:46:15.839 and what they you know what are some of
00:46:19.359 the
00:46:19.920 tougher problems that they dealt with
00:46:22.400 how they dealt with those
00:46:24.000 and how they made decisions
00:46:27.119 at key transition points and usually
00:46:29.200 that's enough for me to get a very good
00:46:31.520 gut feel
00:46:32.240 about someone and what i'm really
00:46:34.880 looking for
00:46:35.680 is evidence of exceptional ability
00:46:39.680 so did they face really difficult
00:46:42.720 problems
00:46:43.520 and overcome them um and
00:46:46.720 and then of course you want to make sure
00:46:47.760 that that if there was some
00:46:50.640 significant accomplishment were they
00:46:52.319 really responsible or somebody else more
00:46:54.079 responsible
00:46:55.280 and usually the person who's had to
00:46:57.920 struggle with the problem
00:46:59.040 they really understand it you know and
00:47:01.040 they don't forget
00:47:02.400 you know if it was very difficult so you
00:47:05.119 can ask them
00:47:06.160 detailed very detailed questions about
00:47:07.839 it and they will they'll know the answer
00:47:09.520 whereas the person who was not truly
00:47:11.440 responsible for
00:47:13.440 that accomplishment uh will not know the
00:47:15.280 details there's no need even to have
00:47:17.440 a college degree at all or even
00:47:20.800 high school i mean if somebody graduated
00:47:23.119 from a great university
00:47:24.079 that may be indeed that may be an
00:47:25.920 indication that they will be capable of
00:47:27.760 great things but it's not necessarily
00:47:29.760 the case um
00:47:31.280 you know if you look at say people like
00:47:34.880 bill gates or larry ellison steve jobs
00:47:38.319 these guys didn't graduate from college
00:47:40.400 but if you had a chance to hire them of
00:47:41.839 course that would be
00:47:43.520 a good idea so you know just
00:47:47.280 looking just for evidence of exceptional
00:47:49.760 ability
00:47:51.040 and if there's a track record of
00:47:52.480 exceptional achievement then it's likely
00:47:54.400 that that will
00:47:55.200 continue into the future what sort of
00:47:57.359 things do you look for in people
00:47:59.200 or in processes that make the workforce
00:48:01.520 better
00:48:02.400 well i think the massive thing that can
00:48:05.760 be done is
00:48:06.559 to make sure your incentive structure is
00:48:08.400 such that uh
00:48:10.079 innovation is rewarded and lack of
00:48:12.000 innovation is punished
00:48:13.359 there's got to be a characteristic so
00:48:16.559 if somebody is innovating um
00:48:20.079 and doing making good good progress
00:48:23.760 then they should be promoted sooner um
00:48:26.880 and if somebody is
00:48:28.240 completely failing to innovate um not
00:48:30.400 every role requires innovation
00:48:32.000 but if they're in a role where
00:48:34.640 innovation
00:48:35.280 is should be happening and it's not
00:48:38.160 happening then they should
00:48:39.920 either not be promoted or exited and let
00:48:42.720 me tell you you'll get promote you could
00:48:44.240 you'll you'll get innovation real fast
00:48:46.240 does that carrot and stick approach
00:48:48.559 help uh do you think people be
00:48:51.839 more risk averse
00:48:54.960 or less risk averse
00:48:58.000 when trying different things you've got
00:49:00.079 to have some acceptance of failure
00:49:02.480 failure must be an option if failure is
00:49:04.400 not an option it's going to result in
00:49:05.599 extremely conservative choices
00:49:07.359 and you may not may get something even
00:49:10.640 worse than
00:49:11.440 lack of innovation things may go
00:49:12.960 backwards what you really want is
00:49:15.520 you want reward and punishment to be
00:49:18.079 proportionate to the actions that you
00:49:19.920 seek
00:49:21.280 so if uh if what you're seeking is
00:49:24.240 innovation
00:49:25.359 then you should reward success and
00:49:27.280 innovation
00:49:28.400 um and only
00:49:32.160 there there should be minor consequences
00:49:34.880 for lack of
00:49:36.240 minor consequences for for trying and
00:49:38.079 failing
00:49:39.280 should there should be minor with
00:49:42.400 significant rewards for trying and
00:49:43.920 succeeding
00:49:46.079 minor consequences for trying and not
00:49:48.800 succeeding
00:49:49.839 um and big and major negative
00:49:52.800 consequences for not
00:49:54.079 trying if you have that incentive
00:49:56.160 structure you will get innovation
00:49:58.000 like you can't believe the purpose of
00:49:59.920 neural link like
00:50:01.119 what do we what's our goal our goal is
00:50:03.359 to solve
00:50:04.400 important spine and brain problems with
00:50:06.559 a seamlessly seamlessly
00:50:08.160 implanted device so you want to have a
00:50:10.319 device that you can basically
00:50:12.079 put in your head and feel and look
00:50:15.119 totally normal
00:50:16.480 but it solves some important problem
00:50:20.720 in your brain or spine so going into the
00:50:23.440 neural link architecture
00:50:24.880 what we've done over the past year is
00:50:26.480 dramatically simplify
00:50:28.079 the device so we we about a year ago we
00:50:30.960 had a device which
00:50:32.319 uh had multiple parts including a piece
00:50:35.760 that it had to sort of sit behind your
00:50:37.839 ear
00:50:39.040 and it was it was it was complex and you
00:50:41.359 and
00:50:42.079 you wouldn't still look totally normal
00:50:43.920 you'd have a thing behind your ear
00:50:45.359 so um we've simplified this to
00:50:49.520 simply something that is about
00:50:53.359 the size of a large coin um and it it
00:50:56.640 goes
00:50:57.599 uh in your skull replaces a piece of
00:50:59.920 skull
00:51:00.880 um and the wires uh then then connect
00:51:04.480 uh within a few centimeters or about an
00:51:07.040 inch away from
00:51:08.000 the device um and this is sort of what
00:51:11.440 it looks like
00:51:13.440 this is a little device i mean frankly
00:51:15.839 to to sort of simplify this
00:51:18.240 uh what we're i mean it's more than this
00:51:21.440 but it's
00:51:22.319 in a lot of ways it's kind of like a
00:51:24.000 fitbit in your skull with tiny wires
00:51:26.319 our current prototype version 0.9 has
00:51:29.200 about a thousand channels
00:51:31.040 so that's about 100 times better than
00:51:34.079 the
00:51:34.800 the next best
00:51:38.079 consumer device that's available and
00:51:40.640 it's a 23 millimeters by eight
00:51:42.160 millimeters it actually
00:51:43.520 uh fits quite nicely in your skull just
00:51:45.760 your skull is
00:51:47.119 about 10 millimeters thick so
00:51:50.400 it fits it goes flush with your skull
00:51:52.880 it's invisible
00:51:53.760 and all you can see afterwards is
00:51:55.119 there's a tiny scar and if it's under
00:51:57.040 your hair you can't see it at all in
00:51:58.319 fact i could
00:51:59.040 have a neural link right now and you
00:52:00.319 wouldn't know it's also inductively
00:52:02.000 charged so
00:52:03.200 it's charged in the same way that you
00:52:05.040 cho you charge a smart watch or a phone
00:52:07.520 um and so you can use it all day uh
00:52:10.079 charge it at night
00:52:11.200 and have full functionality so you would
00:52:12.800 really um
00:52:15.440 you know it would be completely seamless
00:52:18.160 and
00:52:19.119 yeah no wires uh in terms of getting a
00:52:21.040 link so that
00:52:22.400 we you need to have the device a a great
00:52:24.880 device and you also need to have a great
00:52:26.319 robot that
00:52:28.000 puts in the electrodes and
00:52:31.119 it does the surgery so you want the
00:52:32.720 surgery to be as as automated
00:52:35.040 and as possible and the only way you can
00:52:37.280 achieve the level of precision that's
00:52:38.559 needed
00:52:39.200 is with an advanced robot the link
00:52:41.520 procedure the
00:52:42.480 the installation of a link done in under
00:52:44.800 an hour
00:52:46.319 so you can basically go in the morning
00:52:47.760 and leave the hospital in the afternoon
00:52:49.920 and it can be done without general
00:52:51.359 anesthesia
00:52:53.359 so this is our surgical robot and we
00:52:56.000 actually ultimately want this robot to
00:52:57.599 do
00:52:58.720 essentially the entire surgery uh so in
00:53:01.920 everything from from incision uh
00:53:05.280 removing the the skull inserting the
00:53:07.040 electrodes placing the device
00:53:09.040 um and then um closing things up and
00:53:11.599 having you ready to leave so
00:53:13.119 we want to have a fully automated system
00:53:15.280 how do you spend your days now
00:53:17.280 like what what do you allocate most of
00:53:19.520 your time to
00:53:20.559 my time is mostly split uh well between
00:53:23.680 spacex and
00:53:24.720 and tesla and of course i try to spend
00:53:27.200 um
00:53:29.520 it's a part of every week at open ai so
00:53:32.160 i spend most
00:53:33.359 i spend basically half a day at openai
00:53:36.240 most weeks
00:53:37.440 and then and then i have some opening
00:53:40.319 stuff that happens during the week
00:53:42.319 i think a lot of people think i must
00:53:43.920 spend a lot of time with media
00:53:45.839 or or on businessy things but actually
00:53:48.640 almost
00:53:49.440 uh almost all my time like 80 of it is
00:53:51.599 spent on engineering design
00:53:53.760 in engineering and design so it's um
00:53:56.640 developing next generation
00:53:58.000 product that's
00:54:01.440 80 of it i think a lot of people think
00:54:03.680 i'm kind of a business person or
00:54:05.280 something which is fine like business is
00:54:06.800 fine but um
00:54:08.160 like i uh but really it's
00:54:11.680 you know it was like it's spacex uh gwen
00:54:14.000 shotwell is chief operating officer she
00:54:16.079 kind of manages um uh legal finance
00:54:19.760 um sales um and kind of general business
00:54:23.040 activity and then
00:54:24.240 my time is almost entirely with the
00:54:26.480 engineering team
00:54:27.520 working on improving the falcon 9 and
00:54:30.960 the dragon spacecraft and developing the
00:54:33.200 most colonial architecture
00:54:35.200 i mean at tesla it's working on the
00:54:38.799 model 3
00:54:39.599 and you know some in the design studio
00:54:43.200 typically have a day week
00:54:47.599 dealing with aesthetics and and
00:54:50.960 look and feel things and and then most
00:54:53.839 of the rest of the week is just going
00:54:54.799 through engineering
00:54:56.160 of of the car itself as well as
00:54:58.240 engineering of the
00:54:59.599 the factory um because the the biggest
00:55:03.040 epiphany i've had is that what really
00:55:05.839 matters
00:55:06.400 is the is the machine that builds the
00:55:08.000 machine the factory
00:55:09.920 um and this that is at least towards
00:55:11.440 magnitude harder than the vehicle itself
00:55:14.079 what are the scenarios that scare you
00:55:16.640 most
00:55:17.359 humanity really is not evolved to think
00:55:20.000 of
00:55:20.400 existential threats in general we're
00:55:22.480 involved to think about
00:55:23.520 things that are very close to us near
00:55:26.000 term
00:55:26.880 to to be upset with other humans and not
00:55:29.359 not to really to think about things that
00:55:31.040 could destroy humanity as a whole
00:55:33.520 but then in recent decades recent
00:55:36.960 just really in the last century we had
00:55:38.960 nuclear bombs which
00:55:40.079 are could potentially destroy
00:55:41.839 civilization obviously
00:55:43.680 we have ai which could destroy
00:55:45.200 civilization uh we have global warming
00:55:47.440 which could destroy civilization or at
00:55:49.359 least severely disrupt
00:55:51.520 uh civilization um and excuse me how
00:55:53.599 could ai
00:55:55.200 destroy civilization you know it would
00:55:57.680 be something
00:55:59.040 the same way that humans destroyed the
00:56:02.000 habitat
00:56:02.640 of primates i mean it wouldn't
00:56:05.680 necessarily be destroyed but
00:56:07.920 we might be relegated to a small corner
00:56:09.520 of the world when homo sapiens
00:56:11.760 became much smarter than other primates
00:56:14.079 i pushed
00:56:14.799 all the other ones into small habitats
00:56:19.200 couldn't ai even in this moment just
00:56:21.359 with the technology that we have
00:56:23.359 before us be used in some fairly
00:56:25.520 destructive ways you could make a swarm
00:56:27.280 of assassin drones for
00:56:28.799 very little money by just taking the the
00:56:31.680 face id
00:56:32.880 chip that's used in cell phones and
00:56:35.920 uh having a small explosive charge and a
00:56:38.880 standard drone
00:56:40.000 and have them just do a grid sweep of
00:56:41.680 the building until they
00:56:43.280 find the person they're looking for ram
00:56:45.119 into them and explode
00:56:46.720 you can do that right now no extra no
00:56:48.400 new technologies needed right now
00:56:50.480 people just think this stuff is of of
00:56:52.559 sci-fi novels and movies and it's so far
00:56:54.799 away but
00:56:55.760 every time i hear you speak it's like
00:56:56.960 well no this stuff is sitting
00:56:58.960 it's right here probably a bigger risk
00:57:00.960 than being hunted down
00:57:02.720 by a drone is that
00:57:06.000 uh ai would be used to make incredibly
00:57:08.880 effective propaganda
00:57:11.119 that would not seem like propaganda so
00:57:12.799 these are deep fakes
00:57:14.160 yeah influence the direction
00:57:17.520 of society influence elections
00:57:19.920 artificial intelligence
00:57:21.119 just hones the message holds the message
00:57:23.920 check looks the feed looks at the
00:57:25.040 feedback
00:57:25.599 makes this message slightly better
00:57:27.440 within milliseconds it could it can
00:57:30.880 adapt its message and shift and react to
00:57:34.079 news
00:57:34.799 and there's so many uh social media
00:57:36.559 accounts out there that
00:57:38.720 are not people they can't how do you
00:57:41.839 know it's a first another person
00:57:44.240 people look like they have a much better
00:57:46.000 life
00:57:47.280 than they really do people
00:57:50.400 are posting pictures of when they're
00:57:52.000 really happy they're
00:57:53.599 modifying those pictures to be better
00:57:55.280 looking
00:57:56.880 even if they're not modifying the
00:57:58.079 pictures they're at least selecting the
00:57:59.440 pictures for the best lighting the best
00:58:01.200 angle
00:58:02.240 so people basically seem
00:58:05.280 uh they're way better looking than they
00:58:07.839 basically really are
00:58:09.680 um and they're way happier seeming
00:58:13.280 than they really are so if you look at
00:58:15.280 everyone on instagram you might think
00:58:17.040 man
00:58:18.079 they're all these happy beautiful people
00:58:20.240 and
00:58:21.599 i'm not that good looking and i'm not
00:58:23.280 happy so i'm a suck
00:58:25.440 you know and that's gonna make me feel
00:58:28.400 sad
00:58:30.000 when in fact those people you think are
00:58:32.559 super happy
00:58:33.760 actually not that happy some of them are
00:58:36.319 really depressed they're very sad
00:58:39.200 some of the happiest seeming people
00:58:41.200 actually some of the saddest people in
00:58:42.480 reality
00:58:44.000 so i think i think things like that can
00:58:45.520 make people quite sad
00:58:47.839 this may sound corny but love is the
00:58:50.480 answer
00:58:52.160 wouldn't hurt to have more love in the
00:58:53.280 world i think you know
00:58:55.680 i think people should be nicer to each
00:58:57.520 other and
00:58:58.720 give people and give give more credit to
00:59:01.680 others
00:59:03.040 and don't assume that they're mean until
00:59:05.440 you know they're actually mean
00:59:07.280 you know just it's easy to demonize
00:59:10.400 people
00:59:11.280 you're usually wrong about it people are
00:59:13.280 nicer than you think
00:59:15.760 give people more credit
00:59:18.960 there's going to be some amount of
00:59:19.920 failure
00:59:22.240 but you want your net output that useful
00:59:25.440 output to maximized
00:59:27.200 failure is essentially irrelevant unless
00:59:30.319 it is catastrophic
00:59:32.160 the final thing i would encourage you to
00:59:33.520 do is now is the time to take risk
00:59:36.480 as you get older your obligations
00:59:38.240 increase so
00:59:40.720 and once you have a family you start
00:59:42.480 taking risk not just for yourself but
00:59:44.480 for your family as well it gets much
00:59:45.920 harder
00:59:46.480 to do things that might not work out
00:59:50.079 so now is the time to do that
00:59:53.200 before you before you have those
00:59:54.480 obligations so i would encourage you to
00:59:56.880 take risks now do something bold
01:00:00.079 you won't regret it
