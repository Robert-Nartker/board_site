# tactiq.io free youtube transcript
# Sam Altman on His Feud with Elon Muskâ€”and the Battle for AI's Future
# https://www.youtube.com/watch/DfOt_cqXCFI

00:00:00.280 what is the fundamental conflict between
00:00:04.279 Elon Musk and his various allies meta
00:00:07.200 being one of them and you guys like what
00:00:09.160 what is the disagreement fundamentally
00:00:11.120 about look I'm I don't live inside
00:00:13.080 elon's head so this is a little bit of
00:00:15.839 of speculation
00:00:18.279 uh Elon definitely did a lot to help
00:00:21.199 open eye in the early days and in spite
00:00:22.720 of all of this I'm very grateful and I
00:00:24.560 think he's just a sort of legendary
00:00:27.279 entrepreneur um he's also clearly a
00:00:29.679 bully and he's also someone who clearly
00:00:31.480 likes to get in fights you know right
00:00:32.840 now it's me it's been Bezos Gates
00:00:35.600 Zuckerberg lots of other people hi
00:00:37.559 honestly listeners Barry here and I'm so
00:00:39.879 excited for you to listen to this wide
00:00:41.840 ranging and fascinating conversation
00:00:44.280 with open AI Sam Altman before you watch
00:00:47.520 it I wanted to come and talk to you
00:00:50.160 about a big endof year Push by the end
00:00:53.840 of 2024 in just a few days we want to
00:00:57.039 get to a million Free Press subscribers
00:00:59.519 a million people who Value Independence
00:01:02.359 and curiosity and who above all want a
00:01:04.959 news source that reflects reality if
00:01:07.680 you're here if you're on this YouTube
00:01:09.400 page we know it's not just because you
00:01:11.479 believe in Fearless old school
00:01:13.000 journalism for yourself it's because you
00:01:14.960 think it's a necessity for democracy
00:01:17.600 free pressers tell us again and again
00:01:19.680 that we're not just a media company
00:01:21.479 we're a public trust I'm confident that
00:01:24.360 by becoming one of the first million
00:01:26.280 free pressers you will be getting in on
00:01:28.479 the ground floor so before you watch
00:01:30.920 this video take out your cell phone or
00:01:34.119 open a new tab and go to vf.com
00:01:37.960 subscribe and become a free presser
00:01:40.479 today if you're already signed up you
00:01:42.720 can earn a free year subscription or
00:01:45.320 even a pair of TGIF Socks pre-worn by
00:01:47.759 Nelly if you refer your friends or
00:01:49.880 family members okay one more time before
00:01:51.960 we get to Sam Altman go to the free
00:01:53.920 press's website at vp.com ssubscribe and
00:01:58.119 help us get to our goal of a million
00:02:00.640 free pressers by 2025 we are so close
00:02:04.280 and we really appreciate your support on
00:02:06.560 to the show from the free press this is
00:02:09.199 honestly and I'm Barry Weiss just a few
00:02:12.319 years ago as AI technology was beginning
00:02:14.920 to spill out of startups in Silicon
00:02:16.840 Valley and hit our smartphones the
00:02:19.080 political and cultural conversation
00:02:21.239 about this nent technology was not yet
00:02:23.760 clear or at least it wasn't clear yet to
00:02:25.879 civilians like me I remember asking
00:02:28.400 former Google CEO Schmidt on honestly in
00:02:31.480 January 2022 if AI was just like and
00:02:35.280 this is actually what I said the sexy
00:02:37.319 robot in exmachina I literally said to
00:02:40.000 him what is AI how do you define it I do
00:02:43.400 not understand I cringe listening back
00:02:46.599 to that because today in the waning days
00:02:49.040 of 2024 not only has it become clear
00:02:51.959 what AI is and how to use it chat GPT
00:02:54.959 averages more than 120 million daily
00:02:57.440 active users and processes over a
00:02:59.879 billion queries per day but it's also
00:03:02.319 becoming clear what the political and
00:03:04.920 cultural ramifications and the arguments
00:03:07.440 in the debates around AI are and what
00:03:10.519 they're going to be over the next few
00:03:12.319 years those are the big questions who
00:03:15.080 gets to lead us into this new age of AI
00:03:17.640 technology what company is going to get
00:03:19.920 there first and Achieve market dominance
00:03:22.879 how those companies are going to be
00:03:24.440 structured so that bad actors with bad
00:03:26.959 incentives can't manipulate this
00:03:29.159 technology for evil purposes what role
00:03:31.879 the government should play in regulating
00:03:33.879 all of this at the center of these
00:03:36.120 important questions at least for right
00:03:38.040 now are two men Sam ultman and Elon Musk
00:03:42.640 and if you haven't been following they
00:03:44.439 aren't exactly in alignment I don't
00:03:46.799 trust open AI I don't trust Sam Alman
00:03:49.879 and I and I don't think we want to have
00:03:51.200 the most powerful AI in the world
00:03:53.439 control by someone who is not
00:03:55.040 trustworthy it'd be profoundly
00:03:56.799 unamerican to use political power to the
00:04:01.439 degree that Elon has it to hurt your
00:04:04.959 competitors and Advantage your own
00:04:06.360 businesses they started off as friends
00:04:08.640 and business partners in fact Sam and
00:04:10.720 Elon co-founded openai the company that
00:04:13.200 makes chat GPT in 2015 but over the
00:04:17.079 years Elon Musk grew increasingly
00:04:19.798 frustrated with open AI until he finally
00:04:22.360 resigned from the board in
00:04:24.199 2018 the feud between ultman and musk
00:04:27.400 escalated this past year when Elon sued
00:04:30.280 Sam and open aai on multiple occasions
00:04:33.160 to try and prevent open AI from
00:04:35.520 launching a for-profit arm of the
00:04:37.759 business a structure that Elon claims is
00:04:40.520 not only never supposed to happen in
00:04:42.759 open AI he likes to remind people that a
00:04:46.039 nonprofit transparent company should not
00:04:49.120 become a closed for-profit one but he
00:04:52.240 argues that changing its structure in
00:04:54.280 this way might even be illegal now on
00:04:56.800 the one hand this is a very complex
00:04:58.520 disagreement to understand every single
00:05:01.120 detail of it you probably need a law
00:05:03.360 degree and special expertise in American
00:05:05.759 tax law neither of which I happen to
00:05:07.560 have but you don't need any special
00:05:09.600 degree or specialization to understand
00:05:12.280 that at its heart the feud is about
00:05:14.479 something much bigger and more
00:05:16.240 existential than open ai's business
00:05:18.280 model although that's extremely
00:05:19.639 important and something we discuss today
00:05:21.720 what this is really about I think
00:05:23.759 foundationally is a fight over who will
00:05:26.840 ultimately control this technology and
00:05:29.759 technology that some say if used
00:05:31.960 incorrectly could very well make human
00:05:34.400 beings obsolete so the stakes are low
00:05:37.840 here to tell his side of the story is
00:05:40.000 Sam ultman we talk about where AI is
00:05:42.880 headed why he thinks super intelligence
00:05:45.759 in other words the moment where AI
00:05:47.600 surpasses human capabilities is closer
00:05:50.120 than ever we talk about the Perils of AI
00:05:52.639 bias and censorship why he donated a
00:05:55.280 million dollars to Trump's inaugural
00:05:57.039 fund as a person who had long opposed
00:05:59.280 Trump what happens if America loses the
00:06:01.639 AI race to a foreign power like China
00:06:04.080 and of course what went wrong and is
00:06:06.199 going wrong between him and the richest
00:06:08.520 man on earth we'll be right
00:06:12.660 [Music]
00:06:15.680 back Sam Alman thank you so much for
00:06:18.360 coming on honestly thank you the last
00:06:21.440 time we spoke and I know you've given a
00:06:23.039 zillion interviews since then but it was
00:06:24.599 in April of 2023 and it feels like a
00:06:27.919 world away Chachi PT had just launched
00:06:31.440 and people were just at the very
00:06:33.720 beginning of trying to figure out like
00:06:35.800 in the abstract what this technology was
00:06:38.440 and how it might transform their
00:06:40.000 everyday lives and now sitting here in
00:06:44.319 December of 2024 chat GPT is a household
00:06:47.319 name so is open Ai and of course some of
00:06:49.680 your competitors are too like perplexity
00:06:51.759 and Gemini and Claude average Americans
00:06:54.319 are using these tools every day
00:06:56.599 everything from math tutoring to
00:06:58.240 debugging code to dra emails and it's
00:07:00.479 very very good at doing that tell me
00:07:02.400 about how chat GPT and I guess AI
00:07:05.440 technology more broadly has changed
00:07:07.960 since we last spoke a year and a half
00:07:09.280 ago and whether or not it's where you
00:07:11.520 expected it to be today or further along
00:07:14.080 so I think there's two different things
00:07:15.400 we talk about one is how much the
00:07:16.599 technology itself has changed and that
00:07:18.280 has gotten way better I if you think
00:07:20.240 about the AI we were excited about back
00:07:22.280 in April of 2023 it was so primitive
00:07:25.240 relative to what we have now and the
00:07:26.639 things that the technolog is capable of
00:07:29.120 are
00:07:30.080 pretty mind-blowing to me but even more
00:07:31.840 than that the the rate at which it will
00:07:33.599 continue to get better over the next
00:07:34.879 year and if we came back in another 18
00:07:36.919 months and talked about what it can do I
00:07:39.000 think it'll feel like as big or maybe
00:07:40.520 even bigger as a gap from April 2023 to
00:07:43.440 December of 2024 um as projecting that
00:07:46.680 same amount of time I guess that's more
00:07:48.199 like 20 months going forward the other
00:07:49.919 thing that's happened is it's really
00:07:52.840 integrated into society like back then
00:07:55.400 it was still a curiosity something may
00:07:57.319 people had heard of people really use it
00:07:59.879 a lot for like a lot of their work their
00:08:01.840 personal lives their
00:08:04.240 it's I've never seen a technology become
00:08:06.960 widely adopted this fast not just as
00:08:08.960 something people like dabble with but
00:08:10.159 something that people like really use in
00:08:11.919 all the ways you were talking about so
00:08:15.080 that that part of the adoption curve
00:08:17.759 happened much more quickly than I
00:08:19.039 thought I expected the technology to
00:08:21.120 happen quickly give me a sense of like
00:08:23.599 how are you using the tool that you have
00:08:26.120 helped create in your daily life like
00:08:28.520 the way that most people know we're
00:08:29.759 using it Tyler Cowen and lots of people
00:08:31.720 who are like passionate early adopters
00:08:34.360 it almost seems to have like replaced
00:08:35.799 Google for them and it's just like a
00:08:37.039 much much deeper Google is that how for
00:08:40.360 I use it in all sorts of ways but the
00:08:41.880 newest one um a few months ago we reled
00:08:45.640 we released search integration and now
00:08:48.320 chat GPT can search the internet for
00:08:51.200 kind of real-time information and of
00:08:54.680 everything we've ever shipped that was
00:08:56.399 the one that felt like it doubled my
00:08:57.720 usage all at once
00:09:00.000 and since then I mean I must have still
00:09:02.800 used Google for something but I can't
00:09:06.040 remember what it is wow and I switched
00:09:10.360 chat gbt to be my default search in
00:09:12.760 Chrome uh and I have not looked
00:09:16.920 back the degree to
00:09:20.000 which that behavior changed in me for
00:09:22.360 something that was really deeply
00:09:24.320 ingrained and now the fact that like
00:09:27.360 when I remember the way that I used to
00:09:29.000 search
00:09:30.560 um feels like kind of oh man that was
00:09:33.320 like a pre- iPhone kind of equivalent
00:09:35.320 that's the sort of like level of shift
00:09:37.160 that I feel about it um that's that's
00:09:39.800 been the most surprising change to me in
00:09:42.440 the last few months is that I don't I do
00:09:46.480 all my searching now inside of chat what
00:09:48.320 do you call it do you call it searching
00:09:49.920 or is there a verb in the way that
00:09:51.360 Googling is a verb I still call it
00:09:52.839 search I mean I just like people other
00:09:54.760 people say like I chatted it or I chat
00:09:57.279 whatever people say I chatted it a lot
00:09:58.959 like people seem to just only call it
00:10:00.720 chat but I I I would say I just use
00:10:02.800 search Sam in September so just a few
00:10:05.800 months ago you published this Manifesto
00:10:07.839 on your website predicting the emergence
00:10:10.920 of super intelligence in the next few
00:10:12.880 years or as you put it and memorably in
00:10:15.079 the next few thousand days explain to us
00:10:18.160 what super intelligence is tell us how
00:10:21.920 we'll know if it's actually here and how
00:10:24.600 it stands to change people's lives over
00:10:27.680 the next decades
00:10:30.040 one one thing that I use as a sort of my
00:10:34.000 attempt at my own mental framework for
00:10:35.560 it is the rate of scientific
00:10:38.160 progress um if the rate of scientific
00:10:41.519 progress that's happening in the world
00:10:43.279 as a whole tripled maybe even like 10x
00:10:47.000 you know the discoveries that we used to
00:10:48.480 expect to take 10 years and the
00:10:50.399 technological progress that we used to
00:10:51.920 expect to take 10 years if that happened
00:10:54.519 every year and then we compounded on
00:10:56.399 that the next one and the next one and
00:10:57.880 the next one that me would feel like
00:11:00.839 super intelligence had arrived and it
00:11:03.600 would I think in many ways
00:11:06.399 change the way that Society the economy
00:11:09.519 work it what it won't change and I think
00:11:12.959 a lot of the sort of AI commentators get
00:11:15.800 this wrong is it won't change like the
00:11:17.760 Deep fundamental human drives uh and so
00:11:20.720 in that sense you know we've been
00:11:21.680 through many technological revolutions
00:11:24.079 before things that we tend to care about
00:11:26.480 and uh what what drive all of us
00:11:29.959 I think change very little or maybe not
00:11:32.279 at all through most of those but the
00:11:34.560 world in which we exist will change a
00:11:36.360 lot okay well Sam one of the reasons we
00:11:38.920 wanted to have this conversation with
00:11:40.519 you today is not just because we want to
00:11:42.880 hear about the ways that AI is going to
00:11:44.880 transform the way that we live and work
00:11:47.880 but because you're in a very public
00:11:49.639 battle right now with your original open
00:11:51.800 AI co-founder Elon Musk and I think it's
00:11:54.959 safe to say that most listeners of this
00:11:56.959 show will like vaguely know that there's
00:11:59.440 a conflict between Elon Musk having to
00:12:02.000 do with this one of his companies one of
00:12:05.519 his many companies but there's certainly
00:12:07.320 not following the nitty-gritty details
00:12:09.000 of of the various lawsuits and of and of
00:12:11.320 the conflict more generally so I want to
00:12:13.360 try and summarize it for you in the most
00:12:16.120 Fair way that I can and then you'll tell
00:12:17.839 me if I've gotten it wrong or or where
00:12:20.040 I've where I've overstepped so open AI
00:12:22.199 begins in 2015 and it starts as a
00:12:24.600 nonprofit and in a blog post introducing
00:12:28.040 open AI to the world in December of that
00:12:30.519 year you wrote this open aai is a
00:12:32.920 nonprofit artificial intelligence
00:12:34.959 research company our goal is to advance
00:12:37.399 digital intelligence in the way that is
00:12:39.279 most likely to benefit Humanity as a
00:12:41.399 whole unconstrained by a need to
00:12:43.680 generate Financial return since our
00:12:45.800 research is free from Financial
00:12:47.199 Obligations we can better focus on a
00:12:49.519 positive human impact and this was a
00:12:51.240 huge aspect of the brand then fast
00:12:54.320 forward four years in 2019 open AI moves
00:12:57.519 to what it called a hybrid model with a
00:13:00.000 for-profit arm that got a billion doll
00:13:02.279 investment from Microsoft in that year
00:13:05.160 since then Microsoft has ped something
00:13:07.720 like $1 13 billion it might be a higher
00:13:09.959 number more into the company and Elon
00:13:13.480 was one of the co-founders as I mention
00:13:15.279 since the beginning but his relationship
00:13:17.519 with the company soured over time
00:13:19.760 because he disagreed with the shift that
00:13:22.320 I just described the shift from this
00:13:24.120 nonprofit model to a hybrid model and he
00:13:27.040 eventually leaves the company and steps
00:13:29.199 down from the board and that takes us to
00:13:31.920 this year in which Elon has sued you and
00:13:34.680 openai on several different occasions so
00:13:37.079 far this year and he has gone given many
00:13:39.519 interviews and posted countless amount
00:13:41.839 of tweets or X's or whatever we're
00:13:43.800 supposed to call them about this
00:13:45.600 conflict all of the lawsuits claim that
00:13:48.760 you were in some kind of contract
00:13:50.800 violation by putting profits ahead of
00:13:53.240 the public good in the move to advance
00:13:55.759 Ai and then last month and this is the
00:13:58.399 most recent of vment Elon asked the
00:14:01.000 district judge in California to block
00:14:03.680 open AI from converting to this
00:14:05.839 for-profit structure okay that was a
00:14:08.440 mouthful did I summarize it properly and
00:14:11.399 is there anything crucial that I left
00:14:13.120 out or you summarize it properly but uh
00:14:16.000 I mean it was Elon that most wanted to
00:14:18.240 convert not not even convert it was Elon
00:14:20.320 that Most Wanted open AI to be a for
00:14:23.000 profit at one point and had made a bunch
00:14:24.680 of proposals that would have also things
00:14:28.120 like opening I being part of Tesla but
00:14:30.079 mostly just create a new for-profit that
00:14:31.880 he was going to be in control of and you
00:14:33.959 know so other than that I think a lot of
00:14:36.360 the summary there is is correct I have a
00:14:38.680 bunch of thoughts and opinions on it but
00:14:40.480 as a statement of facts that was
00:14:42.240 otherwise mostly correct give us like
00:14:44.279 the 10,000 foot version what is the
00:14:47.440 fundamental conflict between Elon Musk
00:14:51.320 and his various allies meta being one of
00:14:53.720 them and you guys like what what is the
00:14:55.560 disagreement fundamentally about look
00:14:58.160 I'm I don't live inside elon's head so
00:15:00.360 this is a little bit of of speculation
00:15:04.360 uh Elon definitely did a lot to help
00:15:07.240 open eye in the early days and in spite
00:15:08.759 of all of this I'm very grateful and I
00:15:10.560 think he's just a sort of legendary
00:15:13.279 entrepreneur um he's also clearly a
00:15:15.759 bully and he's also someone who clearly
00:15:17.480 likes to get in fights you know right
00:15:18.880 now it's me it's been Bezos Gates
00:15:21.639 Zuckerberg lots of other people and I
00:15:24.279 think fundamentally this is
00:15:26.480 about opening eyes doing really well
00:15:29.480 Elon cares about doing really well Elon
00:15:33.319 started uh and now runs a very direct
00:15:36.600 competitor that's trying to do exactly
00:15:39.639 what open AI does uh
00:15:42.720 and I'll point out is a structure uh you
00:15:46.399 know as like a public benefit Corp and I
00:15:48.399 heard Elon has majority ownership and
00:15:50.040 control and seems like a reasonable
00:15:52.360 thing he would do I think a lot of the
00:15:54.120 press has been misreported we're not
00:15:56.800 like the non even if we go through with
00:15:59.120 of the any of the conversion ideas or
00:16:02.680 Evolution ideas we're talking about it's
00:16:04.120 like the nonprofit goes away the
00:16:05.720 nonprofit doesn't like stop being
00:16:07.040 nonprofit becomes a for-profit we've
00:16:09.279 talked publicly about maybe we evolve
00:16:11.360 our current LLC into a PBC but anything
00:16:14.720 we do would strengthen the nonprofit the
00:16:16.440 nonprofit would continue to exist would
00:16:18.399 continue to serve hopefully better serve
00:16:21.079 the same purpose and the overall mission
00:16:23.560 of the company that you talked about
00:16:24.839 which is develop this incredible
00:16:26.639 technology do it in a way that we think
00:16:28.600 is maximally beneficial to humans and
00:16:30.959 get it out into the world for people we
00:16:33.160 keep doing that I'm incredibly proud of
00:16:34.920 our track record on doing that so far
00:16:37.160 people as you were saying earlier use
00:16:38.560 chat GPT and love it there's an
00:16:40.199 incredible free tier of chat GPT uh we
00:16:42.959 lose money on it it's not ad supported
00:16:44.360 or anything we just want to put AI in
00:16:45.639 people's hands we continue to want to
00:16:47.720 deploy this technology uh so that people
00:16:51.319 co-evolve with it understand it that the
00:16:53.279 world is going through this process it's
00:16:55.319 going through right now of contending
00:16:56.800 with AI and eventually AGI and thinking
00:16:59.079 how it's going to go and everything
00:17:01.199 we're doing I believe Elon would be
00:17:03.000 happy about if he Wen in control of the
00:17:04.439 company he left when he thought we were
00:17:06.400 like on a trajectory to certainly fail
00:17:08.919 um he and also wouldn't do something
00:17:11.160 where he had like total control over
00:17:12.400 open AI but I think it's like a little
00:17:14.640 bit of a sideshow and the right thing
00:17:16.679 for us to do is just keep doing
00:17:18.559 incredible research keep shipping
00:17:20.039 products people love and and most
00:17:21.720 importantly like keep pursuing this
00:17:23.599 mission of AGI to benefit people and
00:17:26.319 getting that out into the world for
00:17:27.679 someone who's just sort of tuning into
00:17:29.559 this topic why is it important Sam that
00:17:33.400 open AI has a for-profit arm or converts
00:17:37.240 in the way that you've been talking
00:17:38.280 about why why is that essential to your
00:17:40.080 growth when we started openi we thought
00:17:44.080 it's hard to go back and remember how
00:17:45.960 different things were in
00:17:47.480 2015 um that was before language models
00:17:50.360 and chatbots it was way before chat gbt
00:17:52.120 we were doing research and Publishing
00:17:53.520 papers and working on AIS that could
00:17:55.360 play video games and control Rob about
00:17:57.120 of hands and things like that and we we
00:17:59.679 were supposed to get a billion dollars
00:18:01.000 but ended up not we thought with a
00:18:03.120 billion dollars we could make
00:18:04.720 substantial progress towards what we
00:18:06.039 were trying to do as we learned more and
00:18:08.400 got into the scaling language model
00:18:10.080 world we realized that it was not going
00:18:11.679 to cost 1 billion or even 10 but like
00:18:14.000 100 billion plus and we couldn't do that
00:18:16.760 as a nonprofit so that was the
00:18:18.799 fundamental reason for it okay so like
00:18:21.799 it's Bas and maybe another way to say it
00:18:23.520 is like it's absolutely essential for
00:18:25.280 the computational power to create okay
00:18:28.919 every other effort pursuing AI has
00:18:30.600 realized this and has set up in some way
00:18:32.159 where they can sort of Access Capital
00:18:33.880 markets you've said a lot of different
00:18:35.360 things about Elon in recent days you
00:18:36.919 gave this interview at dealbook where
00:18:38.679 Andrew Ros sorin is sort of asking you
00:18:40.760 how you feel about the conflict and you
00:18:42.400 say sad and you also say that you think
00:18:45.200 elon's companies are awesome and then he
00:18:47.520 asked you you know do you think he's
00:18:49.480 going to use his Newfound political
00:18:51.120 influence to kind of punish you or
00:18:53.760 punish open AI or punish his competitors
00:18:56.799 and you said in that interview that you
00:18:58.240 thought he was do the right thing how do
00:19:00.120 you square that with what you just told
00:19:01.559 me which is that elon's a bully bullies
00:19:03.640 don't typically do the right thing I
00:19:05.480 think they can totally be like I think I
00:19:08.280 think there are people who will really
00:19:10.000 be a jerk on Twitter who will still not
00:19:12.880 like abuse the system of a country
00:19:15.000 they're now in a sort of extremely
00:19:17.559 influential political role
00:19:19.480 for that seems completely different to
00:19:22.080 me until now much of this battle you
00:19:25.880 know for those of us who are like
00:19:26.880 perpetually online and perpetually on
00:19:28.280 Twitter we have been following the
00:19:30.080 conflict via like tweets lobed sub
00:19:32.600 tweets it's all sort of been playing out
00:19:35.400 in real time on Twitter for us to watch
00:19:37.919 open AI though has sort of been in like
00:19:40.200 response mode sometimes or mostly kind
00:19:42.760 of ignoring everything that's sort of
00:19:44.720 how i' characterize it that changed a
00:19:46.760 few days ago when you guys published
00:19:49.600 this very very long memo on open it's
00:19:52.440 like a blog post on open ai's website
00:19:54.440 people should go and read it again we'll
00:19:55.799 put it in the show notes and it's sort
00:19:57.280 of like complete it's it's like a
00:19:59.480 timeline going back to
00:20:01.440 2015 proving from your perspective that
00:20:05.200 you know via emails and screenshots of
00:20:08.159 texts and explanations of those
00:20:10.240 screenshots and those texts that Elon
00:20:13.720 wanted open aai to or or Elon rather was
00:20:17.360 open to open AI being a
00:20:20.000 for-profit going all the way back then I
00:20:23.120 read all 29 Pages for those who don't
00:20:26.039 want to do that they could go to chat
00:20:27.360 GPT and ask them to summ asked chat to
00:20:30.080 summarize it here's how chat gbt
00:20:32.200 summarized it this article details the
00:20:34.280 rift between Elon Musk and open AI
00:20:36.280 leadership par particularly Sam Alman
00:20:38.880 stemming from musk's dissatisfaction
00:20:41.000 with open AI shift from a nonprofit to a
00:20:43.559 hybrid for-profit model this Feud is
00:20:45.679 crucial chat told me because it
00:20:47.280 underscores the broader ethical dilemma
00:20:49.600 of how AI should be developed and
00:20:51.240 controlled whether it should prioritize
00:20:53.320 public good or corporate profit
00:20:55.440 especially as powerful AI Technologies
00:20:57.679 become increasingly influential in
00:20:59.320 society in the economy I thought that
00:21:01.520 was pretty good what do you think no but
00:21:03.960 on your general point we you are right
00:21:05.760 that we do not sit there and like throw
00:21:07.400 tomatoes back and forth on Twitter um
00:21:09.720 the reason for this one was we had to
00:21:11.039 make a legal filin and we wanted to
00:21:13.159 provide some context we published about
00:21:14.919 this once before also when we had to
00:21:16.720 make a legal filing I've lost track of
00:21:18.679 how many times that Elon has sued us I
00:21:21.640 think it's like four you know withdraws
00:21:24.039 changes goes for this preliminary
00:21:25.919 injunction whatever our job is to
00:21:29.039 build AGI in a way that benefits
00:21:31.480 humanity and figure out how to safely
00:21:33.039 and broadly distribute it um our job is
00:21:35.559 not to engage in like a Twitter fight
00:21:39.159 with
00:21:40.600 Elon but when we have to respond to
00:21:43.159 Illegal filing we will Pro we will and
00:21:45.640 sometimes we'll provide context I think
00:21:47.480 we've only done this twice in the early
00:21:49.559 days of open aai the brand like the way
00:21:52.240 I encountered the brand of it was
00:21:56.080 transparency and nonprofit like those
00:21:58.919 were the things that it over and over
00:22:01.600 emphasized and the reason you said that
00:22:04.039 you couldn't take any equity and the
00:22:05.480 reason you took such a small salary is
00:22:07.720 because you said I you know I don't want
00:22:09.840 to be conflicted I want to always be
00:22:11.600 motivated to do the thing that's best
00:22:13.159 for Humanity the day after open AI
00:22:15.919 launched in December in 2015 you
00:22:17.919 described it to Vanity Fair as a
00:22:19.679 nonprofit company to save the world from
00:22:22.080 a dystopian future you also said that
00:22:25.200 trying to make open AI a for-profit
00:22:27.640 would lead to quote misaligned
00:22:29.120 incentives that would be suboptimal to
00:22:31.559 the world as a whole I guess I want to
00:22:34.080 ask like do you still agree with that
00:22:35.880 but simply you've had to adapt to the
00:22:38.080 reality which is that developing these
00:22:39.960 models takes billions and billions and
00:22:42.760 billions of dollars two things one I I
00:22:45.080 think I was like a little bit wrong
00:22:46.520 about that um and I have been although I
00:22:49.520 have had
00:22:51.039 concerns um I have been impressed by how
00:22:54.720 much not just us but the other AI Labs
00:22:57.880 even though they have this like wild
00:23:00.240 sort of Market or economic incentive
00:23:03.799 have really been focused on developing
00:23:07.960 safe models I think there's many factors
00:23:10.080 that went into that we did get a little
00:23:11.760 lucky on the direction the technology
00:23:13.880 went but also if you deploy these models
00:23:16.240 in a way that is harmful to people you
00:23:18.159 would like very quickly I believe lose
00:23:19.760 your license to operate if it was an
00:23:21.200 obvious one now there are subtle things
00:23:23.320 that can go wrong like I think social
00:23:24.799 media is an example of a place where
00:23:27.640 maybe the Harms W so obvious at the time
00:23:30.000 and then there was an emergent property
00:23:31.279 at scale and you could imagine something
00:23:32.799 happening but the incentive problem has
00:23:34.760 been better than I thought at the time
00:23:36.760 and I will cheerfully say I was like a
00:23:38.159 little bit naive about how the world
00:23:39.480 Works 10 years ago and I feel better now
00:23:42.120 naive how oh the the the pressure the
00:23:44.880 societal pressure on big companies and
00:23:47.200 the sort of the power of researchers to
00:23:49.520 push their companies to do the right
00:23:50.840 thing even in even in the face of this
00:23:54.360 gigantic profit motive have been pretty
00:23:56.400 good but there is something that I don't
00:23:58.880 feel naive about that I felt at the time
00:24:00.520 too which is it continues to be fairly
00:24:04.760 crazy to me that this is happening in
00:24:08.360 the hands of a small number of private
00:24:09.960 companies to me this feels like the
00:24:12.000 Manhattan Project are the Apollo program
00:24:13.840 of our time and those were not done by
00:24:16.919 private companies and I think is like a
00:24:19.039 mark of a well-functioning society do
00:24:21.360 you think that we need a Manhattan
00:24:22.880 Project here I think the companies are
00:24:24.799 going to do the right thing and it's
00:24:26.679 going to go well and I I don't think
00:24:28.480 government effort in this current world
00:24:30.200 would work at all I don't think it'd be
00:24:31.840 good if it did honestly I just I I wish
00:24:34.000 we were in a world where I said this is
00:24:35.559 you know where I felt like that was the
00:24:36.760 way it should and was happening meta
00:24:38.880 right now is also siding with Elon a few
00:24:41.520 days ago meta asked California's AG to
00:24:44.919 block open AI from becoming a for-profit
00:24:47.360 this is what they said in their letter
00:24:48.880 open ai's conduct could have seismic
00:24:50.880 implications for Silicon Valley if open
00:24:53.000 ai's new business model is valid
00:24:54.840 nonprofit investors would get the same
00:24:56.520 for-profit upside as those invest in the
00:24:59.000 conventional way in for-profit companies
00:25:01.080 while also benefiting from benefiting
00:25:02.960 from the tax writeoffs bestowed by the
00:25:04.799 government this Echoes what musk said
00:25:07.240 last year when he said I'm confused as
00:25:09.600 to how a nonprofit which I donated to
00:25:12.159 somehow became a market cap for profit
00:25:14.360 in other words if this is legal like why
00:25:16.679 isn't everyone doing this I don't know
00:25:18.320 why I Med accept that letter but I do
00:25:19.760 know they know that's not how it works I
00:25:21.520 I know that part's in bad faith if if
00:25:23.679 you in any of these worlds our nonprofit
00:25:26.520 will keep going and the people that
00:25:28.000 invest in the nonprofit uh don't like
00:25:31.200 you don't get to have a benefit from a
00:25:32.640 nonprofit donation ACW to a sort of
00:25:35.399 for-profit equity of course and and they
00:25:37.760 know that too you can imagine lots of
00:25:39.640 other reasons that meta might have sent
00:25:41.200 this letter you can imagine they wanted
00:25:43.360 I mean you can imagine they wanted to
00:25:44.559 Curry favor with Elon you can imagine
00:25:46.240 that they felt like it would help them
00:25:48.960 compete with us um you could imagine
00:25:50.919 that they were like annoyed with us for
00:25:53.679 a perceived anti-open Source stance
00:25:56.520 which I don't think is accurate or
00:25:58.520 something that I feel I don't know you
00:26:00.480 should ask them what the reason was for
00:26:02.039 the civilian who's hearing how does a
00:26:05.320 nonprofit become a for-profit what's the
00:26:07.679 answer it doesn't like the nonprofit
00:26:09.360 stays as the nonprofit I believe that
00:26:10.919 the opening eye nonprofit is on a
00:26:12.640 trajectory I hope if we do well to be
00:26:15.200 the largest and most impactful nonprofit
00:26:17.480 of all time that nonprofit doesn't
00:26:19.360 become anything else like many other
00:26:22.080 things this our world our ecosystem can
00:26:25.760 have a for-profit business also but that
00:26:29.679 doesn't the nonprofit does not convert
00:26:31.240 the nonprofit does not go anywhere the
00:26:32.360 nonprofit does not stop doing nonprofit
00:26:34.120 things at the end of the day Sam who is
00:26:36.399 going to profit most from the success of
00:26:38.399 open AI everyone I'll tell you what I
00:26:40.799 hope everyone gives their analogy for um
00:26:45.679 What technological Revolution this is
00:26:47.320 most like you know it's the Industrial
00:26:48.880 Revolution it's like electricity it's
00:26:51.480 like the
00:26:52.399 web the thing I hope for is that it's
00:26:56.120 like the transistor we discovered a new
00:26:59.399 important fundamental physical law
00:27:02.360 whatever you want to call it um we did a
00:27:05.159 bunch of research so did others and it
00:27:08.799 it will seep into all aspects of the
00:27:12.000 economy products everything and you and
00:27:15.679 I today uh are using many devices with
00:27:19.240 transistors in them to make this podcast
00:27:21.399 possible computer has some your
00:27:23.480 microphone has some the all of the
00:27:27.039 internet equipment between and me has a
00:27:28.840 lot but we don't sit here and think
00:27:31.320 about transistors and the transistor
00:27:35.200 company does not sit here and make all
00:27:37.320 of the the money it is this this new
00:27:39.679 Incredible scientific discovery that's
00:27:42.120 seeped into everything we do and
00:27:44.519 everybody made a lot of
00:27:45.960 money that's what I hope AI will be like
00:27:48.519 and I think there's many reasons why
00:27:50.080 it's the best analogy will you have
00:27:52.039 Equity or do you have Equity or what
00:27:53.880 kind of stake do you have in this new
00:27:56.279 capped for profit well so we haven't
00:27:58.519 formed a new entity yet um we have
00:28:01.919 obviously considered uh forming a new
00:28:04.279 entity or maybe converting our existing
00:28:05.919 ALC into one is more accurate um I have
00:28:09.640 a tiny sliver of equity from a old YC
00:28:13.519 fund um I used to have some via sequa
00:28:16.600 fund but that one turned out to be
00:28:17.760 easier to like sell and not keep the
00:28:20.039 position in um so I have a very small
00:28:22.320 amount that was like quite insignificant
00:28:24.320 to me in terms of what I will or won't
00:28:26.559 have going forward I don't I know it's
00:28:28.679 not like there's no current plan or
00:28:30.480 promise for me to get anything I I will
00:28:32.519 and I if I got anything it would not be
00:28:35.000 there were like outlandish rumors about
00:28:36.799 some number that would not happen do you
00:28:38.880 get why people are fixated on that for
00:28:40.799 sure as I've said many times before if I
00:28:42.840 could go back in time I would have taken
00:28:45.440 Equity I think again I understand more
00:28:47.279 about why my earlier misgivings were
00:28:50.240 misplaced I also get that it's weird for
00:28:52.000 me to take it now after not earlier on
00:28:54.360 the other hand I would love to never
00:28:56.000 have to answer this question again and
00:28:57.519 be like normal company I run it I've got
00:29:00.000 some Equity investors don't have to
00:29:01.679 worry that I'm like misaligned there
00:29:02.960 does the whole like a of Suspicion of
00:29:04.960 not having any is one of the decisions I
00:29:08.000 regret the most of opening eye structure
00:29:10.240 things but I understand why people are
00:29:11.960 fixated on it uh that makes sense if you
00:29:14.159 could go back in time how would you have
00:29:15.679 done this from the beginning like let's
00:29:17.200 wind back the clock to 2015 if an oracle
00:29:19.720 had said to me on what was it November
00:29:22.440 of 2015 before we set out number one
00:29:25.679 you're going to need 100 plus billion
00:29:27.200 dollars number two even though you have
00:29:29.519 no idea today how you're going to ever
00:29:31.279 productize this and you think of
00:29:32.360 yourself as a research lab eventually
00:29:34.120 you're going to become a company that
00:29:35.320 does have way to productize it and
00:29:36.440 business model it so you can explain to
00:29:38.000 investors why they're not just funding a
00:29:40.159 research lab um and number three that
00:29:43.159 the incentives of people working on this
00:29:46.240 are going to be more naturally kept in
00:29:48.519 check because it's not going to be what
00:29:50.720 I and many others thought at the time of
00:29:52.159 like one effort that is way far ahead of
00:29:53.760 everyone else but something more like
00:29:55.480 the transistor that seeps out and so
00:29:57.080 there will be better equilibrium
00:29:59.000 Dynamics if an oracle had told me all
00:30:00.919 three of those things that turned out to
00:30:02.200 be true I would say great let's be a
00:30:04.159 public benefit Corp how essential was
00:30:07.159 Elon to getting open AI off the ground
00:30:09.399 like if if the Oracle also told you
00:30:11.120 about this fight that would ensue with
00:30:12.600 someone that you regarded as your close
00:30:14.720 friend would you have said you know
00:30:17.360 don't need him can do it myself no he
00:30:19.440 was really
00:30:20.519 helpful I'm super appreciative I think
00:30:22.760 it was the first time I ever saw Elon
00:30:24.440 Musk was on stage at a conference you
00:30:26.720 were interviewing him you guys had a
00:30:29.279 wonderful Dynamic you seem like you were
00:30:30.960 really good friends he has said some
00:30:34.159 really harsh things about you he's
00:30:36.080 compared you to Littlefinger in the Game
00:30:37.600 of Thrones most devastatingly said I
00:30:41.000 don't trust him and I don't want the
00:30:42.559 most powerful AI in the world to be
00:30:44.039 controlled by someone who isn't
00:30:45.480 trustworthy why is he saying that I
00:30:48.760 think it's because he wants the most
00:30:50.519 powerful a in the world to be controlled
00:30:51.760 by him and again I've seen elon's
00:30:54.960 attacks to many other people many
00:30:57.279 friends of mine you know everyone gets
00:30:58.720 their period of time in his Spotlight
00:31:01.559 but this all seems like standard
00:31:03.440 behavior from him I'm trying to put
00:31:05.080 myself in a position of a former friend
00:31:07.679 a former co-founder of mine saying those
00:31:10.240 kinds of things about me you you
00:31:13.639 seem relatively calm about it no I'm
00:31:16.480 upset by it for sure I I was talking to
00:31:18.320 someone recently who uh I did think of
00:31:21.039 as close and they said like Elon doesn't
00:31:23.200 have any friends Elon doesn't do peers
00:31:25.000 Elon doesn't do friends that was sort of
00:31:26.760 a sad moment for me um because I do
00:31:29.880 think of him as a friend but
00:31:31.960 I I don't know I can look at this like
00:31:34.200 somewhat dispassionately I remember what
00:31:35.960 it was like when he said opening I has
00:31:37.760 has a 0% chance of success and you know
00:31:39.919 you guys are idiots and I'm ping funing
00:31:41.679 and I'm going to do my own thing there
00:31:42.840 were moments since then where it felt
00:31:44.639 like he kind of wanted to reconcile and
00:31:47.600 figure out a way to work together and I
00:31:48.880 remember moments where he's just like
00:31:51.440 you know off doing his thing on Twitter
00:31:53.960 but if it were only towards me I think
00:31:56.360 it'd be much more painful but you know I
00:31:58.480 think you see who he is on Twitter and
00:32:00.399 so I can like hold it somewhat
00:32:03.159 impersonally and just be like this is
00:32:05.480 about Elon this is not about me it still
00:32:08.559 sucks
00:32:10.360 um I've had a long time to get used to
00:32:12.559 it I guess this recent blog post um the
00:32:16.559 that that went up on open AI site said
00:32:18.840 that Elon should quote be competing in
00:32:20.760 the marketplace rather than in the
00:32:22.320 courtroom and the cynical view of course
00:32:24.559 is to say and you've alluded to this in
00:32:26.240 this conversation that Elon who now owns
00:32:28.679 an open AI competitor himself called xai
00:32:31.639 is suing you not out of some concern
00:32:34.320 over AI safety or anything else but
00:32:36.720 really just to get in on the competition
00:32:39.200 what do you say to that you know is this
00:32:40.679 really is the cynical view true is this
00:32:42.880 really just a fight to be the first to
00:32:44.440 dominate the market or you should ask
00:32:46.320 him I hope yeah I hope to I invited him
00:32:48.840 on great you're not just known as one of
00:32:52.320 the most important AI CEOs AI developers
00:32:55.799 in the world you're also a very very
00:32:57.840 well-known proponent of AI regulation
00:33:00.240 and the cynical view here right is that
00:33:02.639 in the very same way that you could cast
00:33:04.840 dispersions on elon's motives you could
00:33:07.120 look at the way that you have lobbied
00:33:08.760 for AI regulations as a way to stifle
00:33:11.960 competition and benefit your company
00:33:14.120 obviously you've heard that argument
00:33:15.360 before I I think too much regulation
00:33:17.639 clearly has huge negative consequences
00:33:20.639 in society right now and many places we
00:33:22.919 have too much I mean Elon has also been
00:33:25.320 a lot of proponent of calling for AI
00:33:26.840 regulation as have
00:33:28.279 as has the heads of most other large
00:33:32.120 efforts when you step on an airplane you
00:33:34.480 think you you know very high likelihood
00:33:36.639 it's going to be a safe experience when
00:33:38.840 you eat food in the US you don't think
00:33:41.240 too much about food safety uh some
00:33:44.519 regulation is clearly a good thing now I
00:33:46.480 can imagine versions of AI regulation
00:33:48.840 that are really problematic and would
00:33:51.159 disadvantage smaller efforts and I think
00:33:54.159 I think that would
00:33:55.519 be a real mistake
00:33:58.639 um but for some safety guard rails on
00:34:03.480 the most powerful systems that should
00:34:05.559 only affect the people at the frontier
00:34:06.880 that only affect opening eye and a small
00:34:08.199 handful of others I don't think we're at
00:34:09.839 the level yet where these systems have
00:34:12.839 huge safety impli implications but I
00:34:16.079 don't think we're like wildely far away
00:34:17.679 either but the argument that some of
00:34:19.839 these startups are making startups like
00:34:22.719 um there's an AI startup called hugging
00:34:24.399 face which is an unbelievable name um
00:34:26.760 the founder of ay company called
00:34:28.239 stability AI they're basically saying
00:34:31.079 what Sam and the other big guys the
00:34:33.480 incumbents are trying to do open aai
00:34:35.399 Google and apple basically asking
00:34:37.800 government to kind of build a moat
00:34:39.159 around you and stifle the competition
00:34:41.040 through regulatory capture what do you
00:34:42.839 say to those people and this is sort of
00:34:44.480 like the the argument between big Tech
00:34:46.079 and little Tech we can frame it in all
00:34:47.480 kinds of ways what do you say to those
00:34:49.359 people who are saying we want to get in
00:34:51.199 on the competition the regulation that
00:34:53.918 people like Sam and others at many other
00:34:56.040 times are pushing for will hurt us and
00:34:58.599 benefit them well if what they're saying
00:35:00.359 is we're behind opening eyes so it
00:35:01.760 doesn't matter and what we're calling
00:35:03.359 for is only regulation at the frontier
00:35:05.839 like only like only stuff that
00:35:08.520 is new and
00:35:10.640 untested but you know otherwise put out
00:35:12.720 whatever open source model you want I
00:35:14.760 don't think it's reasonable for them to
00:35:16.880 make that argument I I I don't know I'm
00:35:18.760 curious what you think if we if we do
00:35:20.680 let's say we succeed and make a super
00:35:22.880 intelligence you know we make this
00:35:24.280 computer program that is smarter maybe
00:35:27.119 more capable than all of humanity put
00:35:28.960 together do you think there should be
00:35:30.640 any regulation on that at all or just
00:35:32.200 they just say none I definitely think
00:35:33.880 first of all I don't even understand
00:35:35.560 what we're talking about when we talk
00:35:37.040 about super intelligence like you
00:35:39.560 understand what that means and the
00:35:40.599 implications of it in a way that I just
00:35:42.440 don't um so that's number one and number
00:35:45.640 two you know if if this technology is as
00:35:51.160 powerful as people like you and Elon and
00:35:54.640 so many others that are closer to it say
00:35:56.560 that it is of course I think it should
00:35:58.560 be regulated in some way how and when is
00:36:01.359 obviously like the relevant question for
00:36:03.760 sure how and when matters a lot but but
00:36:07.800 uh I agree with that and and I could
00:36:09.920 easily see it going really wrong
00:36:11.800 recently Mark andrion was on this show
00:36:14.200 and he talked to me about his perception
00:36:18.359 of what the Biden Administration was
00:36:20.079 trying to do around AI technology he
00:36:23.079 came on and made the argument and told a
00:36:25.119 story really that he experienced he says
00:36:27.680 that the Biden Administration was trying
00:36:29.599 to sort of completely control Ai and
00:36:32.359 what they were aiming to do was to make
00:36:35.119 it so closely regulated by the
00:36:36.960 government that in his words there would
00:36:39.000 only be sort of two or three big
00:36:40.839 companies that they would work with and
00:36:42.839 that they were trying to ultimately
00:36:44.240 protect them from competition is that
00:36:46.599 true do you know what he's referencing
00:36:48.359 was open AI one of those companies I
00:36:50.240 don't know what he's referencing I also
00:36:51.680 will say very very clearly I think
00:36:54.680 regulation that reduces competition for
00:36:57.079 AI is very bad thing that's so openi was
00:37:00.560 not one of those companies no I don't
00:37:02.920 actually know what that's about but it's
00:37:04.160 we've certainly as far as I know have
00:37:05.599 never you weren't in a room ever with
00:37:07.319 the Biden Administration other AI
00:37:08.720 companies no I don't I don't I don't
00:37:10.920 even think like the Biden Administration
00:37:12.480 is competent enough to I mean we were in
00:37:14.520 a room with them but never and other
00:37:16.720 companies in the administration but
00:37:18.079 never like here's our conspiracy theory
00:37:19.760 we're going to make it only you few
00:37:21.560 companies they can build Ai and then you
00:37:22.800 have to do we say never anything like
00:37:24.160 that what was your feeling in general
00:37:25.920 about the Biden Administrations
00:37:28.040 posture toward Ai and Tech more
00:37:29.599 generally you just you just said like
00:37:31.920 you didn't think they'd have the
00:37:32.800 confidence to I think Gina Rondo was is
00:37:36.640 fantastic you know every conversation I
00:37:38.680 had with her I thought she kind of got
00:37:40.079 it um overall I would say the
00:37:43.079 administration was not that effective uh
00:37:46.200 the things that I would most that I
00:37:48.640 think should have been the
00:37:49.280 administrations priorities and I hope
00:37:50.560 will be the next administration's
00:37:51.720 priorities are building out massive a
00:37:54.560 infrastructure in the US having a supply
00:37:56.880 chain in the US things like that when
00:37:59.000 Mark was on I asked him to kind of
00:38:00.920 Steelman the Biden administration's
00:38:02.839 perspective or Steelman the perspective
00:38:04.359 that this should be heavily regulated
00:38:06.560 and he basically drew the analogy to the
00:38:08.280 Manhattan Project and the development of
00:38:10.079 the atomic bomb when the government felt
00:38:12.839 that it needed to make sure that this
00:38:14.920 new science and Innovation remained
00:38:16.960 classified first of all do you think
00:38:18.839 that that's a good analogy and if so if
00:38:22.240 if it is as powerful as nuclear weapons
00:38:24.839 wouldn't it make sense for this to be
00:38:27.599 not open Ai and Gemini and Claud but
00:38:30.359 rather a project of the federal
00:38:32.560 government I think all the analogies are
00:38:34.280 tough cuz they work in some ways and
00:38:35.720 don't work in other ways like you can
00:38:37.400 you can point to things that are similar
00:38:39.839 to the nuclear era you can talk about
00:38:42.960 like you know it takes enormous
00:38:45.319 resources and huge amounts of energy to
00:38:48.319 enrich uranium on one hand or to produce
00:38:50.359 these models on the other um so you can
00:38:53.119 find things like that that work and then
00:38:55.480 the use of one of these models and the
00:38:56.880 use of a new nulear weapon are like
00:38:58.599 quite different things and sort of the
00:38:59.880 geopolitical implications are also quite
00:39:01.440 different things I think to Steelman the
00:39:03.760 argument of people who say things like
00:39:06.000 it's like nuclear weapons I think what
00:39:08.200 they mean is that it's it's extremely
00:39:10.040 expensive and ex has extreme
00:39:11.720 geopolitical consequences we don't know
00:39:13.920 exactly what those are or how to think
00:39:15.760 about them but because we don't know
00:39:17.359 exactly what they are shouldn't we have
00:39:19.359 like a principle of letting the
00:39:20.480 government decide I can imagine other
00:39:22.720 governments at other times in history
00:39:24.240 where we would have we should be very
00:39:25.920 thrilled about that outcome I think
00:39:27.760 putting the current United States
00:39:29.480 government in charge of developing AGI
00:39:33.560 faster and better than our competitors
00:39:35.720 would not likely go well I think the the
00:39:37.880 kind of the decline in state capacity in
00:39:40.920 this country is not a new observation
00:39:43.200 but a mournful one at the beginning of
00:39:45.520 the nuclear age we had people in this
00:39:48.040 country who functioned almost like Chief
00:39:51.079 science officers right I'm thinking
00:39:52.920 about people like vanav Bush who helped
00:39:55.520 launch the Manhattan Project and came up
00:39:58.200 with a National Science Foundation and
00:40:00.000 kind of guided American policy for those
00:40:04.119 first few like very crucial years of n
00:40:06.760 of of nuclear energy does that person
00:40:09.280 right now whether or not they're in DC
00:40:11.480 or not does that person exist like if we
00:40:14.119 wanted to have someone like that who
00:40:16.680 sort of understood the technology had no
00:40:19.960 Financial stake in it and could talk
00:40:23.319 whether it's President Biden or Trump or
00:40:25.000 whoever comes after him sort of the pros
00:40:27.079 and cons not just of the development of
00:40:29.960 AI here but the competition with China
00:40:33.440 like does that does that person exist
00:40:36.599 actually right now in America like could
00:40:38.640 you be that person arguably I think the
00:40:41.920 the
00:40:43.040 willingness it's coming back a little
00:40:44.920 bit but for a long time the willingness
00:40:46.400 of the American public to be
00:40:49.480 excited about future developments in
00:40:52.880 science and technology has been gone I
00:40:54.680 sort of think it went away with the
00:40:57.960 nuclear weapons actually if I had to
00:40:59.400 pick one moment in time there was sort
00:41:01.240 of a you know a weird like few decade
00:41:03.640 hangover before it there was the
00:41:05.640 generational change when the bomb was
00:41:08.240 dropped kind of got older and in power
00:41:10.920 like I don't think America ever embraced
00:41:14.480 the excitement and belief in science and
00:41:16.480 technology driving the world forward to
00:41:18.560 the same degree as as we used to you
00:41:20.720 read these stories about what people
00:41:22.040 like that used to do and how revered
00:41:23.920 they were and how people believed that
00:41:27.640 scientific technological progress more
00:41:29.440 broadly was going to make the world
00:41:30.720 better um that seems missing now and I
00:41:34.520 don't think it's because we don't have
00:41:35.599 an individual who could do that I think
00:41:37.920 it's because the government doesn't want
00:41:39.760 it and the public doesn't want it I mean
00:41:41.680 what do you make of not just the
00:41:43.680 political Vibe shift but the cultural
00:41:45.560 Vibe shift that we've been experiencing
00:41:47.319 since November 5th like if you made that
00:41:49.079 argument to me 8 weeks ago i' would say
00:41:51.119 yeah Sam's probably right now it feels
00:41:53.040 like a different country there's a huge
00:41:55.079 cultural Vibe shift and I think there's
00:41:56.359 a very positive there's positive
00:41:58.040 momentum in many ways I'm not sure that
00:41:59.880 it exists for hey we think science is
00:42:02.359 really important again and science is
00:42:03.920 what's going to save us and you know
00:42:05.200 solve all of our problems do you think
00:42:06.599 that or do you think it's like that's
00:42:08.240 the one area where I haven't felt it I
00:42:10.319 just think that there's a shift in the
00:42:11.720 direction of growth is a good thing
00:42:15.400 technological progress is a good thing
00:42:17.920 nihilism is feels like it's p and
00:42:20.640 falling out of favor like I feel that
00:42:23.440 change happening in a dramatic way now
00:42:25.880 maybe it's because I spend a lot of time
00:42:28.200 on X and like a lot of it's sort of like
00:42:31.400 fomenting there and sort of Leaping from
00:42:33.760 the online into the real world so you
00:42:36.760 know if if you went and like talked to
00:42:38.280 the average PhD student uptown at
00:42:41.440 Colombia I don't think that they would
00:42:43.240 have the same experience I do because
00:42:45.160 everything's so vulcanized as I said
00:42:46.400 earlier I think it is getting better
00:42:47.880 even on S like I I strongly agree with
00:42:50.520 you on the kind of General shift towards
00:42:53.680 excitement about growth and success and
00:42:57.599 having the country and the economy do
00:42:59.440 well um I some I I do somewhat agree as
00:43:02.480 I was saying earlier that I think even
00:43:05.760 excitement about science is in a better
00:43:08.400 place than it's naugher but I when you
00:43:11.599 talk about those people who were like
00:43:13.000 the scientific ambassadors of the
00:43:14.480 country and who people like really
00:43:16.520 listen to and were excited about and you
00:43:19.559 know preach to a willing
00:43:22.319 audience I'm still not sure I feel that
00:43:26.240 I think there's that excitement for
00:43:28.800 business but not for science well one of
00:43:32.240 the companies that I feel excited about
00:43:35.359 um perhaps it's controversial to say
00:43:37.119 this but I just think the founders is
00:43:38.319 one of the most interesting people in
00:43:39.440 the country is Paul mer lucky and his
00:43:41.680 company and um andal Industries and open
00:43:45.520 AI recently entered into an agreement
00:43:48.160 with andil to develop AI with military
00:43:51.280 applications now previously open AI had
00:43:54.119 had a Prohibition against using its
00:43:56.040 technology for weapon
00:43:57.680 now with the caveat of course that
00:43:58.839 you're concentrating on defensive
00:44:00.280 systems at the moment the sorts of
00:44:02.520 things that could you know guard us
00:44:03.920 against attacks like drone swarms
00:44:06.040 perhaps like what's happening in New
00:44:07.119 Jersey right now we don't have time to
00:44:08.359 talk about that what made you change
00:44:10.040 your mind fundamentally about
00:44:12.079 integrating your company's technology um
00:44:15.280 into even a defensive Weaponry system
00:44:17.960 yeah so we have a set of principles that
00:44:19.839 we established and we approved this one
00:44:21.880 for some use cases that comply with
00:44:23.520 those but I think if the leading United
00:44:26.040 States efforts do not help defend the
00:44:27.800 United States and our allies against our
00:44:30.640 adversaries we're going to be in a very
00:44:32.200 bad place and so we need to figure out
00:44:34.680 how to do that a year and a half ago
00:44:36.480 when we were talking part of our
00:44:38.400 conversation was whether or not we were
00:44:41.440 like where the AI arms race with China
00:44:44.359 was I think now it's like well and
00:44:47.119 definitively clear that we are very much
00:44:49.280 in that arms race with China um and you
00:44:52.880 know I think even people who worry about
00:44:56.200 the power of AI
00:44:58.079 in this country feel like well if it's a
00:45:00.040 choice between us and China it's got to
00:45:02.359 be us we got to win spell out for us Sam
00:45:05.720 in your mind because I'm sure you're
00:45:07.079 thinking about this all the time like
00:45:09.200 what it looks like if China wins the AI
00:45:12.079 arms race like what what happens to
00:45:15.119 America what happens to the world
00:45:17.040 whatever China wants and do you think
00:45:18.760 the possibility of that happening is a
00:45:20.520 real one them winning uh I mean we
00:45:22.240 intend to work our hardest to
00:45:23.640 make sure they don't how do we know if
00:45:25.359 they are winning given how much they lie
00:45:27.760 and also steal stuff from us this is the
00:45:30.000 hard thing right we we know what they
00:45:32.200 publicly release we don't know what they
00:45:33.920 don't publicly release um we have a lot
00:45:36.520 of signals and we have a intelligence
00:45:38.680 system but it's my own stance on this is
00:45:41.559 we have got to try to be
00:45:44.280 cooperative and uh arms races are bad
00:45:46.880 for everybody involved we've learned
00:45:48.160 that lesson again and again throughout
00:45:50.240 history but we need to be able to win if
00:45:53.440 we need to I am hopeful that this can be
00:45:55.760 a great moment for world peace and I
00:45:58.160 believe that if there's ever a time for
00:46:00.480 Humanity to come together this seems
00:46:02.319 like a good candidate and I want us to
00:46:04.119 get there but we can't be naive about
00:46:06.280 that President Trump is talks a lot
00:46:08.119 about you know peace through strength is
00:46:10.760 your is the Sam Alman open AI version of
00:46:13.200 peace through strength we have to crush
00:46:16.160 get ahead and win on AI so it's not even
00:46:18.720 a question that China could do whatever
00:46:21.160 it wants not crushes we have to be ahead
00:46:23.319 and then we have to be as willing to
00:46:25.079 work together as possible and I think
00:46:27.400 that is somewhat similar to peace
00:46:28.760 through strength it's like if there's an
00:46:30.480 arms race we'll win it but we don't want
00:46:32.440 to meaning if there's an arms race we
00:46:34.200 want to win but we don't want the arms
00:46:35.400 race period yeah but well it's not even
00:46:38.559 that it's more like if there's any path
00:46:41.119 towards doing this as a collaborative
00:46:43.680 effort we should but we have to be C we
00:46:46.920 can't control what other entities do you
00:46:50.079 mean collaborate with our enemies we
00:46:52.040 collaborate with China yeah actually
00:46:53.640 I'll say that directly I I I think we
00:46:55.359 collaborate with people we don't get
00:46:56.520 along with all the time in areas where
00:46:58.200 it's in our strategic interest to do so
00:47:00.400 and this is one where I think the
00:47:01.559 interests of the world and certainly the
00:47:03.040 mission of our company would dictate
00:47:05.359 that if it is possible to be truly
00:47:07.400 collaborative we should do that are we
00:47:09.040 doing that right now with China on AI
00:47:11.720 like you know more than I do I was going
00:47:13.280 to say you might know more than I like
00:47:15.200 that that will be a big question for the
00:47:16.720 new Administration but that's not going
00:47:18.079 to happen at the company to company
00:47:19.160 level that's going to happen like the
00:47:20.520 presidents at the two countries level if
00:47:22.280 Trump called you tomorrow and said hey
00:47:24.440 Sam I want to make you like aiar AI
00:47:28.000 regulation Chief you can do whatever you
00:47:30.079 want in this position what's the first
00:47:32.200 thing that you would do what's the most
00:47:33.760 important thing that the person in that
00:47:35.359 position would do us infrastructure and
00:47:37.559 supply chain add a little bit more for
00:47:39.359 people that don't know what that means
00:47:40.839 build our own chips here build enough
00:47:43.200 energy to run data centers here change
00:47:44.920 what it takes to build data centers here
00:47:46.400 but be able to like build the very
00:47:50.079 expensive complex supply chain very
00:47:52.240 expensive infrastructure in the United
00:47:53.680 States bias and censorship in AI is a
00:47:56.920 enormous topic and one that we think a
00:47:59.319 lot about here at the Free Press and you
00:48:01.520 know the most obvious example of this
00:48:03.359 the one that trended for days and
00:48:04.800 everyone was laughing at that was when
00:48:06.480 Gemini generated those images of like
00:48:09.680 black George Washington and like a trans
00:48:11.800 naazi and it was hilarious but in a way
00:48:14.400 it was really serious because it felt
00:48:16.200 like only the most sort of like
00:48:18.000 exaggerated hyperbolic obvious example
00:48:21.359 of a much much deeper endemic problem
00:48:24.559 which is the bias that is baked into
00:48:27.680 these
00:48:29.000 Technologies both because of the people
00:48:31.000 programming those Technologies and
00:48:32.720 because of the information that they're
00:48:34.880 sort of scraping online talk to us about
00:48:38.040 how you're thinking about it at chat GPT
00:48:40.079 because obviously the system that is
00:48:42.760 closest to reality it seems to me will
00:48:45.319 will win in the end of the day if if a
00:48:47.760 chat gbt is giving me images of you know
00:48:50.400 he's telling me George Washington was
00:48:51.720 trans I'm like I'm not going to rely on
00:48:53.200 this we don't do that so okay fine but
00:48:55.359 you understand my point how do you think
00:48:57.559 about the problem of bias and how you're
00:48:59.599 solving for it I think there are two
00:49:01.559 things that matter uh one is what
00:49:04.920 flexibility a user has
00:49:07.319 to get the system to behave the way they
00:49:09.799 want and I think or we think there
00:49:12.319 should be very wide bounds like you know
00:49:14.839 there are some things like you don't
00:49:16.040 want a system to tell you how to create
00:49:18.079 nuclear weapons fine we can all agree on
00:49:19.920 that but if you want a system to be
00:49:22.440 pretty offensive and you ask it to be I
00:49:24.000 think part of alignment is doing what
00:49:25.319 its user asks for for Within These broad
00:49:29.480 bounds that Society agrees on the second
00:49:32.079 thing that really matters is what the
00:49:33.160 defaults are so if you don't do any of
00:49:35.319 that which most users don't and you ask
00:49:38.200 whatever controversial question you want
00:49:40.760 how should the system respond and we put
00:49:43.359 a ton of work into both of those things
00:49:46.280 um we also try to write up how the model
00:49:48.400 should behave we call this the model
00:49:49.599 spec such that you can tell if it's a
00:49:51.680 bug or you disagree with us on some
00:49:53.680 stance is it possible to build a like
00:49:57.040 chat GPT or any other technology in this
00:49:59.799 Lane that we can't even conceive of yet
00:50:02.040 that doesn't have a political point of
00:50:03.559 view isn't that inevitable I think no
00:50:06.359 matter how neutral you try to write the
00:50:07.920 thing it will either be useless because
00:50:09.359 it will just say I can't answer that
00:50:10.680 because there's politics and everything
00:50:12.400 or it will have some sort of point of
00:50:13.680 view which is why what we think we can
00:50:15.520 do is write down what we intend for our
00:50:18.280 default people can debate that if
00:50:20.280 there's bugs in there we can look at the
00:50:21.920 bugs if there's problems with how we
00:50:24.000 defined it we can change what the
00:50:25.160 definition is and retrain the system
00:50:27.240 but yeah I don't think any system can be
00:50:29.559 per no two people are ever going to
00:50:31.040 agree that one system is perfectly
00:50:32.359 unbiased but that's another reason why
00:50:34.160 personalization matters so much do you
00:50:36.040 believe that AI or chat GPT has a
00:50:39.799 responsibility to fight pernicious ideas
00:50:43.040 let me give you an example of what I
00:50:44.200 mean like if you knew that by putting
00:50:47.000 your thumb on the scale in the teeniest
00:50:48.960 tiniest way you might be able to usher
00:50:51.319 in a world where there's less racism
00:50:53.520 less anti-Semitism less misogyny and
00:50:56.280 maybe would even be invisible to people
00:50:58.480 because you know they wouldn't know you
00:51:00.400 know at a certain point as we've just
00:51:01.640 talked about this is going to be you
00:51:03.760 know I don't know if this was Mark or
00:51:04.880 somebody else the control layer of all
00:51:06.839 of our information how do you think
00:51:08.480 about that actually here's one thing
00:51:10.559 I've been thinking about recently as a
00:51:12.000 principal like open AI has not adopted
00:51:14.599 this at all but this has just been an
00:51:15.839 idea that I think gets at what you're
00:51:17.480 saying like let's say let's say we
00:51:20.359 discover some new thing where it's like
00:51:23.640 if you do this people learn way better
00:51:26.640 if chat GPT responds always with the
00:51:30.119 Socratic method or whatever students
00:51:32.000 using it learn way better um but let's
00:51:34.640 say user preferences are not to get the
00:51:37.240 socratic message users just say like I
00:51:38.920 just want you to answer my question then
00:51:40.640 like how should we decide what to do
00:51:43.359 there as the default behavior and one
00:51:46.319 idea that I have increasingly been
00:51:48.680 thinking about
00:51:50.040 is what if we're always just really
00:51:52.240 clear when we make a change to the spec
00:51:54.480 and so you'll never have our thumb the
00:51:56.760 scale hiding behind an algorithm which I
00:51:58.680 think Twitter does all the time for
00:52:00.000 example and all sorts of weird things
00:52:01.319 there like We'll always tell you what
00:52:03.599 the behav what the intended behavior is
00:52:06.319 and if we make a change to it we'll
00:52:07.599 explain why but if we do discover
00:52:10.000 something like what you just said uh or
00:52:12.079 like what I just use as an example and
00:52:13.799 we say okay when people are using it for
00:52:15.720 Education we are going to use the
00:52:17.760 Socratic method um because it does seem
00:52:20.280 to have this measurable effect and
00:52:21.640 here's why we're doing it we can debate
00:52:23.160 that publicly maybe we change our
00:52:24.480 default if you convince us otherwise um
00:52:26.599 um anyone can of course change that in
00:52:28.599 their user preferences because the AI is
00:52:30.119 like a tool for you and should do what
00:52:31.480 you want but I think the thing that
00:52:32.760 would be wrong is if we changed that and
00:52:34.280 didn't reflected in the spec and didn't
00:52:35.680 tell people we were changing it um you
00:52:37.920 know I think the like black box of the
00:52:39.480 Twitter algorithm for example it's like
00:52:42.280 doesn't feel good to me Sam you've
00:52:44.000 donated a million dollars to Trump's
00:52:45.680 inauguration and it turned some heads
00:52:47.720 because in the past you've called him a
00:52:49.000 racist a misogynist and a conspiracy
00:52:50.880 theorist among other things you've been
00:52:53.200 a prolific donor to democratic
00:52:55.599 candidates and causes over the years but
00:52:57.240 now you say that Trump is going to lead
00:52:58.799 us into the age of AI and you're eager
00:53:00.960 to support his efforts to ensure America
00:53:02.920 stays ahead is this a change of heart a
00:53:05.720 political Evolution A vibe shift inside
00:53:07.880 of you what's going on all of those
00:53:09.799 things and also I I hope I mean like
00:53:12.559 he's our president and I wish him every
00:53:14.599 bit of success and anyway we can you
00:53:16.400 know work to support this part of what
00:53:18.040 he wants to do we want to do what's the
00:53:19.720 vibe shift inside of you we know that
00:53:21.280 there's one going on inside Silicon
00:53:23.040 Valley and one going on in the culture
00:53:25.079 how have you changed in the past few
00:53:26.599 years I mean a ton of ways but one one
00:53:29.079 is that I uh you know I've watched for
00:53:31.480 the last maybe 10 12 years as I think
00:53:34.400 things have gotten off track things have
00:53:37.520 been good in some ways but I think
00:53:38.520 gotten really off track in terms of how
00:53:40.559 we think about the importance of
00:53:43.200 growth uh and economic success and a
00:53:46.760 focus on the right things in in the
00:53:48.680 country and in the world more broadly
00:53:50.040 and I think it got it got very off track
00:53:52.799 and I'd say the vibe shift is a hope
00:53:54.400 that as we're facing down one of these
00:53:56.240 most most important moments in
00:53:57.920 technological history um that can help
00:54:00.200 drive a VI A vibe shift back to what I
00:54:02.480 believe in very deeply which is that
00:54:05.280 growth is the only good way forward do
00:54:09.000 you think growth and the growth of open
00:54:10.680 Ai and the growth of AI more generally
00:54:13.200 is a patriotic Duty yes I I actually
00:54:16.200 wrote something like I someone just sent
00:54:17.799 this back to I wrote something more than
00:54:18.920 10 years ago about how growth like I
00:54:21.359 think it's my very first blog post ever
00:54:23.359 about how growth was the central
00:54:24.720 ingredient to democracy working well
00:54:26.880 and I I think the world got badly
00:54:29.640 confused about that and I'm happy to see
00:54:31.079 it re recognized I'm going to use my 30
00:54:33.119 seconds on a lightning round Sam
00:54:34.720 lightning round what are the Drone
00:54:36.200 things what are the flying objects
00:54:37.640 flying over New Jersey right now I have
00:54:39.359 no idea I'm really interested in this
00:54:40.680 question do you know um no we're
00:54:42.720 reporting on it a lot I find it
00:54:44.119 interesting that various electeds are
00:54:46.480 saying it's the Iran Mothership or China
00:54:48.599 do you think Twitter has become better
00:54:50.400 or worse since Elon Musk took control
00:54:52.280 worse you're having a baby will you will
00:54:54.799 you let your kid have an AI friend
00:54:57.160 yes will you let them go on social media
00:54:59.559 at some point will you let them have
00:55:01.079 screen time yes what's your favorite
00:55:03.079 sports car you love sports cars there's
00:55:05.000 a lot of good ones I can't pick one
00:55:07.040 what's your favorite of yours no I can't
00:55:09.280 pick a single favorite I'm mcar f one
00:55:12.000 favorite movie The Dark Knight do you
00:55:13.720 have any normal Hobbies I like have
00:55:15.880 dinner with my friends I go hiking I
00:55:17.799 like you know exercise I just like sit
00:55:20.760 around my friends doing dumb stuff
00:55:23.760 I I don't know yeah it feels pretty
00:55:25.880 normal you built a treehouse recently
00:55:28.079 why did you do that why' you do that um
00:55:30.319 it was Thanksgiving and uh we looking
00:55:32.400 activity for like the adults and the
00:55:33.960 kids that we all thought everybody was
00:55:35.119 at our Ranch and we want an activity we
00:55:36.680 all thought would be fun and was not
00:55:38.240 just sitting around drinking all day and
00:55:39.280 it was great would you box Logan Paul no
00:55:42.400 will we enter World War II in 2025 I
00:55:45.000 hope not what's your New Year's
00:55:46.359 resolution I know what it does Sam Alman
00:55:48.960 thank you so much for coming on honestly
00:55:50.640 thank you
