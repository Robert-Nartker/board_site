# tactiq.io free youtube transcript
# Sam Altman talks with MIT President about AI
# https://www.youtube.com/watch/_Vik6UJTFyk

00:00:05.839 thank you both for being here um they
00:00:08.880 probably neither of them probably need
00:00:10.160 an introduction but of course uh for the
00:00:12.400 record uh Sally corn became mit's 18th
00:00:15.599 president in January 1st 2023 a cell
00:00:18.680 biologist who 8year tenure at duk's
00:00:20.840 University as Provost earned her
00:00:23.000 reputation as a brilliant administrator
00:00:25.320 a creative Problem Solver a leading
00:00:27.760 advocate for faculty excellence and
00:00:29.400 student well being and in her first year
00:00:31.880 of MIT amongst many initiatives she has
00:00:34.399 focused as well on AI and has testified
00:00:37.559 mit's efforts to make sure that AI is
00:00:39.559 broadly beneficial for society uh Sam
00:00:42.440 Alman is an entrepreneur an investor a
00:00:45.000 programmer he co-founded open AI in 2015
00:00:48.520 he is their CEO and open AI is an AI
00:00:52.520 research and deployment company whose
00:00:55.120 mission is to ensure that artificial
00:00:57.079 general intelligence benefits all of
00:00:59.079 humanity Sam and Sally thank you for
00:01:01.640 being here I'm going to turn it over to
00:01:03.039 both of you great thanks so much
00:01:05.319 [Applause]
00:01:11.080 Mark so uh welcome I will say I think
00:01:13.920 this is the most uh popular event I've
00:01:16.159 seen since I arrived at MIT um and I
00:01:19.000 should say that the questions I'm asking
00:01:20.840 were uh submitted by students and sort
00:01:23.600 of gone through and curated but this
00:01:25.720 really reflects the Curiosity of our
00:01:28.000 community here and enthusiasm for you
00:01:30.159 being here so let me start with oh sorry
00:01:33.320 thank you for having me absolutely um so
00:01:36.399 let me let me just dive in so according
00:01:38.960 to columnist Uh Kevin Rose and others in
00:01:41.240 some circles the question is what is
00:01:42.920 your P Doom it's a common Icebreaker or
00:01:45.479 so I'm told most of you probably know
00:01:47.799 that P Doom or probability of Doom is
00:01:50.479 calculated on a scale from 1 to 100 and
00:01:52.560 the higher the score you give the more
00:01:54.759 strongly you believe we end up in a
00:01:56.200 doomsday scenario where AI eliminates
00:01:58.880 all human life so Sam just to break the
00:02:01.320 ice what is your
00:02:03.920 P um I I I think it's sort
00:02:07.920 of the badly formed question um hey I'm
00:02:12.160 glad I didn't take any responsibility I
00:02:14.640 I think it's like a great question for
00:02:16.080 people that uh it's a great way to like
00:02:18.560 sound smart and important and it's like
00:02:20.720 I I have as much fun pontificating on
00:02:22.680 numbers as anybody else but you know
00:02:25.239 whether you say it's 2 or 10 or 20 or 90
00:02:29.640 um the point is it's not zero um and I
00:02:35.040 think another reason I think it's a
00:02:37.000 badly formed question is that it sort of
00:02:38.640 assumes that it's a static system um I
00:02:41.959 think what we need to do is find a way
00:02:44.680 to make the future great make the future
00:02:48.040 exist like not tolerate any uh you know
00:02:51.680 branches of it we just sort of have the
00:02:53.720 the the Doom come into play but um I
00:02:56.760 think Society always
00:03:01.040 holds space for doomsayers there's value
00:03:03.239 to that um I'm happy that they exist I
00:03:06.400 think it makes us think harder about
00:03:08.040 what we're doing but I think the better
00:03:10.360 question is what needs to happen to
00:03:12.640 navigate safety sufficiently well right
00:03:15.920 so be aware as you're developing things
00:03:17.640 of that possibility but not indulge in
00:03:19.840 them too much more than aware like
00:03:21.280 really take confront it and take it
00:03:23.200 extremely seriously yes fair enough so
00:03:26.440 you know this is you've been uh in this
00:03:28.680 business for a little while now how have
00:03:30.040 your views of AI changed over the past
00:03:31.920 decade five or 10 years ago did you
00:03:34.159 expect AI GPT would become as powerful
00:03:37.080 as it
00:03:38.560 has well I honestly still think it's not
00:03:41.799 very good uh I I think we will make it
00:03:44.280 very good but we have a ton of work in
00:03:46.280 front of us
00:03:48.519 uh I think 10 years ago I probably had a
00:03:51.319 more naive conception of AI as this like
00:03:55.159 creature that was going to be off doing
00:03:57.200 stuff or this like you know magic super
00:03:59.840 intelligence in the sky that would like
00:04:02.560 figure things out and Rain money on us
00:04:04.480 and we were going to try to like figure
00:04:05.840 out how to live our lives but it was all
00:04:07.120 sort of confusing and now I think of it
00:04:09.920 much more like any other technological
00:04:12.760 Revolution hopefully the biggest and the
00:04:15.079 best and the most important and the
00:04:16.560 greatest benefits but you know we have
00:04:19.000 like a new tool in the tech tree of
00:04:21.279 humanity and people are using it to
00:04:23.759 create amazing things I think it will
00:04:26.280 continue to get way more capable and way
00:04:28.400 more autonomous over time
00:04:30.320 um but even then like I think it's just
00:04:32.400 going to integrate into society in
00:04:35.840 a in an important and transformative way
00:04:39.240 but something that is somehow going to
00:04:41.639 be you know I think like if AGI got
00:04:44.240 built
00:04:45.199 tomorrow and you asked me what would
00:04:47.080 happen the next day 10 years ago I would
00:04:49.320 have said can't really imagine it it
00:04:51.479 should be just this like absolute
00:04:54.800 transformation Singularity everything is
00:04:56.840 different all at once and I now think it
00:04:59.039 won't really be like that at all so you
00:05:01.320 think in some in some extent you were
00:05:03.720 optimistic but you couldn't have placed
00:05:05.360 yourself in the current moment in either
00:05:07.400 way and it's the same for I I don't know
00:05:09.320 like optimistic or pessimistic I think I
00:05:10.960 was just like somewhat wrong I mean
00:05:13.000 there were ways in which it was too
00:05:14.199 optimistic and too pessimistic at the
00:05:15.759 same time um you know if we make like if
00:05:19.560 we make something that is like you know
00:05:22.720 as smart as all of the super smart
00:05:24.440 students here uh that's a great
00:05:27.280 accomplishment in some sense there's
00:05:29.759 already a lot of like smart people in
00:05:31.000 the world so maybe things go faster um
00:05:33.520 maybe quality of life goes up maybe the
00:05:35.080 economy Cycles a little bit faster but
00:05:38.280 you know like if the rate of scientific
00:05:40.479 discovery becomes 10 times faster than
00:05:42.400 it is
00:05:43.720 today I I don't know how different
00:05:45.759 that'll feel to us living through it at
00:05:47.240 that time oh that's interesting that's
00:05:49.360 interesting so you know just moving
00:05:51.759 stepping back a little bit and looking
00:05:53.319 at the current models thinking about AI
00:05:55.880 systems like je chat gbt um what do you
00:05:59.160 think is NE necessary to remove bias
00:06:01.479 from the systems and can you offer an
00:06:03.120 example of thinking about bias that are
00:06:05.360 in today's AI systems and how we might
00:06:07.280 think about that going
00:06:10.960 forward I I think we've made
00:06:12.720 surprisingly good progress about how we
00:06:14.680 can align the system to behave according
00:06:16.960 to a certain set of values um I think
00:06:19.960 you know for as much as people love to
00:06:21.840 talk about this and say that oh you
00:06:23.440 can't use these things because they're
00:06:24.840 just like spewing toxic waste all the
00:06:27.199 time like if fuse GPT for behaves kind
00:06:30.440 of the way you want it to and reasonably
00:06:32.160 well and you know we're able to get it
00:06:34.759 to follow not perfectly well but better
00:06:37.520 than I at least thought was going to be
00:06:39.479 possible by this point um a given set of
00:06:42.120 values but that that gets to a now
00:06:44.080 harder question which is who decides
00:06:46.479 what what what bias means and what
00:06:49.360 values mean how do we how do we
00:06:51.639 decide what the system is supposed to do
00:06:54.919 uh how much you know how much does
00:06:56.919 society Define broad bounds around the
00:06:58.879 edges versus how much do we say
00:07:02.919 um you as a user like we trust you to
00:07:05.680 use the Tool uh you know not everybody
00:07:07.720 will use it in a way we like but that's
00:07:09.160 kind of the ca the case of tools um I
00:07:12.479 think it's important to give people a
00:07:14.960 lot of control over how they use these
00:07:17.479 tools um and even if that means that
00:07:20.840 they may use them in ways that you or I
00:07:22.759 don't always like um but there are some
00:07:25.280 things that a system just shouldn't do
00:07:27.960 uh and will have to kind of collectively
00:07:31.000 negotiate what those are I mean you know
00:07:33.319 it's interesting thinking about whether
00:07:35.360 you can make the model sort of less
00:07:37.000 biased to than we are as human beings in
00:07:38.840 a sense because you know you talk about
00:07:41.080 things like in you know in medicine uh
00:07:44.360 you know bias against certain
00:07:45.879 demographic groups for instance they're
00:07:48.039 actually trained on the way our human
00:07:49.759 doctors are behaving
00:07:52.479 right they are but then we do this rhf
00:07:55.560 step where we can exert quite a lot of
00:07:58.000 influence uh
00:08:00.080 humans are clearly very biased creatures
00:08:02.919 and often unaware of it and I don't
00:08:04.960 think that GPT 4 or five shares our same
00:08:08.960 psychological exactly flaws probably it
00:08:11.039 has its own different ones um but yeah I
00:08:14.159 think these systems can be way less bias
00:08:15.639 than
00:08:16.319 humans something to strive for you know
00:08:19.319 aside from bias other things that you
00:08:21.159 know have sort of been in part of the
00:08:23.400 public Consciousness in terms of
00:08:25.159 concerns Etc are privacy issues which
00:08:28.520 Loom large for a lot of people when
00:08:30.199 considering uh the future of llms so you
00:08:33.159 know how do we navigate the balance
00:08:34.919 between personal privacy and the need
00:08:36.958 for shared data to train AI
00:08:44.200 models I can imagine this future in
00:08:47.040 which if you want you have a
00:08:49.519 personalized AI that knows that has read
00:08:53.000 every email every text every message
00:08:55.200 you've ever sent or received has an
00:08:57.440 access to a full recording of your life
00:08:59.880 um knows every document you've ever
00:09:01.600 looked at every TV show you've ever read
00:09:03.360 every everything you've ever said or
00:09:05.279 heard or seen like all of your bits of
00:09:07.240 input in and out
00:09:10.240 and you can imagine that that would be a
00:09:12.600 super helpful thing to have you can also
00:09:15.160 Imagine the privacy concerns that that
00:09:17.560 would present and I think if we stick on
00:09:20.800 that frame and not say well should you
00:09:22.560 know AI be able to train on this data or
00:09:24.240 to that data but how are we going to
00:09:25.800 navigate the Privacy versus utility
00:09:29.079 versus safety tradeoffs or security
00:09:31.440 tradeoffs that come with that um and
00:09:33.880 like what does it even mean like do we
00:09:35.519 need a new definition of like privileged
00:09:38.079 information so that your AI companion
00:09:40.120 never has to like testify against you or
00:09:41.760 can't be subpoena by a court I don't
00:09:43.000 even know what right the problems are
00:09:44.959 going to be um but this question of
00:09:48.200 where we all will individually set the
00:09:51.279 Privacy versus utility tradeoffs and the
00:09:54.160 advantages that will be possible for
00:09:56.360 someone to have if you say I am going to
00:09:57.959 let this thing train on my entire life
00:10:00.120 um that's like a new thing for society
00:10:02.040 to navigate I don't know what the
00:10:04.079 answers will be I don't know where most
00:10:05.200 people will make the tradeoffs I don't
00:10:07.000 know what we'll say or like is even
00:10:08.720 permissible in the bounds um but we
00:10:13.120 faced a little bit of that before about
00:10:14.720 where we trade off some privacy for
00:10:16.360 utility with the services we all
00:10:19.240 use but that can go so incredibly far
00:10:22.839 with AI there are all these things that
00:10:24.720 we've had to negotiate with the internet
00:10:26.720 things about how we think about privacy
00:10:28.640 how we think about online ads um that
00:10:31.320 when you intersect them with AI become
00:10:34.320 much higher stakes and much bigger
00:10:35.920 trade-offs uh that I think we're going
00:10:37.560 to start really facing yeah I know
00:10:39.279 that's interesting in terms of um also
00:10:42.600 how much individual control there's
00:10:44.600 exerted and other words when you're
00:10:46.200 talking about aggregated data you know
00:10:47.880 good example when you're talk about
00:10:49.120 higher Stakes again is you know sort of
00:10:51.120 health record health record data Etc and
00:10:54.279 you know how much we can build into it
00:10:56.000 some sort of personal ability to sort of
00:10:57.680 set that sliding scale uh with how much
00:11:00.399 information you're willing to have as
00:11:01.760 part of that training I think in that
00:11:04.160 case we are a little
00:11:06.360 bit you know a little bit off in terms
00:11:09.000 of how we have the conversation um what
00:11:11.200 what you want out of GPT 5 or six or
00:11:13.680 whatever is for it to be the best
00:11:15.639 reasoning engine possible um it is true
00:11:19.680 that right now the way the only way we
00:11:22.440 currently know how to do that is by
00:11:24.279 training on tons and tons of data and in
00:11:26.760 the process of that it is learning
00:11:28.880 something about how to do very very
00:11:31.120 limited reasoning or cognition or
00:11:33.240 whatever you want to call it but the
00:11:35.519 fact that it can memorize data or the
00:11:37.959 fact that it's storing data at all in
00:11:39.760 its parameter space I think we'll look
00:11:42.000 back and say that was kind of like a
00:11:43.920 weird waste of resources like it is true
00:11:48.040 that gp4 can kind of act like a database
00:11:51.160 but barely it's slow it's expensive it
00:11:53.880 doesn't work very well it's not it's not
00:11:56.120 really what you want um it's just kind
00:11:58.279 of as a side effect of the only way we
00:12:00.600 know how to make a model that a
00:12:02.120 reasoning engine right now um it has all
00:12:04.880 these other properties but I assume at
00:12:07.360 some point we'll figure out how to sort
00:12:09.399 of separate the reasoning engine n from
00:12:14.079 all of the uh you know needing tons of
00:12:17.399 data or storing the data in there and
00:12:18.839 we'll be able to treat them as sort of
00:12:21.040 separate things and I think that'll make
00:12:22.360 some of these privacy issues easier no
00:12:24.320 that makes a lot of sense um speaking
00:12:26.720 about openness there's been a
00:12:28.160 considerable discussion of whether open
00:12:30.040 AI is actually open you said that while
00:12:33.040 it may not be completely open source
00:12:34.760 it's open in other ways can you say a
00:12:36.760 little bit more about this and how you
00:12:37.959 think about it um we make a great free
00:12:42.800 AI tool available hundreds of millions
00:12:45.240 of people hopefully billions of people
00:12:46.440 will use it in the future uh we don't
00:12:49.680 run ads we just do this as like a public
00:12:52.199 good because we think it's important to
00:12:53.560 put the tool in people's hands um and we
00:12:56.120 want it to be very widely available very
00:12:58.079 easy to use very helpful
00:13:00.000 um I think that's just a cool thing it
00:13:04.560 is a cool
00:13:09.519 thing you know it is funny how um how it
00:13:13.680 starts as you're talking about how AI
00:13:15.240 gets built into a or our normal lives
00:13:17.160 and that'll evolve I mean I think
00:13:18.720 probably most of us remember the first
00:13:20.600 time we saw chat GPT and we're like oh
00:13:23.399 my God that is so cool now we're trying
00:13:25.920 to think about what the next Generations
00:13:27.760 you know what are the next generation of
00:13:29.120 coolness going to be by the way I think
00:13:31.240 that's great I think it's awesome that
00:13:34.120 for um you know 2 weeks everybody was
00:13:37.760 freaking out about gp4 and thought it
00:13:39.279 was super cool and then by the third
00:13:41.320 week everyone was like exactly come on
00:13:43.040 where's GPT 5 I'm tired of waiting EXA
00:13:45.279 exactly I I actually think that says
00:13:47.279 something like legitimately great about
00:13:49.279 human expectation and striving and why
00:13:51.360 we all have to like continue to make
00:13:52.639 things better um and I think it's like
00:13:55.199 great that a baby born today uh will
00:13:58.199 never know a world in which the products
00:14:00.880 and services they use are not
00:14:02.519 intelligent we'll never know a world in
00:14:04.000 which cognition is not like abundant and
00:14:07.120 part of everything that you use so I
00:14:10.040 think this like this human discontent
00:14:12.800 with the state of things and the
00:14:14.279 expectation that the world should get
00:14:17.440 better every year uh I think that's
00:14:20.360 awesome yeah no great I agree um so in
00:14:23.800 the past year uh new electricity
00:14:26.079 speaking of less exciting edge of things
00:14:28.320 the new electricity demand from Ai and
00:14:30.600 data centers has been cited as an
00:14:32.600 environmental concern at the same time
00:14:34.800 many you know talk about AI assisting
00:14:36.759 and decarbonization what are your
00:14:38.759 thoughts about this the tension between
00:14:40.920 its effect on climates climate its
00:14:43.720 ability to potentially help us uh fight
00:14:46.240 the impact of climate change uh as it
00:14:48.519 moves
00:14:51.240 forward I'll answer that specifically
00:14:53.440 and a more General observation um it is
00:14:56.800 true that AI needs a huge amount of
00:14:59.920 energy but not huge relative to what the
00:15:02.199 rest of the world needs if we have to
00:15:04.600 spend and I I don't even think you know
00:15:07.399 if we spent 1% of the world's
00:15:09.600 electricity training powerful Ai and
00:15:12.519 that AI helped us figure out how to get
00:15:14.839 to uh non-carbon based energy or do
00:15:17.440 carbon capture better that would be a
00:15:19.360 massive win um and even if we didn't do
00:15:22.079 that uh if that 1% of compute that we
00:15:26.240 spent on AI let people live their lives
00:15:28.680 better and have to
00:15:30.399 like yeah I read this thing about the
00:15:32.480 compute Google used once compared to the
00:15:35.240 amount of carbon that people used to
00:15:37.000 spend driving in their cars places to
00:15:39.040 get information and you know you have
00:15:41.639 people saying Google's so horrible we
00:15:43.160 should shut it down it's like spending
00:15:44.519 El energy and it's it was intellectually
00:15:46.920 a very dishonest thing to say because it
00:15:48.680 was a net Savings in energy uh I think
00:15:51.519 pretty clearly the internet in general
00:15:53.680 and the you know what it lets us do for
00:15:55.800 telecommuting probably also a savings um
00:15:59.040 so you know for like AI yeah it's going
00:16:01.360 to need a lot of energy we're going to
00:16:03.040 keep figuring out way more efficient
00:16:04.440 algorithms way more efficient chips
00:16:06.199 we're going to get Fusion we're going to
00:16:07.720 power the stuff this way um so I think
00:16:10.800 it is like important to address this
00:16:13.959 issue um but we will in all of these
00:16:17.440 fantastic
00:16:18.600 ways but I think this points to
00:16:20.680 something else which is the you know you
00:16:22.560 open asking about P doom and the level
00:16:26.319 of doomeris in society right now I think
00:16:30.240 the way we are teaching our young people
00:16:32.079 that the world is totally screwed that
00:16:33.880 it's hopeless to try to solve problems
00:16:35.759 that all we can do is like sit in our
00:16:37.319 bedrooms in the dark and think about how
00:16:38.759 awful we are is a really deeply
00:16:41.040 unproductive streak and I hope MIT is
00:16:44.360 different than a lot of other college
00:16:45.639 campuses I assume it is but you all need
00:16:48.160 to like make it part of your life
00:16:49.920 mission to fight against this Prosperity
00:16:52.920 abundance um you know a better life next
00:16:55.560 year a better life for our children that
00:16:57.240 is the only path forward that is the
00:16:58.480 only way to have a functioning society
00:17:00.680 and there will always be people who want
00:17:01.959 to sit around and say we shouldn't do AI
00:17:04.319 because we may burn a little more carbon
00:17:05.959 or we shouldn't do AI because you know
00:17:07.640 we haven't fully addressed bias and it
00:17:09.919 turns out a couple years later we made a
00:17:11.199 lot of progress on both of those things
00:17:13.199 and the anti-progress streak the anti
00:17:15.679 likee people deserve a great life streak
00:17:17.959 who are usually the people that have
00:17:19.640 quite a lot of privilege in the first
00:17:20.839 place um is something I hope you all
00:17:22.520 fight against God yeah
00:17:27.530 [Applause]
00:17:30.039 you know I I uh you know coming to MIT
00:17:33.240 fairly fresh from the outside I think
00:17:34.720 this is sort of core to the MIT ethos
00:17:36.679 which is you know naming the problems
00:17:38.080 and figuring out a way to solve them I'm
00:17:39.760 very happy to be here yeah it's
00:17:41.320 fantastic we're very happy you are here
00:17:43.200 the other thing you said that really
00:17:44.880 struck me as we think about the costs of
00:17:46.720 AI I don't just mean monetary costs
00:17:48.280 whether it's climate or anything else it
00:17:50.120 is this notion of
00:17:51.880 deducting what the uh what AI can
00:17:55.080 contribute to the problem as not a c you
00:17:57.440 know what I mean as sort of a long-term
00:17:58.840 term uh balancing of The Ledger that I
00:18:01.000 think is important yeah so that's
00:18:03.159 interesting um does open AI intend to
00:18:05.799 build tools that will specifically
00:18:07.520 impact science and engineering or will
00:18:10.120 you be more focused on sort of business
00:18:12.080 and consumer
00:18:13.559 applications um for sure we intend to do
00:18:17.480 that I think the most like personally
00:18:19.919 the thing I am most most interested in
00:18:22.240 is how we use AI to increase the rate of
00:18:25.600 scientific discovery I believe that is
00:18:27.480 the core engine of human progress and
00:18:30.919 that it is the only way we drive the
00:18:32.400 sustainable economic growth that we were
00:18:33.919 talking about earlier people aren't
00:18:35.520 content with gp4 they want gp5 they want
00:18:38.000 things to get better um everyone wants
00:18:40.320 like more and better and faster uh and
00:18:43.440 science is how we get there so of all of
00:18:46.159 the things of all the great things that
00:18:47.559 AI will do um I am personally most
00:18:51.280 passionate about the impact that I hope
00:18:53.559 it will have expect it will have on
00:18:55.919 science that said um it may this may all
00:18:59.799 be more one-dimensional than we think um
00:19:02.240 if we make a great AI tool that can help
00:19:06.600 people solve any kind of problem in
00:19:08.679 front of them that can help people
00:19:10.039 reason in new ways uh that's great for
00:19:12.960 consumers that's great for scientists
00:19:14.240 that's great for businesses it's great
00:19:15.400 for Education it it may the the G of AGI
00:19:19.840 the general is sort of the surprising
00:19:21.360 piece yeah I know that's really
00:19:22.880 interesting and now I come back to your
00:19:24.520 comment about you know sort of getting
00:19:26.360 in your old car and driving to the
00:19:27.720 library you know you know a lot of that
00:19:30.120 the the creativity part is still human
00:19:32.039 but a lot of the um aggregating all of
00:19:34.280 the knowledge that you that you can use
00:19:36.480 as a launching Point can really be
00:19:38.919 expedited you know by asking a few key
00:19:41.679 questions uh to AI up front totally I
00:19:44.840 mean again the the what any one
00:19:47.360 individual and certainly what any group
00:19:49.200 of us will be capable of um I think it's
00:19:53.480 going to if we could go see what each of
00:19:56.799 us can do 10 or 20 years in the future I
00:19:58.480 think it would astonish us today yeah um
00:20:01.039 if you know it's like maybe in a few
00:20:02.960 years it's like each of us has like a
00:20:05.280 great Chief of Staff or uh like PhD
00:20:08.400 student or whatever analogy you want
00:20:10.159 that's off like helping us optimize
00:20:11.960 ourselves and do our best work and our
00:20:14.960 best ideas and whatever and then maybe
00:20:16.960 at some point it's like each of us has
00:20:18.600 like a full company full of like
00:20:20.320 brilliant Experts of anything um just
00:20:23.480 working super productively together cool
00:20:26.480 so you know what do you have what advice
00:20:28.320 do you have for we have a lot of young
00:20:30.640 researchers or people who are aspiring
00:20:32.520 to be young researchers in the audience
00:20:34.480 uh what's sort of your general advice uh
00:20:36.679 for making a real impact in the world
00:20:38.280 and you know you alluded to it in terms
00:20:39.840 of thinking about possibilities and not
00:20:42.159 sitting in your bedroom in the dark I
00:20:44.039 think that's a good base recommendation
00:20:46.440 um but I'm just wondering what else uh
00:20:48.640 what else you might want to say to this
00:20:50.159 audience about that um first of all I
00:20:54.240 think this is probably the most exciting
00:20:57.520 time to be launching your career um in
00:21:01.280 many decades maybe ever I don't know but
00:21:03.280 it's like whatever it is it's a really
00:21:04.520 big deal and the fact that you have this
00:21:05.760 huge Tailwind means I think you can um I
00:21:09.799 think you can take more risk than usual
00:21:11.320 I think if you do something doesn't work
00:21:12.480 out there's just going to be phenomenal
00:21:13.799 opportunities for a long time uh I think
00:21:16.000 you have you can have more impact than
00:21:17.880 normal and so there's like a premium on
00:21:20.480 you know having this be a period where
00:21:21.960 you work really hard I certainly would
00:21:23.720 be biased to do something with AI um but
00:21:28.880 like of course I'm going to say that so
00:21:30.000 maybe it's wrong
00:21:33.120 um I think in general the the the kind
00:21:35.960 of
00:21:38.039 core the the most important to lesson to
00:21:41.080 learn um early on in your career is that
00:21:43.799 you can kind of figure anything out um
00:21:46.919 and that no one has all of the answers
00:21:49.360 when they start out but you just sort of
00:21:51.559 like stumble your way through it have
00:21:54.120 like a fast iteration speed try to like
00:21:56.640 drift towards the most
00:21:59.039 interesting problems to you and be
00:22:00.600 around the most impressive people and
00:22:03.559 have this like trust that you'll
00:22:06.400 successively iterate to the right thing
00:22:08.360 and and you can kind of like you can do
00:22:10.640 more than you think faster than you
00:22:12.159 think uh and people it takes a while to
00:22:15.559 learn that lesson um but it it it it
00:22:19.840 gets you know you see it work a few
00:22:22.600 times and you really start to trust it
00:22:24.960 um and so like you can just do stuff
00:22:28.600 sounds like not real advice or like very
00:22:31.039 empty advice but I think is like it's
00:22:33.919 it's much more profound than it sounds
00:22:35.440 on the surface the other thing I would
00:22:37.400 say is figuring
00:22:39.480 out relatively early on and this takes
00:22:41.919 some practice kind of like what your own
00:22:45.559 personal I don't even know what to call
00:22:47.480 it like passion missionstatement like
00:22:50.440 the kind of way you want to spend your
00:22:52.039 time or what you really care about um
00:22:55.240 and we talked about like this concept of
00:22:58.039 like techno abundance as a way to
00:23:01.440 drive um like prosperity and better
00:23:05.039 lives for people that that's been
00:23:06.320 something for me that has always really
00:23:08.320 resonated and I've always tried to
00:23:09.520 figure out like how to work on that but
00:23:12.000 having some sort of like letting letting
00:23:13.960 yourself develop some sort of like
00:23:15.799 guiding principle of how you make
00:23:17.480 decisions about how to allocate your
00:23:18.880 time and where to try to like steer
00:23:21.760 things that that was like that's been
00:23:23.799 very helpful to me yeah I think this
00:23:25.960 follow follow your passion and also you
00:23:27.760 know from you're painting a picture of a
00:23:29.480 world where there's sort of infinite
00:23:31.320 possibilities and you know doing you
00:23:34.440 can't always be so strategic about what
00:23:36.080 you think is good for you to do you want
00:23:38.080 to do something that imagines all those
00:23:40.400 possibilities and follows those passions
00:23:43.159 it yeah for me it's like like passion is
00:23:46.760 not
00:23:47.919 quite the right word it's like something
00:23:50.520 closer to like what is the moral
00:23:53.799 obligation for me to work on and then
00:23:56.799 and I on like the really bad days is
00:23:58.520 when I'm not having fun that's somehow
00:24:00.120 like it's much more motivating than just
00:24:02.320 the thing I like doing the most that's
00:24:05.120 interesting um another element of this
00:24:08.520 and this is actually a really core part
00:24:10.159 of uh mit's culture is entrepreneurship
00:24:12.880 and so we have a lot of aspiring
00:24:14.640 entrepreneurs so there's you know
00:24:17.159 developing the sort of underlying ideas
00:24:19.360 but how do you think about building um
00:24:21.880 successful companies in today's
00:24:23.640 ecosystem and what part of the value
00:24:25.200 chain should new where where should new
00:24:26.880 startups sort of focus their effort
00:24:32.159 again I think this is like the best time
00:24:34.840 for new startups in particular in a very
00:24:37.720 long time uh startups tend to succeed uh
00:24:40.919 right around the time of big platform
00:24:42.279 shifts big companies are slower and less
00:24:46.080 Innovative than startups but they have a
00:24:48.919 lot of other advantages um when you get
00:24:52.440 the speed and iteration and cycle time
00:24:54.520 Advantage the most is when like the
00:24:57.559 ground is shaking
00:24:59.120 um and right now I think you can there
00:25:02.360 was like a moment like this right when
00:25:03.640 the internet happened there was a Moment
00:25:05.840 Like This although smaller after mobile
00:25:08.000 uh there was another moment also smaller
00:25:09.720 after AWS and this idea of like cloud
00:25:12.360 services and then for a very long time
00:25:15.679 like more than a decade we've just been
00:25:18.600 sort of waiting and I think now we
00:25:20.960 finally have a new platform and so if
00:25:23.720 history is a guide which usually it is
00:25:25.320 and I suspect it will be this time um
00:25:28.399 it's an amazing time to start a company
00:25:30.960 and the advantages you have as a company
00:25:33.600 are you can move much faster you can
00:25:37.200 like live in the future more than like
00:25:39.399 big companies that have quarterly or
00:25:41.080 annual or whatever they have planning
00:25:42.640 Cycles um and that's how you win and I
00:25:45.919 think this is a great time to do it
00:25:48.000 excellent I think there's a lot of
00:25:49.120 people here who uh are take that to
00:25:51.760 heart can I say one more thing about
00:25:53.320 that um I will issue like a that was all
00:25:55.360 the positive um here
00:25:59.120 here's a warning
00:26:01.559 um with any new tech platform you can
00:26:06.000 always drive phenomenal short-term
00:26:08.320 growth and so you have this class of AI
00:26:11.720 startups like you used to have a class
00:26:13.000 of mobile startups and before that like
00:26:14.399 you used to have a class of Internet
00:26:15.559 startups that were not building an
00:26:17.360 enduring business um but instead we're
00:26:19.799 building this sort of like novelty thing
00:26:22.880 that was and be and you kind of delude
00:26:25.760 yourself because you get amazing fast
00:26:27.679 growth
00:26:28.799 and because there's this like magic new
00:26:30.880 technology and you know the dust hasn't
00:26:33.120 settled yet just because there is a
00:26:35.600 magic new technology it does not excuse
00:26:37.720 you from like the laws of physics of a
00:26:40.000 business you still have to figure out a
00:26:42.080 way to build um some sort of switching
00:26:44.960 cost some sort of relationship with
00:26:46.360 customers some sort of compounding
00:26:49.960 advantage over time and in
00:26:54.760 the in the Gold Rush moments I think
00:26:58.279 startups at their Peril often forget
00:27:00.360 that so you still have to like do all
00:27:02.440 things a business always has to do yeah
00:27:04.480 that's really that's I think important
00:27:07.039 important advice um you know the other
00:27:10.120 thing you know it's interesting how this
00:27:11.440 question was phrased and I'll read it in
00:27:13.279 a minute but now that I've heard you
00:27:14.960 talk a little bit I might phrase it a
00:27:16.559 little bit differently the question was
00:27:18.000 phrased in what ways might technology
00:27:20.159 like chat GPT threaten versus help the
00:27:22.960 future of work but uh it sounds like you
00:27:26.640 you tilt much more towards help but also
00:27:29.120 thinking about you know what that means
00:27:30.799 in in real terms how does it how does it
00:27:32.919 help people in their future employment
00:27:35.320 one of the things that annoys me most
00:27:37.440 about people who work on AI is when they
00:27:39.200 stand up and with a straight face say oh
00:27:40.960 this will never cause any job
00:27:42.120 elimination yeah you know this is just
00:27:43.960 an additive thing this is just going to
00:27:45.880 it's all going to be great like this is
00:27:47.320 going to eliminate a lot of current jobs
00:27:49.559 and this is going to
00:27:51.840 change the way that a lot of current
00:27:54.320 jobs function and this is going to
00:27:56.799 create entirely new jobs
00:28:00.320 it that always happens with technology
00:28:03.080 um it's probably never happened this
00:28:05.120 fast although again we may be like
00:28:06.559 drinking the Kool-Aid too much and the
00:28:08.399 inertia of society may be such that it's
00:28:10.159 slower than we think
00:28:13.000 but I kind of expect we're only a
00:28:16.559 generation or two away from models that
00:28:19.480 for the first time show some degree of
00:28:21.760 real economic impact good and bad um but
00:28:25.360 something you can measure and there will
00:28:28.360 be classes of jobs that totally go away
00:28:30.760 there will be classes of jobs where you
00:28:32.720 have to change what you do a lot there
00:28:34.360 will be classes of jobs where the
00:28:35.799 productivity compensation whatever you
00:28:37.360 want to talk about whatever measure goes
00:28:39.120 up by like a giant Factor um and then
00:28:43.159 there will be things that feel like jobs
00:28:45.960 to the people of the future that to us
00:28:48.240 today look like a complete Indulgence
00:28:51.159 and waste of time as what many of us do
00:28:54.039 today would look like to people from
00:28:55.880 hundreds of years ago uh
00:29:00.600 I think as long as you believe that
00:29:02.880 humans very deeply want
00:29:05.559 to create and be useful and feel like
00:29:10.519 they're making like relative
00:29:12.399 differential progress all of which are
00:29:14.279 things I would bet on hard um we're not
00:29:17.080 going to run out of things to do
00:29:20.159 um I love reading contemporaneous
00:29:23.279 accounts from people living through
00:29:24.840 previous technological revolutions and
00:29:27.320 what they say about man we're all going
00:29:28.799 to only work four hours a week if we
00:29:30.640 work at all and we're just going to you
00:29:32.519 know it like you say it every time
00:29:36.559 it in some way it does feel to me like
00:29:39.399 this time is
00:29:41.440 different and as a matter of degree it
00:29:44.120 might be and as a matter of speed I
00:29:46.240 really think it will be um and I have
00:29:49.279 some concern about how quickly we can
00:29:50.799 adapt to this kind of
00:29:52.279 change but I have no real concern that
00:29:54.760 we can eventually adapt to this kind of
00:29:56.360 change um I'm sure the social contract
00:29:59.120 will change um I'm
00:30:01.720 sure most jobs will be different in the
00:30:04.480 future than they are today but like the
00:30:08.080 Deep human drivers don't seem to me
00:30:10.679 likely to go anywhere interesting and
00:30:13.240 and obviously different categories of
00:30:14.600 jobs are going to be really
00:30:15.440 differentially affected for sure um so
00:30:19.080 uh with President Biden's recent
00:30:20.480 executive order on AI as well as the
00:30:22.360 Congressional hearings on AI regulation
00:30:25.080 um there is a concern that regulatory
00:30:26.840 Frameworks might solidify the position
00:30:29.000 of established players might stifle
00:30:31.240 Innovation competition accessibility how
00:30:33.760 do you envision AI be regulation because
00:30:36.399 we really are at a critical moment being
00:30:38.039 designed to uphold Innovation and
00:30:39.760 competition while ensuring that the
00:30:41.600 field remains accessible for merging uh
00:30:44.399 players to Pioneer the next
00:30:45.840 transformative
00:30:48.159 Technologies um I I think we faced
00:30:50.880 versions of this with other kinds of
00:30:53.760 Regulation like you want to know that
00:30:56.840 the food you buy in a g grocery store is
00:30:59.720 unlikely to make you sick and we kind of
00:31:01.840 all agree that regulation there is good
00:31:04.639 but you also want to be able to like
00:31:05.679 grow food in your backyard without
00:31:07.039 having to like go through a bunch of
00:31:09.519 like hoops and you get to do that too
00:31:14.360 um I think for AI systems there will be
00:31:18.039 some threshold above which we say okay
00:31:22.159 uh the system presents a level of risk
00:31:26.000 that we don't want to take without
00:31:29.200 reasonable safety precautions and then I
00:31:31.880 think there will be a level of AI
00:31:33.279 systems where we say even though there's
00:31:34.600 going to be misuse um we should open
00:31:38.000 source these and let people use them and
00:31:39.799 there should be no regulatory burden on
00:31:41.480 companies developing them um because
00:31:43.519 we're willing to make the Innovation and
00:31:45.120 freedom
00:31:46.159 tradeoff for the negative safety
00:31:48.799 consequences at level X right and then
00:31:51.120 level y can be totally different
00:31:54.480 and I totally get the impulse to say
00:31:59.240 any regulatory action is unacceptable
00:32:03.440 because it's just big companies that are
00:32:06.320 going to use it to like for regulatory
00:32:09.000 capture um and you know if Society
00:32:11.840 decides we don't want to regulate AI at
00:32:13.399 all and we'll just take our chances I'll
00:32:16.399 accept the outcome of a democratic
00:32:18.679 process it seems to me good to have some
00:32:23.080 voices saying like well you open with a
00:32:25.720 p Doom question exactly um if if the
00:32:28.799 framing of that question is
00:32:30.519 correct uh then it seems to me useful to
00:32:33.840 have some voices
00:32:35.480 saying let's not act out of fear but
00:32:38.159 proceed with some reasonable caution no
00:32:40.559 that makes sense um speaking of which um
00:32:43.480 you've said you believe that the
00:32:44.679 upcoming presidential election won't be
00:32:47.159 the same as the last one I think we all
00:32:48.760 think that uh but there are lessons to
00:32:50.600 be learned from 2020 uh what are these
00:32:52.880 lessons how can we mitigate the risk AI
00:32:55.159 Pro opposes to the Democratic process
00:32:58.480 and to uh the future of Democracy in
00:33:02.279 America you
00:33:05.440 know maybe maybe the use of advanced AI
00:33:08.279 will be the least interesting thing
00:33:09.440 about this election the way it's shaping
00:33:11.120 up uh
00:33:14.919 I I do
00:33:19.399 think yeah I think there will be like
00:33:21.799 better deep fakes of course and there
00:33:23.919 will be better like troll Farms of
00:33:26.600 course um
00:33:29.519 what I think is more interesting is
00:33:31.320 trying to get ahead of to the degree
00:33:34.360 that we can this is easier said than
00:33:35.840 done but trying to get ahead of the new
00:33:38.720 things that just weren't possible before
00:33:41.519 um so like customize one-on-one
00:33:43.880 persuasion where uh an AI system reads
00:33:47.480 all of your social media posts and
00:33:49.639 targets something just at you that
00:33:51.760 wasn't really possible with all of the
00:33:53.720 sort of like online disinformation and
00:33:55.320 trolling of the last election right and
00:33:58.240 that's the kind of new thing that I wish
00:34:00.720 we were taken more seriously yeah I mean
00:34:02.720 that's the logical extension as you
00:34:04.880 mentioned of you know advertisements
00:34:06.760 coming up when you're surfing the web
00:34:08.399 people know you buy X kinds of shoes or
00:34:10.239 whatever how you think about things
00:34:12.599 absolutely um a more local question uh
00:34:15.960 one of mit's educational priorities is
00:34:18.399 to train tomorrow's leaders to be in
00:34:20.560 essence Computing bilingual meaning that
00:34:22.760 regardless of their chosen field they
00:34:24.320 will need to be fluent in computer
00:34:26.119 science and AI to advance their work
00:34:27.719 work can you comment on the impact of
00:34:30.199 our way of thinking I don't know if
00:34:31.480 people talk you about blending Blended
00:34:33.480 Computing thinking about how we CH train
00:34:35.760 bilinguals how they can start thinking
00:34:37.800 about the future
00:34:39.239 careers really learning two different
00:34:41.918 fields and using Computing as a way into
00:34:44.320 into other
00:34:46.159 areas one of the general observations I
00:34:49.760 make about the history of computing it
00:34:51.199 has it gotten
00:34:53.560 increasingly more accessible and more
00:34:55.839 natural over time M um
00:34:59.400 you know like I heard these stories of
00:35:02.280 people with like punch cards that they
00:35:03.960 would like have these crazy systems
00:35:05.480 about how they sorted them and they
00:35:06.599 would drop them and that was a big mess
00:35:07.960 and it was like that was like a not a
00:35:10.480 natural thing to use
00:35:12.839 um undergraduate education sounds tough
00:35:16.720 um low-level programming languages were
00:35:18.880 a step forward um but still like not a
00:35:21.480 thing that most of the world knew how to
00:35:23.800 use or I think a very natural tool in
00:35:26.520 some sense um and then you get to like
00:35:29.200 current programming languages and
00:35:30.760 they're way more accessible and way
00:35:32.280 easier to use also way more expressive
00:35:33.839 and more powerful
00:35:36.640 um along with that Evolution you go from
00:35:40.560 uh like the command line um to a gooey
00:35:44.400 to like a mouse and a keyboard to just
00:35:46.280 like touching your phone like you don't
00:35:48.119 that's that was like not very that was
00:35:50.000 pretty natural right um and then you go
00:35:53.040 to language which is Supernatural um you
00:35:55.880 know like people are very good at
00:35:58.160 language as a way to use the computer
00:36:00.640 you can sort of ask chat GPT something
00:36:02.359 but also way to program the computer so
00:36:03.800 you saw these things like converge to
00:36:05.160 this one interface
00:36:08.640 and you don't have to be that bilingual
00:36:10.880 anymore right like right the the you get
00:36:14.200 like the same way that you like talk to
00:36:16.400 a a friend or a colleague you can talk
00:36:18.280 to a
00:36:19.359 computer and this is I think a more
00:36:21.839 profound thing than it sounds like on
00:36:23.839 the surface um the degree to which we
00:36:26.760 can push
00:36:29.359 Ai and people to have the same kind of
00:36:32.560 interface um so I'm like more excited
00:36:34.599 about humanoid robots than I am about
00:36:36.040 other forms because I think the world is
00:36:37.480 just very designed for humans and we
00:36:38.720 should keep it that way but we want the
00:36:40.160 benefits of robots that can help us um I
00:36:42.920 think we want AI systems to do their
00:36:44.839 cognition and language to communicate
00:36:46.280 with us in language um it's a very
00:36:50.280 human Focus thing um so my hope is we
00:36:54.800 don't all have to be a bilingual that's
00:36:56.920 interesting I mean because we have I
00:36:58.640 would
00:36:59.480 say an over an overwhelming number of
00:37:02.280 our students are obviously interested in
00:37:03.960 CS and now that we've sort of rolled out
00:37:06.480 you know these Blended areas I think you
00:37:08.720 know it may be sort of the sort of first
00:37:10.920 generation of what you're talking about
00:37:12.560 in other words people who who then you
00:37:14.760 know next Generations it will be
00:37:16.280 completely intuitive totally yeah so
00:37:18.640 that's really interesting um let's see
00:37:23.079 uh there's there's some uh backup things
00:37:26.240 here that other folks subit so how do
00:37:28.599 you think AI will impact the financial
00:37:31.079 sector thinking about sort of banking
00:37:34.920 and Equity have you thought about that
00:37:36.280 at all um first of all if we're if we're
00:37:39.160 on the backup questions and we want to
00:37:40.400 like let people shout out if you want to
00:37:41.640 do that I'm totally down otherwise we
00:37:43.160 can do them either way let me see we can
00:37:46.079 we can do some uh well let me ask you
00:37:47.839 then well why to answer that I have a
00:37:49.160 couple other things and then then if
00:37:50.400 people really feel compelled to yell out
00:37:52.800 some questions related to this interview
00:37:56.119 um then then
00:37:58.599 then we can do that but go ahead
00:38:02.200 um
00:38:03.960 I I haven't thought as much as I would
00:38:06.560 like to about any specific area cuz
00:38:09.079 figuring out how to get the general
00:38:10.800 purpose intelligence and what that means
00:38:13.200 has been pretty allc consuming um you
00:38:17.160 know like education and Healthcare are
00:38:18.640 kind of maybe the two specifics that
00:38:20.200 I've thought about the most on something
00:38:23.000 like the financial system uh I expect AI
00:38:28.160 to like impact that kind of as much as
00:38:30.640 everything else but I don't think I have
00:38:32.480 like a deeply insightful specific thing
00:38:34.319 to say about here's how this really
00:38:36.200 transforms let's think a little bit more
00:38:38.200 than about the educational sector you
00:38:40.440 know we think a lot about how um you
00:38:44.040 know people worry about things about AI
00:38:45.800 in the classroom but I really think
00:38:47.079 there's just a huge potential for how we
00:38:48.920 teach and how we tailor things to
00:38:50.920 individuals Etc so if you could say a
00:38:53.920 word about that I think that would be
00:38:55.200 great
00:38:58.200 I think with what you're seeing people
00:39:01.240 do already just with like regular gbd4
00:39:04.920 in chat gbt when you say like please
00:39:07.000 pretend you're a tutor and help me learn
00:39:08.440 this thing if that can work so well then
00:39:12.640 as people start to take Next Generation
00:39:14.359 models and customize them for learning
00:39:16.720 experiences we're we're going to be in a
00:39:18.960 very good place um and it's it's like
00:39:23.240 pretty awesome to see what people are
00:39:24.640 building already um it's great to hear
00:39:28.560 from teachers about the impact it's had
00:39:30.560 on their students it's great to hear
00:39:31.599 from students about what they're
00:39:32.440 learning on their own but this one seems
00:39:34.720 like a kind of Slam Dunk good use no
00:39:37.040 absolutely so this isn't this isn't one
00:39:39.240 of the sort of pres submitted questions
00:39:40.599 but I'm always really interested when I
00:39:42.040 meet people who are doing really
00:39:43.079 interesting things how you came to this
00:39:46.119 in other words what was your sort of
00:39:48.280 path and how did you how did you think
00:39:50.240 about and make choices as you came to
00:39:51.960 what you're
00:39:53.200 doing um well I was like a very nerdy
00:39:56.400 kid uh I spent like a lot of time
00:39:59.480 reading sci-fi or watching Star Trek or
00:40:01.440 whatever and uh I
00:40:04.520 like that was you know that was like the
00:40:08.440 that was already a time where all
00:40:10.560 contemporary sci-fi was pretty uh
00:40:13.520 dystopic but the older stuff the older
00:40:15.640 Star Trek episodes those were still like
00:40:17.640 kind of optimistic and cool and you sort
00:40:19.640 of like saw how AI was going to be great
00:40:23.680 and kind of how that was obviously the
00:40:26.400 future um and I kind of like
00:40:29.079 abstractedly thought like I would love
00:40:30.480 to work on that someday that would be so
00:40:31.680 cool I never thought it would actually
00:40:32.800 happen um but I was always interested uh
00:40:36.119 and then like you know life went on I
00:40:37.400 got a little bit older uh I went to
00:40:40.240 school and I decided to study in the AI
00:40:43.520 lab
00:40:45.200 um I also got really interested in
00:40:47.119 energy um but this was kind of when I
00:40:49.440 started to believe that like if we could
00:40:50.960 use tech technology to deliver abundance
00:40:53.480 that would not solve the elb problem
00:40:55.079 people would still find all sorts of
00:40:56.240 ways to be unhappy but it was like
00:40:57.960 something I really wanted to do um
00:41:02.079 and uh worked in the AI lab it was like
00:41:05.680 clear that AI was nowhere close to
00:41:08.359 working it's like back in 2004 um got
00:41:12.599 sidetracked a bunch and went off and did
00:41:14.200 a bunch of other stuff and you know did
00:41:15.800 a startup became a startup investor and
00:41:18.240 then in 2012 uh noticed the Alex net
00:41:23.839 paper took me a couple of years to
00:41:26.000 really internalize maybe this thing
00:41:27.400 finally working and I was like should
00:41:29.920 really do something
00:41:31.200 here excellent now I think there's
00:41:33.480 probably one some question on the minds
00:41:35.119 of some of the people here uh what do
00:41:38.079 you look for when you hire somebody I
00:41:40.119 mean there'll be people sitting in this
00:41:41.920 audience who are wondering that
00:41:44.079 intensely first of all we would really
00:41:45.960 love to hear from you and I think this
00:41:47.480 is probably the most the most or second
00:41:50.400 most exciting time in open a history
00:41:52.240 right now um so uh this is like this be
00:41:55.560 a good time to give us a call
00:41:58.160 um number up on the screen what do we
00:42:01.119 look for
00:42:02.960 uh kind of all the obvious stuff like
00:42:06.319 original thinkers smart driven dedicated
00:42:09.440 people people who sort of like are
00:42:11.480 particularly driven to work on AI versus
00:42:15.480 you know there there's always like a set
00:42:16.680 of people who will go work on whatever
00:42:19.319 like the hot area is uh and then there's
00:42:21.480 like a set of people who are like no
00:42:22.559 this is like the thing and we like
00:42:23.960 people for whom this is their thing um
00:42:28.640 but that's kind of it nothing I mean all
00:42:30.559 of the obvious things and nothing that
00:42:32.440 unusual interesting if you were doing
00:42:34.720 something else what would it be I would
00:42:36.680 go work on Fusion full-time Fusion was
00:42:38.480 going to be something else yeah there's
00:42:39.640 definitely people here who are
00:42:40.680 interested in that as well um you know
00:42:43.280 we do have a few minutes um yeah so we
00:42:46.720 don't have any mics I see monus's hand
00:42:49.119 up right away uh but you got to shout
00:42:52.200 I'll repeat the question hi first of all
00:42:54.960 thanks for all all you're doing to the
00:42:56.200 world
00:43:00.559 andit um so I have some of the smartest
00:43:04.040 kids in the world even smartest myam are
00:43:06.920 coming to me and saying um how do I how
00:43:10.440 do I want to plan my time for the next 5
00:43:13.240 years while humans are still helpful and
00:43:16.680 useful and it it's it's striking because
00:43:19.880 you know I see AI as an extraordinary
00:43:22.359 tool and I am so looking forward to the
00:43:26.160 future of work where I can do amazing
00:43:28.880 things that I can't do right now where
00:43:30.040 all the mundane stuff that I'm doing now
00:43:31.839 is replaced and I can do super awesome
00:43:34.040 new things but there will come a time
00:43:37.880 given the two curves of how fast human
00:43:39.760 intelligence has progressed through the
00:43:41.720 old school Evolution and nurture of
00:43:44.400 course and the curve with which AI is
00:43:48.079 progressing which is just ridiculous so
00:43:50.640 these curves are going to cross very
00:43:52.920 soon and I'm just curious what are some
00:43:56.040 unique capabilities that humans have
00:43:59.480 that you think we will not be able to
00:44:01.760 replicate with AI what are some of the
00:44:03.920 architectures that will help us maybe
00:44:06.319 replicate those things and push beyond
00:44:08.160 that and how do you see the future of
00:44:10.000 human versus machine intelligence more
00:44:12.319 broadly two conflicting answers to that
00:44:15.240 um number one I kind of suspect
00:44:18.680 that forever onwards from now it's
00:44:21.480 always going to feel like man this next
00:44:23.839 5year period is so critical this is when
00:44:25.680 I can really contribute and after that
00:44:27.240 who knows what happens but in practice
00:44:30.839 there's always going to be um it's
00:44:33.880 always going to feel like man this curve
00:44:35.280 of intelligence is rising so fast right
00:44:37.599 now I can use these tools to like out
00:44:39.520 aieve but eventually they outrun it but
00:44:42.599 at any point on that exponential um
00:44:46.040 you'll always be able to use the tools
00:44:48.000 to do amazing things and it'll always
00:44:49.880 feel like then you get totally outrun
00:44:51.880 but you never quite do because like we
00:44:54.520 just become more capable um and I didn't
00:44:58.480 used to really think this way I'm still
00:45:00.800 not sure I'm right uh but it does seem
00:45:03.960 to me
00:45:06.400 like we will just be able to like
00:45:09.000 accomplish more do more things we'll
00:45:11.200 have ridiculous the expectations will go
00:45:13.000 up too to like participate in the
00:45:14.800 economy in some sense but what we can
00:45:18.119 get done
00:45:20.480 uh before like we feel like this wave is
00:45:23.119 going to crash over us we'll always feel
00:45:25.119 like man in this 5 years this is my WI
00:45:27.480 but it's going to be a rolling 5 years
00:45:28.920 forever um and I think we'll find that
00:45:32.240 humans are really good at like humans
00:45:34.240 are we're just we're so wired to care
00:45:36.359 about other humans we're so wired to
00:45:41.119 like focus our energy know what other
00:45:44.200 people want be focused on like
00:45:45.800 delivering value for others uh that you
00:45:50.040 know like I can I can see very extreme
00:45:52.400 worlds where like human money and
00:45:54.000 machine money are just different things
00:45:56.319 um but but there's like an increasing
00:45:59.319 premium on human the human money part of
00:46:03.119 that of That World um I don't know
00:46:05.440 exactly what it's going to look like but
00:46:06.640 I do kind of believe in the biological
00:46:09.520 drivers of humanity not changing that
00:46:11.720 much uh and then the answer on the other
00:46:15.240 side
00:46:16.520 is and this is just speaking personally
00:46:18.880 this is not a well-reasoned or defens
00:46:20.800 like logically defended thing at all
00:46:22.200 this is just what I feel I do feel like
00:46:27.400 like I know I'm going to be nostalgic
00:46:28.720 for this
00:46:29.920 time and it's sort of a strange thing to
00:46:32.200 like feel that while you're living
00:46:33.240 through
00:46:34.400 it that's really
00:46:36.760 interesting I see another hand back
00:46:38.760 there well I'm going to have to ask you
00:46:40.160 to just hand the microphone to whoever
00:46:41.480 you see because I
00:46:48.720 uh can you hear me here sure
00:47:06.920 more bullish on startups building and
00:47:08.720 user applications or infrastructure
00:47:12.680 uh probably at this current phase of
00:47:15.440 where we are on end user applications
00:47:17.599 you got to like pick the right ones you
00:47:19.000 got to pick the ones that that benefit
00:47:21.280 from the models getting better not the
00:47:22.640 ones that are betting the model actually
00:47:24.280 doesn't get better and kind of are
00:47:25.559 fixing the current generation proc s
00:47:28.640 but I think it's
00:47:31.559 like I think there's like a lot of value
00:47:33.599 to unlock there um building
00:47:36.200 infrastructure can be great too I think
00:47:38.079 you can really succeed both ways but
00:47:40.440 like the number of what feel like 100
00:47:42.880 billion dollar application layer
00:47:44.400 companies right now where you can really
00:47:46.319 like do something incredibly useful to
00:47:48.720 people quickly seems pretty exciting to
00:47:54.920 me yeah
00:47:57.680 uh
00:48:07.680 yeah thank you so much for giving this
00:48:09.839 talk I just had a question in terms of
00:48:11.520 the future that you see specifically for
00:48:13.520 open AI I was just wondering you can
00:48:15.680 come out with GPT like 6 7 8 9 10 20 30
00:48:18.960 40 50 60 um you have like your creation
00:48:22.119 of Sora um a lot of companies are
00:48:24.559 creating like life-size robots that are
00:48:26.800 utilizing AI as the backend or a lot of
00:48:29.800 technological companies also using chat
00:48:32.040 GPT as like a foundation for their own
00:48:33.920 llms so I was just wondering for you
00:48:37.079 guys specifically at open AI what do you
00:48:39.359 guys foresee as your niche in the future
00:48:42.400 once you start I guess not perfecting
00:48:46.240 but getting to near Perfection for like
00:48:48.640 your llm
00:48:51.559 models I'm going to go over there um I
00:48:55.599 think it's so far
00:48:57.640 we're so far away from when we start to
00:49:00.119 level off
00:49:02.760 um that that's not currently on our mind
00:49:06.359 um I think you know at least for the
00:49:09.079 next three four model
00:49:11.319 Generations I believe we can make it so
00:49:14.319 incredibly much better every time um we
00:49:17.240 should focus on that and if we can do
00:49:19.640 that everything else will kind of work
00:49:21.160 out uh like I think our our Niche or
00:49:23.920 whatever is we want to deliver
00:49:27.559 great useful impressive cognitive
00:49:30.280 capability for as uh abundantly and
00:49:32.680 inexpensively as we can and there's
00:49:35.000 other good things we can do but if we
00:49:36.280 can just focus on that I think it'll be
00:49:38.040 like a great service to the world and we
00:49:39.559 can go on that for a
00:49:45.000 while uh I have two questions one where
00:49:47.359 did you get your
00:49:49.599 shoes uh Adidas did a Lego collaboration
00:49:52.400 I love
00:49:53.920 Legos amazing cool um my my serious
00:49:57.079 question is how soon do you predict that
00:49:59.079 we'll get to artificial general
00:50:00.319 intelligence and what is open ai's role
00:50:02.799 uh in getting us there you know I don't
00:50:06.040 I don't I I I no longer think there will
00:50:08.799 be a time where the world agrees okay
00:50:11.240 this was the year we crossed the AGI
00:50:13.040 threshold I think the the phrase has
00:50:15.680 become
00:50:18.480 so uh so overloaded as how as a
00:50:22.480 definition I think there are people who
00:50:24.280 would say we'll get there soon I think
00:50:26.040 there's like a lot of people who will
00:50:27.240 say by 2040 like when we have these
00:50:29.119 unbelievably capable systems ah it's not
00:50:31.359 quite AGI yet I can't do this one thing
00:50:34.079 um so I think I think the question is
00:50:36.799 like the only way I know how to like
00:50:38.799 form the question well at this point is
00:50:41.720 what is the range of time that we get to
00:50:43.640 like capability x y and z um but I think
00:50:46.760 that the AGI like I can't make myself do
00:50:50.760 this cuz it's like I it's too much of
00:50:52.640 like a it's too like deep in the OS at
00:50:56.760 this point but I like I try to not use
00:50:58.520 the word AGI anymore it's a total like
00:51:01.119 I'm never going to succeed at that but
00:51:03.640 it's like you know when can we when can
00:51:05.480 we do that new scientific discovery in
00:51:07.119 some areas when can we uh when can we
00:51:09.839 like add a lot of economic value um I
00:51:12.839 don't know I expect that like by the end
00:51:14.680 of this decade we have systems that
00:51:17.240 create really significant economic
00:51:20.920 value excellent so maybe last question
00:51:24.040 uh from the audience where whoever's
00:51:28.799 last one here maybe and then uh I'll
00:51:31.599 close this
00:51:33.240 out
00:51:34.839 yeah thank you so much Sam for the talk
00:51:37.720 um I guess I have more a general
00:51:39.799 question I guess what do you do when
00:51:41.119 you're have a problem to solve and
00:51:42.480 you're stuck and you don't know the way
00:51:43.799 forward um guess what's your thought
00:51:45.720 process on trying to get unstuck I
00:51:48.319 somehow try to like change context I try
00:51:50.280 to talk to different people about it um
00:51:52.400 I try to like in an extreme case I'll
00:51:54.640 like go travel somewhere to like really
00:51:56.359 like kind of change things like the one
00:51:58.000 time I like jet lag is if I'm like
00:51:59.440 really stuck on a problem and I wake up
00:52:00.760 in the middle of the night in some new
00:52:01.839 context and it seems to be helpful um
00:52:05.040 but like some way or other I try to
00:52:06.240 change
00:52:08.000 context well with that um I really want
00:52:10.799 to thank you that was fascinating I
00:52:12.400 think everybody greatly enjoyed the
00:52:14.440 conversation thank you all very much and
00:52:16.660 [Applause]
00:52:18.079 uh thank you thank you
00:52:24.680 awesome sure
00:52:28.200 can I sign this real
00:52:29.050 [Applause]
00:52:33.240 quick he went that
