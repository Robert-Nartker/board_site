# tactiq.io free youtube transcript
# OpenAI CEO Sam Altman testifies during Senate hearing on AI oversight â€” 05/16/23
# https://www.youtube.com/watch/fP5YdyjTfG0

00:00:02.220 foreign
00:01:16.400 I don't want it
00:01:19.380 congrats on congratulations
00:01:26.220 yes
00:02:17.640 welcome to the hearing of the Privacy
00:02:21.180 technology and the law subcommittee
00:02:24.540 uh thank my
00:02:27.300 partner in this effort Senator Hawley
00:02:30.060 ranking member and I particularly want
00:02:32.580 to thank uh Senator Durbin chairman of
00:02:35.340 the Judiciary Committee and he will be
00:02:37.379 speaking shortly this hearing is on the
00:02:40.819 oversight of our artificial intelligence
00:02:43.500 the first in a series of hearings
00:02:46.620 intended to write the rules
00:02:49.019 of AI our goal is to demystify and hold
00:02:53.280 accountable
00:02:54.780 those new technologies to avoid some of
00:02:57.840 the mistakes of the past
00:03:00.120 and now for some introductory remarks
00:03:06.420 too often
00:03:07.860 we have seen what happens when
00:03:09.300 technology outpaces regulation
00:03:12.120 the unbridled exploitation of personal
00:03:14.400 data the proliferation of disinformation
00:03:18.239 and the deepening of societal
00:03:20.760 inequalities
00:03:22.379 we have seen how algorithmic biases can
00:03:26.340 perpetuate discrimination and Prejudice
00:03:30.300 and how the lack of transparency can
00:03:32.940 undermine public Trust
00:03:35.280 this is not the future we want
00:03:39.239 if you were listening from home you
00:03:41.940 might have thought that voice was mine
00:03:43.560 and the words
00:03:45.659 from me but in fact
00:03:47.760 that voice was not mine
00:03:51.000 the words were not mine
00:03:54.000 and the audio was an AI voice cloning
00:03:58.440 software trained on my floor speeches
00:04:01.799 the remarks were written by chat gbt
00:04:05.700 when it was asked
00:04:07.620 how I would open this hearing
00:04:12.900 and you heard just now the result
00:04:16.500 I asked chat GPT
00:04:19.858 why did you pick those themes and that
00:04:22.680 content
00:04:23.940 and it answered and I'm quoting
00:04:27.180 Blumenthal has a strong record in
00:04:29.280 advocating for consumer protection and
00:04:31.259 civil rights he has been vocal about
00:04:33.479 issues such as data privacy
00:04:39.660 and
00:04:42.060 the potential for discrimination in
00:04:44.280 algorithmic decision making
00:04:46.740 therefore the statement emphasizes these
00:04:50.340 aspects uh Mr Altman I appreciate chat
00:04:53.880 gpt's endorsement
00:04:56.160 uh in all seriousness this apparent
00:04:58.940 reasoning is pretty impressive
00:05:02.220 I am sure that we'll look back in a
00:05:04.800 decade and uh view chat GPT and gpt4
00:05:09.540 like we do the first cell phone those
00:05:11.580 big clunky things that we use to carry
00:05:14.460 around but we recognize that we are on
00:05:17.160 the verge really of a new era the audio
00:05:21.199 and my playing it may strike you as
00:05:23.940 curious or humorous but what
00:05:27.720 reverberated in my mind was what if I
00:05:30.960 had asked
00:05:32.160 it and what if it had
00:05:35.100 provided an endorsement of Ukraine
00:05:38.039 surrendering or Vladimir Putin's
00:05:41.100 leadership
00:05:42.419 that would have been really frightening
00:05:44.699 and the prospect is
00:05:47.160 more than a little scary to use the word
00:05:49.680 Mr Altman you have used yourself and I
00:05:52.979 think you have been very constructive in
00:05:55.680 calling attention to the pitfalls as
00:05:58.020 well as the promise and that's the
00:05:59.220 reason why we wanted you to be here
00:06:01.800 today and we thank you and our other
00:06:03.840 Witnesses for joining us for several
00:06:07.139 months now the public has been
00:06:08.400 fascinated
00:06:09.600 with GPT
00:06:11.639 dally and other AI tools these examples
00:06:16.100 like the homework done by chat GPT or
00:06:20.039 the Articles and op-eds that it can
00:06:22.259 write feel like novelties
00:06:24.720 but the underlying advancement of this
00:06:27.720 era
00:06:28.979 are more than just research experiments
00:06:31.680 they are no longer fantasies of Science
00:06:34.979 Fiction they are real and present the
00:06:38.100 promises of
00:06:39.780 curing cancer or developing new
00:06:41.580 understandings of physics and biology or
00:06:43.919 modeling climate and weather
00:06:46.919 all very encouraging and hopeful
00:06:50.699 but we also know the potential Harms
00:06:55.080 and we've seen them already weaponized
00:06:57.300 disinformation housing discrimination
00:06:59.960 harassment of women and impersonation
00:07:02.220 fraud voice cloning
00:07:05.120 deep faith
00:07:07.500 these are the potential risks despite
00:07:12.419 the other rewards and for me
00:07:15.900 perhaps the biggest nightmare is the
00:07:18.479 looming new Industrial Revolution the
00:07:21.539 displacement of millions of workers the
00:07:24.300 loss of
00:07:25.800 huge numbers of jobs the need to prepare
00:07:29.340 for this new Industrial Revolution in
00:07:32.639 skill training
00:07:33.900 and relocation that may be required and
00:07:37.800 already industry leaders are calling
00:07:40.199 attention to
00:07:42.419 those challenges
00:07:46.139 to quote chat gbt
00:07:49.560 this is not
00:07:51.240 necessarily the future that we want
00:07:54.539 we need to maximize the good over the
00:07:58.500 bad
00:07:59.400 Congress has a choice now we have the
00:08:02.039 same Choice when we Face social media we
00:08:04.740 failed to seize that moment the result
00:08:07.020 is
00:08:08.160 Predators on the internet toxic content
00:08:12.000 exploiting children creating dangers for
00:08:14.460 them and Senator Blackburn and I and
00:08:17.819 others like Senator Durbin on the
00:08:20.580 Judiciary Committee are trying to deal
00:08:22.740 with it kids online safety act but
00:08:25.039 Congress failed to meet the moment on
00:08:27.720 social media now we have the obligation
00:08:29.940 to do it on AI before the threats and
00:08:34.260 the risks become real
00:08:38.159 sensible safeguards are not in
00:08:40.740 opposition to Innovation accountability
00:08:43.020 is not a burden far from it
00:08:45.779 they are the foundation of how we can
00:08:47.820 move ahead while protecting public trust
00:08:50.279 they are how we can lead the world in
00:08:52.620 technology and science but also in
00:08:55.560 promoting our Democratic Values
00:08:57.959 otherwise in the absence of that trust I
00:09:01.860 think
00:09:02.580 we may well lose both these are
00:09:06.180 sophisticated technology
00:09:08.279 but there are basic expectations common
00:09:10.620 in our law we can start with
00:09:12.540 transparency
00:09:14.100 AI companies ought to be required to
00:09:15.720 test their systems disclose known risks
00:09:18.360 and allow independent researcher access
00:09:21.060 we can establish scorecards and
00:09:23.519 nutrition labels
00:09:25.620 to encourage competition based on safety
00:09:28.860 and trustworthiness
00:09:31.260 limitations on use there are places
00:09:33.120 where the risk of AI is so extreme that
00:09:36.480 we autoimpose restriction or even ban
00:09:39.060 their use especially when it comes to
00:09:40.800 commercial invasions of privacy for
00:09:44.100 profit and decisions that affect
00:09:45.740 people's livelihoods and of course
00:09:48.779 accountability or liability when AI
00:09:51.779 companies and their clients cause harm
00:09:53.459 they should be held liable
00:09:56.279 we should not repeat our past mistakes
00:10:00.660 for example section 230 forcing
00:10:03.720 companies to think ahead and be
00:10:05.760 responsible for the ramifications of
00:10:07.500 their business decisions can be the most
00:10:10.200 powerful tool of all
00:10:14.459 garbage in garbage out the principle
00:10:17.339 still applies
00:10:18.959 we ought to be aware of the garbage
00:10:21.120 whether it's going into
00:10:24.180 these platforms or coming out of them
00:10:27.120 and the ideas that we develop in this
00:10:31.260 hearing I think will provide a solid
00:10:34.019 path forward I look forward to
00:10:36.899 discussing them with you today and I
00:10:39.720 will just finish on this note the AI
00:10:41.580 industry doesn't have to wait for
00:10:43.320 Congress
00:10:44.880 I hope there are ideas and feedback from
00:10:47.100 this discussion and from the industry
00:10:49.320 and voluntary action such as we've seen
00:10:52.200 lacking
00:10:53.940 in many social media platforms and the
00:10:57.180 consequences have been huge
00:10:59.760 so I'm hoping that we will elevate
00:11:03.180 rather than have a race to the bottom
00:11:06.000 and I think these hearings will be an
00:11:10.079 important part of this conversation this
00:11:12.480 one is only the first
00:11:14.459 the ranking member and I have agreed
00:11:16.680 there should be more and we're going to
00:11:19.019 invite other industry leaders some have
00:11:21.180 committed to come experts academics
00:11:25.200 and the public we hope will participate
00:11:27.480 and with that I will turn to the ranking
00:11:30.120 member Senator Hawley
00:11:32.339 thank you very much Mr chairman thanks
00:11:34.320 to the witnesses for being here I
00:11:36.180 appreciate that several of you had long
00:11:38.459 Journeys to make in order to be here I
00:11:40.320 appreciate you making the time I look
00:11:42.540 forward to your testimony I want to
00:11:44.040 thank Senator Blumenthal for convening a
00:11:46.560 leader on this topic you know a year ago
00:11:48.660 we couldn't have had this hearing
00:11:50.640 because the technology that we're
00:11:52.440 talking about had not burst into public
00:11:54.720 Consciousness that gives us a sense I
00:11:56.700 think of just how rapidly this
00:12:00.060 technology that we're talking about
00:12:01.380 today is changing and evolving and
00:12:04.380 Transforming Our World right before our
00:12:07.440 very eyes I was talking with someone
00:12:08.899 just last night a researcher in the
00:12:12.360 field of Psychiatry who was pointing out
00:12:14.100 to me that the chat GPT and generative
00:12:17.339 AI these large language models it's
00:12:19.500 really like the invention of the
00:12:21.480 internet in scale at least at least and
00:12:24.720 potentially far far more significant
00:12:26.760 than that we could be looking at one of
00:12:29.100 the most significant technological
00:12:31.200 innovations in human history
00:12:34.140 and I think my question is what kind of
00:12:36.300 an innovation is it going to be
00:12:38.519 is it going to be like the printing
00:12:40.320 press
00:12:41.100 that diffused knowledge and power and
00:12:44.700 learning widely across the landscape
00:12:48.120 that empowered ordinary everyday
00:12:50.339 individuals that led to Greater
00:12:52.320 flourishing that led above all to
00:12:53.700 Greater Liberty
00:12:55.139 or is it going to be more like the atom
00:12:56.820 bomb
00:12:58.079 huge technological breakthrough
00:13:00.360 but the consequences severe terrible
00:13:04.079 continue to haunt us to this day
00:13:07.440 I don't know the answer to that question
00:13:08.639 I don't think any of us in the room know
00:13:10.139 the answer to that question because I
00:13:11.399 think the answer has not yet been
00:13:12.959 written and to in a certain extent it's
00:13:14.940 up to us here and to us as the American
00:13:17.880 people to write the answer
00:13:20.700 what kind of Technology will this be how
00:13:23.220 will we use it to better Our Lives how
00:13:25.320 will we use it to actually harness the
00:13:28.200 power of technological innovation for
00:13:30.000 the good of the American people for the
00:13:32.339 liberty of the American people not for
00:13:34.800 the power of the few
00:13:37.500 you know I was I was reminded of the
00:13:40.200 psychologist and writer Carl Young Who
00:13:42.300 said at the beginning of the last
00:13:43.860 century
00:13:45.120 that our ability for technological
00:13:47.519 innovation our capacity for
00:13:49.320 technological revolution had far
00:13:51.120 outpaced our ethical and moral ability
00:13:53.339 to apply and harness the technology we
00:13:56.399 developed that was a century ago I think
00:13:58.860 the story of the 20th century largely
00:14:00.839 bore him out
00:14:02.519 and I just wonder what will we say as we
00:14:04.620 look back at this moment
00:14:06.360 about these new technologies about
00:14:08.339 generative AI about these language
00:14:10.560 models and about the hosts of other AI
00:14:12.839 capacities that are even right now under
00:14:15.000 development not just in this country but
00:14:16.740 in China at the countries of our
00:14:19.079 adversaries and all around the world and
00:14:21.240 I think the question that young pose is
00:14:23.399 really the question that faces us will
00:14:25.380 we strike that balance between
00:14:27.120 technological innovation and our ethical
00:14:29.339 and moral responsibility
00:14:31.440 to humanity to Liberty
00:14:34.800 to the freedom of this country and I
00:14:37.019 hope that today's hearing will take us a
00:14:39.000 step closer to that answer thank you Mr
00:14:40.620 chairman thanks thanks Senator Hawley
00:14:42.660 I'm going to turn to the chairman of the
00:14:45.480 Judiciary Committee and the ranking
00:14:47.160 member Senator Graham if they have
00:14:49.320 opening remarks as well yes Mr chairman
00:14:52.260 thank you very much and Senator Hawley
00:14:53.940 as well uh last week in the this
00:14:56.880 committee full committee Senate
00:14:58.320 Judiciary Committee we dealt with an
00:15:00.660 issue that had been waiting for
00:15:02.279 attention for almost two decades and
00:15:04.620 that is what to do with the social media
00:15:06.959 when it comes to the abuse of children
00:15:09.240 we had four bills initially that were
00:15:12.420 considered by this committee and what
00:15:15.120 may be history in the making we passed
00:15:17.880 all four bills with unanimous roll calls
00:15:20.779 unanimous roll calls I can't remember
00:15:23.519 another time when we've done that and an
00:15:25.440 issue that important it's an indication
00:15:28.199 I think of the important position of
00:15:30.720 this Committee in the National debate on
00:15:33.000 issues that affect every single family
00:15:34.560 and affect our future in a profound way
00:15:38.420 1989 was a historic Watershed year in
00:15:42.060 America because that's when Seinfeld
00:15:44.639 arrived and we have a sitcom which was
00:15:47.639 supposedly about little or nothing which
00:15:49.980 turned out to be enduring I like to
00:15:52.740 watch it obviously and I'm always Marvel
00:15:55.860 when they show the phones that he used
00:15:57.720 in 1989 and I think about those in
00:16:00.660 comparison to what we carry around in
00:16:02.399 our pockets today it's a dramatic change
00:16:04.740 and I guess the question is I look at
00:16:07.380 that is does this change in phone
00:16:09.420 technology that we've witnessed through
00:16:11.399 the sitcom
00:16:13.339 really
00:16:15.199 exemplify a profound change in America
00:16:18.120 still unanswered but the very basic
00:16:21.120 question we face is whether or not
00:16:23.639 this the issue of AI is a quantitative
00:16:27.600 change in technology or qualitative
00:16:30.300 change
00:16:31.920 the suggestions that I've heard from
00:16:33.779 experts in the field suggest it's
00:16:35.339 qualitative
00:16:36.600 is an AI fundamentally different is it a
00:16:40.019 game changer is it so disruptive that we
00:16:43.019 need to treat it differently than other
00:16:44.699 forms of innovation that's the starting
00:16:47.339 point and the second starting point is
00:16:49.680 one that's humbling and that is effect
00:16:51.180 when you look at the record of Congress
00:16:53.639 and dealing with innovation technology
00:16:55.560 and Rapid change were not designed for
00:16:58.259 that
00:16:59.040 in fact the Senate was not created for
00:17:01.320 that purpose but just the opposite slow
00:17:03.540 things down take a harder look at it
00:17:05.400 don't react to the public sentiment make
00:17:08.160 sure you're doing the right thing well
00:17:10.140 I've heard of the potential the positive
00:17:12.179 potential of AI and it is enormous you
00:17:15.000 can go through lists of the deployment
00:17:17.160 of technology that would say that an
00:17:20.280 idea you can sketch on a website for a
00:17:23.459 website on a napkin can generate
00:17:25.859 functioning code pharmaceutical
00:17:27.959 companies could use the technology to
00:17:29.940 identify new candidates to treat disease
00:17:32.940 the list goes on and on and then of
00:17:35.340 course the danger and it's profound as
00:17:37.679 well so I'm glad that this hearing has
00:17:40.080 taken place and I think it's important
00:17:41.880 for all of us to participate I'm glad
00:17:44.039 that it's a bipartisan approach we're
00:17:46.440 going to have to scramble to keep up
00:17:47.820 with the pace of innovation in terms of
00:17:50.100 our government public response to it but
00:17:52.559 this is a great start thank you Mr
00:17:54.059 chairman
00:17:55.320 thanks thanks Senator Irvin it is very
00:17:57.840 much a bipartisan approach very deeply
00:18:00.179 and broadly bipartisan and in that
00:18:02.580 Spirit I'm going to turn to my friend
00:18:04.679 Senator Graham
00:18:11.880 thank you that was not written by AI for
00:18:14.760 sure
00:18:17.760 uh let me introduce now the witnesses
00:18:20.400 we're very grateful to you for being
00:18:22.200 here uh Sam Altman is the co-founder and
00:18:25.679 CEO of open AI the AI research and
00:18:29.640 deployment company
00:18:31.620 behind chat GPT and Dally Mr Altman was
00:18:36.660 president of the early stage startup
00:18:38.700 accelerator why combinator from 1914 I'm
00:18:42.660 sorry 2014 to 2019 open AI was founded
00:18:47.280 in 2015.
00:18:50.039 Christina Montgomery is IBM's Vice
00:18:53.580 President Chief privacy and Trust
00:18:55.320 officer overseeing the company's
00:18:57.720 Global privacy program policies
00:19:00.480 compliance and strategy she also chairs
00:19:03.059 IBM's AI ethics board a
00:19:05.100 multi-disciplinary team responsible for
00:19:07.440 the governance of AI and emerging
00:19:09.480 Technologies Christina has served in
00:19:12.600 various roles at IBM including corporate
00:19:15.539 secretary to the company's board of
00:19:17.940 directors she is a global leader in AI
00:19:21.059 ethics and governments and Ms Montgomery
00:19:24.299 also is a member of the United States
00:19:25.740 Chamber of Commerce AI commission and
00:19:28.860 the United States national AI advisory
00:19:31.799 committee which was established in 2022
00:19:34.620 to advise the president and the national
00:19:37.080 AI initiative office on a range of
00:19:39.780 topics related to AI Gary Marcus is a
00:19:44.760 leading voice in artificial intelligence
00:19:46.860 he's a scientist best-selling author and
00:19:50.400 entrepreneur founder of the robust Ai
00:19:53.340 and geometric AI acquired by Uber if I'm
00:19:57.900 not mistaken an Emeritus professor of
00:20:00.720 Psychology and Neuroscience at NYU Mr
00:20:04.559 Marcus is well known for his challenges
00:20:07.200 to contemporary AI anticipating many of
00:20:10.140 the current limitations decades in
00:20:12.660 advance and for his research in human
00:20:15.179 language development and cognitive
00:20:18.140 Neuroscience thank you for being here
00:20:21.320 and as you may know our custom on the
00:20:25.080 Judiciary Committee is to swear in our
00:20:28.080 Witnesses before they testify so if you
00:20:31.080 would all please rise and raise your
00:20:34.440 right hand
00:20:35.880 you solemnly swear that the testimony
00:20:37.980 that you are going to give is the truth
00:20:40.919 the whole truth nothing but the truth so
00:20:43.080 help to God
00:20:44.460 thank you
00:20:47.460 Mr Altman we're going to begin with you
00:20:49.320 if that's okay
00:20:51.179 thank you thank you chairman Blumenthal
00:20:53.220 ranking member Holly members of the
00:20:55.559 Judiciary Committee thank you for the
00:20:57.360 opportunity to speak to you today about
00:20:58.860 large neural networks it's it's really
00:21:00.480 an honor to be here even more so in the
00:21:02.340 moment than I expected my name is Sam
00:21:03.960 Altman I'm the Chief Executive Officer
00:21:06.240 of openai
00:21:08.160 open AI was founded on the belief that
00:21:10.440 artificial intelligence has the
00:21:11.760 potential to improve nearly every aspect
00:21:14.220 of Our Lives
00:21:15.419 but also that it creates serious risks
00:21:17.400 we have to work together to manage
00:21:20.520 we're here because people love this
00:21:21.960 technology we think it can be a printing
00:21:24.179 press moment we have to work together to
00:21:26.039 make it so
00:21:27.660 open AI is an unusual company and we set
00:21:30.000 it up that way because AI is an unusual
00:21:32.159 technology
00:21:33.659 we are governed by a non-profit and our
00:21:35.820 activities are driven by our mission and
00:21:37.440 our Charter which commit us to working
00:21:39.480 to ensure that the broad distribution of
00:21:41.340 the benefits of AI and to maximizing the
00:21:43.679 safety of AI systems
00:21:46.320 we are working to build tools that one
00:21:48.360 day can help us make new discoveries and
00:21:50.100 address some of Humanity's biggest
00:21:51.299 challenges like climate change and
00:21:53.520 curing cancer
00:21:55.020 our current systems aren't yet capable
00:21:56.880 of doing these things
00:21:58.679 but it has been immensely gratifying to
00:22:00.539 watch many people around the world get
00:22:02.700 so much value from what these systems
00:22:04.500 can already do today
00:22:06.240 we love seeing people use our tools to
00:22:08.340 create to learn to be more productive
00:22:10.679 we're very optimistic that they're going
00:22:12.600 to be fantastic jobs in the future and
00:22:14.580 the current jobs can get much better
00:22:17.760 we also have seen what developers are
00:22:19.620 doing to improve lives
00:22:21.299 for example
00:22:22.620 be my eyes used our new multimodal
00:22:25.080 technology in gpt4 to help visually
00:22:27.360 impaired individuals navigate their
00:22:28.919 environment
00:22:31.320 we believe that the benefits of the
00:22:33.059 tools we have deployed so far vastly
00:22:34.860 outweigh the risks but ensuring their
00:22:36.900 safety is vital to our work and we make
00:22:39.240 significant efforts to ensure that
00:22:40.799 safety is built into our systems at all
00:22:42.659 levels
00:22:44.159 before releasing any new system
00:22:46.500 openai conducts extensive testing
00:22:49.260 engages external experts for detailed
00:22:51.419 reviews and independent audits improves
00:22:54.179 the model's Behavior
00:22:55.559 and implements robust safety and
00:22:57.480 monitoring systems
00:22:59.940 before we release gpt4 our latest model
00:23:02.580 we spent over six months conducting
00:23:05.159 extensive evaluations external red
00:23:07.500 teaming and dangerous capability testing
00:23:10.380 we are proud of the progress that we
00:23:11.880 made
00:23:12.539 gpt4 is more likely to respond helpfully
00:23:14.940 and truthfully
00:23:16.140 and refuse harmful requests than any
00:23:18.659 other widely deployed model of similar
00:23:20.400 capability
00:23:21.840 however
00:23:23.340 we think that regulatory intervention by
00:23:25.200 governments will be critical to mitigate
00:23:27.299 the risks of increasingly powerful
00:23:28.860 models
00:23:30.240 for example the US government might
00:23:32.940 consider a combination of Licensing and
00:23:35.039 testing requirements for development and
00:23:37.140 release of AI models above a threshold
00:23:39.120 of capabilities
00:23:41.220 there are several other areas I
00:23:42.840 mentioned in my written testimony where
00:23:44.580 I believe that companies like ours can
00:23:46.440 partner with governments
00:23:48.240 including ensuring that the most
00:23:49.559 powerful AI models adhere to a set of
00:23:51.720 safety requirements
00:23:53.220 facilitating processes to develop and
00:23:55.260 update safety measures and examining
00:23:57.299 opportunities for Global coordination
00:23:59.520 and as you mentioned I think it's
00:24:01.559 important that companies have their own
00:24:02.820 responsibility here no matter what
00:24:04.200 Congress does
00:24:06.000 this is a remarkable time to be working
00:24:08.039 on artificial intelligence
00:24:10.260 but as this technology advances we
00:24:12.600 understand that people are anxious about
00:24:14.520 how it could change the way we live we
00:24:16.500 are too
00:24:17.820 but we believe that we can and must work
00:24:19.919 together to identify and manage the
00:24:22.140 potential downsides so that we can all
00:24:24.299 enjoy the tremendous upsides
00:24:27.299 it is essential that powerful AI is
00:24:29.220 developed with democratic values in mind
00:24:30.900 and this means that U.S leadership is
00:24:33.059 critical
00:24:34.500 I believe that we will be able to
00:24:35.760 mitigate the risks in front of us and
00:24:37.679 really capitalize on this Technology's
00:24:39.480 potential to grow the US economy and the
00:24:41.820 worlds and I look forward to working
00:24:43.919 with you all to meet this moment and I
00:24:45.480 look forward to answering your questions
00:24:46.440 thank you
00:24:48.419 thank you Mr Altman Ms Montgomery
00:24:52.740 chairman Blumenthal ranking member Holly
00:24:55.380 and members of the subcommittee
00:24:57.480 thank you for today's opportunity to
00:24:59.520 present
00:25:00.780 AI is not new but it's certainly having
00:25:03.360 a moment
00:25:04.679 recent breakthroughs in generative Ai
00:25:07.320 and the Technology's dramatic surge in
00:25:09.960 the public attention has rightfully
00:25:12.480 raised serious questions at the heart of
00:25:14.580 today's hearing
00:25:16.020 what are ai's potential impacts on
00:25:18.419 society
00:25:19.500 what do we do about bias what about
00:25:22.020 misinformation misuse or harmful content
00:25:25.679 generated by AI systems
00:25:28.020 Senators these are the right questions
00:25:29.820 and I applaud you for convening today's
00:25:31.860 hearing to address them head on
00:25:34.500 well AI may be having its moment the
00:25:37.020 moment for government to play a role has
00:25:39.240 not passed us by
00:25:41.039 this period of focused public attention
00:25:43.100 onai is precisely the time to Define and
00:25:46.679 build the right guard rails to protect
00:25:48.720 people and their interests
00:25:51.179 at its core AI is just a tool and tools
00:25:54.240 can serve different purposes
00:25:56.520 to that end IBM urges Congress to adopt
00:25:59.820 a Precision regulation approach to AI
00:26:03.120 this means establishing rules to govern
00:26:05.520 the deployment of AI and specific use
00:26:07.740 cases not regulating the technology
00:26:10.200 itself
00:26:12.000 such an approach would involve four
00:26:14.159 things
00:26:15.600 first different rules for different
00:26:17.279 risks
00:26:18.419 the strongest regulation should be
00:26:20.520 applied to use cases with the greatest
00:26:22.559 risks to people and Society
00:26:25.860 second clearly defining risks there must
00:26:28.679 be clear guidance on AI uses or
00:26:30.779 categories of AI supported activity that
00:26:33.120 are inherently high risk
00:26:34.860 this common definition is key to
00:26:36.779 enabling a clear understanding of what
00:26:38.880 regulatory requirements will apply in
00:26:41.340 different use cases and contexts
00:26:43.500 third be transparent so AI shouldn't be
00:26:46.200 hidden consumers should know when
00:26:47.940 they're interacting with an AI system
00:26:49.380 and that they have recourse to engage
00:26:51.360 with a real person should they so desire
00:26:53.400 no person anywhere should be tricked
00:26:55.799 into interacting with an AI system
00:26:58.380 and finally showing the impact for
00:27:00.659 higher risk use cases companies should
00:27:02.880 be required to conduct impact
00:27:04.320 assessments that show how their systems
00:27:06.419 perform against tests for bias and other
00:27:08.700 ways that they could potentially impact
00:27:10.440 the public and to a test that they've
00:27:12.539 done so
00:27:14.100 by following risk-based use
00:27:16.080 case-specific Approach at the core of
00:27:18.179 precision regulation Congress can
00:27:20.580 mitigate the potential risks of AI
00:27:22.200 without hindering innovation
00:27:24.480 but businesses also play a critical role
00:27:27.360 in ensuring the responsible deployment
00:27:28.980 of AI
00:27:30.179 companies active in developing or using
00:27:32.700 AI must have strong internal governance
00:27:35.000 including among other things designating
00:27:37.980 a lead AI ethics official responsible
00:27:40.140 for an organization's trustworthy AI
00:27:42.419 strategy
00:27:43.679 standing up an Ethics board or a similar
00:27:45.779 function as a centralized Clearinghouse
00:27:47.940 for research resources to help guide
00:27:50.279 implementation of that strategy
00:27:52.980 IBM has taken both of these steps and we
00:27:55.860 continue calling on our industry peers
00:27:57.419 to follow suit
00:27:58.860 our AI ethics board plays a critical
00:28:00.960 role in overseeing internal AI
00:28:03.240 governance processes creating reasonable
00:28:05.460 guard rails to ensure we introduce
00:28:07.380 technology into the world and are
00:28:09.240 responsible and safe manner it provides
00:28:12.059 centralized governance and
00:28:13.260 accountability while still being
00:28:14.580 flexible enough to support decentralized
00:28:17.340 initiatives across IBM's Global
00:28:19.080 operations
00:28:20.340 we do this because we recognize that
00:28:22.559 Society grants our license to operate
00:28:25.860 and with AI the stakes are simply too
00:28:27.900 high we must build not undermine the
00:28:30.240 public Trust
00:28:31.740 the era of AI cannot be another era of
00:28:34.200 move fast and break things
00:28:36.179 but we don't have to slam the brakes on
00:28:38.159 Innovation either these systems are
00:28:40.679 within our control today as are the
00:28:42.960 solutions
00:28:44.340 what we need at this pivotal moment is
00:28:46.440 clear reasonable policy and sound guard
00:28:48.539 rails these guardrails should be matched
00:28:51.059 with meaningful steps by the business
00:28:52.620 Community to do their part
00:28:55.380 Congress and the business Community must
00:28:57.419 work together to get this right the
00:28:59.580 American people deserve no less
00:29:02.100 thank you for your time and I look
00:29:03.779 forward to your questions thank you
00:29:05.539 Professor Marcus
00:29:12.600 thank you Senators today's meeting is
00:29:14.460 historic I'm profoundly grateful to be
00:29:16.860 here I come as a scientist someone who's
00:29:18.779 founded AI companies and is someone who
00:29:20.820 genuinely loves AI but who is
00:29:22.559 increasingly worried there are benefits
00:29:24.840 but we don't yet know whether they will
00:29:26.940 outweigh the risks fundamentally these
00:29:29.760 new systems are going to be
00:29:30.659 destabilizing they can and will create
00:29:33.240 persuasive lies at a scale Humanity has
00:29:35.640 never seen before Outsiders will use
00:29:38.039 them to affect our elections insiders to
00:29:40.559 manipulate our markets and our political
00:29:42.120 systems democracy itself is threatened
00:29:45.360 chatbots will also clandestinely shape
00:29:47.580 our opinions potentially exceeding what
00:29:49.740 social media can do choices about data
00:29:52.320 sets that AI companies use will have
00:29:54.059 enormous unseen influence those who
00:29:56.580 choose the data will make the rules
00:29:58.320 shaping society and subtle but powerful
00:30:00.720 ways
00:30:01.860 there are other risks too many stemming
00:30:03.899 from the in from the inherent
00:30:05.760 unreliability of current systems a law
00:30:08.279 professor for example was accused by a
00:30:10.140 chatbot of sexual harassment untrue and
00:30:14.039 it pointed to a Washington Post article
00:30:15.659 that didn't even exist
00:30:17.159 the more that that happens the more that
00:30:18.960 anybody can deny anything as one
00:30:21.360 prominent lawyer told me on Friday
00:30:22.679 defendants are starting to claim that
00:30:24.659 plaintiffs are making up legitimate
00:30:26.399 evidence these sorts of allegations
00:30:28.320 undermine the abilities of juries to
00:30:30.539 decide what or who to believe and
00:30:32.399 contribute to the undermining of
00:30:33.720 democracy
00:30:35.100 poor medical advice could have serious
00:30:36.899 consequences too an open source large
00:30:39.240 language model recently seems to have
00:30:40.860 played a role in a person's decision to
00:30:42.720 take their own life the large language
00:30:44.580 model asked the human if you wanted to
00:30:46.440 die why didn't you do it earlier and
00:30:48.600 Then followed up with were you thinking
00:30:50.100 of me when you overdosed without ever
00:30:52.080 referring the patient to the human
00:30:53.340 health that was obviously needed another
00:30:55.860 system rushed out and made available to
00:30:58.260 millions of children told a person
00:31:00.120 posing as a 13 year old how to lie to
00:31:02.880 her parents about a trip with a 31 year
00:31:04.919 old man
00:31:06.419 further threats continue to emerge
00:31:08.220 regularly a month after gpt4 was
00:31:10.620 released openai released chat GPT
00:31:12.720 plugins which quickly LED others to
00:31:14.700 develop something called Auto GPT with
00:31:16.980 direct access to the internet the
00:31:18.539 ability to write source code and
00:31:20.220 increased powers of automation this may
00:31:22.500 well have drastic and difficult to
00:31:24.360 predict security consequences what
00:31:26.880 criminals are going to do here is to
00:31:28.380 create counterfeit people it's hard to
00:31:30.299 even Envision the consequences of that
00:31:32.399 we have built machines that are like
00:31:34.380 Bulls in a china shop powerful Reckless
00:31:37.200 and difficult to control
00:31:40.260 we all more or less agrees on the values
00:31:42.480 we would like for our AI systems to
00:31:44.520 honor we want for example for our
00:31:46.080 systems to be transparent to protect our
00:31:48.299 privacy to be free of bias and above all
00:31:50.520 else to be safe but current systems are
00:31:53.279 not in line with these values current
00:31:55.020 systems are not transparent they do not
00:31:57.120 adequately protect our privacy and they
00:31:59.159 continue to perpetuate bias and even
00:32:01.140 their makers don't entirely understand
00:32:02.940 how they work most of all we cannot
00:32:05.460 remotely guarantee that they're safe and
00:32:07.559 hope here is not enough
00:32:09.360 the big Tech company's preferred plan
00:32:11.580 boils down to trust us but why should we
00:32:14.039 the sums of money at stake are
00:32:15.480 mind-boggling emissions drift open ai's
00:32:18.299 original mission statement proclaimed
00:32:19.679 our goal is to advance AI in the way
00:32:21.779 that most is most likely to benefit
00:32:23.340 Humanity as a whole unconstrained by a
00:32:26.220 need to generate Financial return seven
00:32:28.620 years later they're largely beholden to
00:32:30.299 Microsoft embroiled in part in epic
00:32:32.399 battle of search engines that routinely
00:32:34.140 make things up and that's forced
00:32:36.059 alphabet to rush out products and
00:32:37.559 de-emphasize safety Humanity has taken a
00:32:40.200 back seat
00:32:41.460 AI is moving incredibly fast with lots
00:32:44.039 of potential but also lots of risks we
00:32:46.320 obviously need government involved and
00:32:48.059 we need the tech companies involved both
00:32:49.980 big and small but we also need
00:32:52.440 independent scientists not just so that
00:32:54.720 we scientists can have a voice but so
00:32:56.100 that we can participate directly in
00:32:57.899 addressing the problems and evaluating
00:32:59.340 Solutions and not just after products
00:33:01.679 are released but before and I'm glad
00:33:03.179 that Sam mentioned that we need tight
00:33:05.159 collaboration between independent
00:33:06.659 scientists and governments in order to
00:33:08.520 hold the company's feet to the fire
00:33:09.980 allowing independent access to the
00:33:12.179 assist independent Sciences allowing
00:33:14.279 independent scientists access to these
00:33:16.080 systems before they are widely released
00:33:18.059 as part of a clinical trial like safety
00:33:20.279 evaluation is a vital First Step
00:33:22.460 ultimately we may need something like
00:33:25.200 CERN Global International and neutral
00:33:27.899 but focused on AI safety rather than
00:33:30.299 high energy physics we have
00:33:32.460 unprecedented opportunities here but we
00:33:34.500 are also facing a perfect storm of
00:33:36.600 corporate irresponsibility widespread
00:33:38.880 deployment lack of adequate regulation
00:33:40.919 and inherent unreliability
00:33:43.200 AI is among the most world-changing
00:33:45.299 Technologies ever already changing
00:33:47.399 things more rapidly than almost any
00:33:48.840 technology in history we acted too
00:33:51.000 slowly with social media many
00:33:53.100 unfortunate decisions got locked in with
00:33:55.500 lasting Consequence the choices we make
00:33:57.899 now will have lasting effects for
00:33:59.880 decades maybe even centuries the very
00:34:02.340 fact that we are here today in
00:34:03.720 bipartisan fashion to discuss these
00:34:06.059 matters gives me some hope thank you Mr
00:34:08.339 chairman thanks very much Professor
00:34:10.500 Marcus we're going to have seven minute
00:34:12.960 rounds of questioning and I will begin
00:34:15.679 first of all Professor Marcus we are
00:34:18.780 here today because we do face that
00:34:21.000 perfect storm some of us might
00:34:23.399 characterize it more like a bomb in a
00:34:26.460 china shop not a bull and as Senator
00:34:30.899 Hawley indicated there are precedents
00:34:33.960 here not only the Atomic Warfare era but
00:34:38.099 also the Genome Project the research on
00:34:41.099 genetics where there was International
00:34:44.060 cooperation as a result and we want to
00:34:47.460 avoid those past mistakes as I indicated
00:34:49.260 in my opening statement that we're
00:34:51.119 committed on social media
00:34:52.859 that is precisely the reason we are here
00:34:55.980 today chat GPT
00:34:58.800 makes mistakes all AI does and it can be
00:35:02.520 a convincing liar
00:35:04.740 what people call hallucinations
00:35:07.500 that might be an innocent problem in the
00:35:10.380 opening of a Judiciary subcommittee
00:35:12.839 hearing where a voice is impersonated
00:35:15.359 mine in this instance I or quotes from
00:35:19.619 research papers that don't exist but
00:35:22.740 chat GPT and Bard are willing to answer
00:35:26.040 questions about life or death matters
00:35:27.900 for example drug interactions
00:35:30.660 and those kinds of mistakes can be
00:35:33.300 deeply damaging I'm interested in how we
00:35:35.940 can have reliable information about the
00:35:38.820 accuracy and trustworthiness of these
00:35:41.880 models and how we can create competition
00:35:44.700 and consumer disclosures that reward
00:35:48.320 greater accuracy
00:35:51.000 the National Institutes of standards and
00:35:53.280 Technology actually already has an AI
00:35:56.720 accuracy test the face recognition
00:36:00.119 vendor test
00:36:01.920 it doesn't solve for all the issues with
00:36:05.160 facial recognition but the scorecard
00:36:07.820 does provide useful information about
00:36:10.260 the capabilities and flaws of these
00:36:12.599 systems so there's work on models to
00:36:15.359 assure accuracy and integrity my
00:36:18.900 question uh let me begin with you Mr
00:36:21.960 Altman is should we consider independent
00:36:24.720 testing labs
00:36:26.280 to provide scorecards and nutrition
00:36:29.460 labels or the equivalent of nutrition
00:36:31.380 labels packaging that indicates to
00:36:34.320 people whether or not
00:36:36.359 the content can be trusted what the
00:36:39.240 ingredients are and what the garbage
00:36:42.180 going in may be because it could result
00:36:45.240 in garbage going out
00:36:47.280 yeah I I think that's a great idea I
00:36:50.760 think that companies should put their
00:36:52.920 own sort of you know hear the results of
00:36:54.599 our test of our model before we release
00:36:55.980 it here's here's where it has weaknesses
00:36:58.560 here's where it has strengths but also
00:37:00.960 independent audits for that are very
00:37:02.820 important
00:37:03.960 these models are getting more accurate
00:37:06.000 over time uh you know this is this is as
00:37:09.420 we have I think said as loudly as anyone
00:37:11.940 this technology is in its early stages
00:37:13.680 it definitely still makes mistakes we
00:37:16.440 find that people that users are pretty
00:37:18.480 sophisticated and understand where the
00:37:21.359 mistakes are that they need or likely to
00:37:23.579 be that they need to be responsible for
00:37:26.460 verifying what the models say that they
00:37:28.740 go off and check it
00:37:30.540 um
00:37:31.560 I I worry that as the models get better
00:37:33.720 and better uh the users can have sort of
00:37:37.140 less and less of Their Own
00:37:38.880 discriminating thought process around it
00:37:40.680 but but I think users are more capable
00:37:42.960 than we could often give them credit for
00:37:45.000 in in conversations like this I think a
00:37:47.400 lot of disclosures which if you've used
00:37:49.260 chat gbt you'll see about the
00:37:51.119 inaccuracies of the model are also
00:37:53.579 important and I'm I'm excited for a
00:37:56.940 world where companies publish with the
00:38:00.180 models information about how they behave
00:38:03.359 where the inaccuracies are and
00:38:05.040 independent agencies or companies
00:38:07.440 provide that as well I think it's a
00:38:08.880 great idea I alluded in my opening
00:38:12.119 remarks to the the jobs issue the
00:38:14.700 economic effects on employment uh I
00:38:19.079 think you have
00:38:20.339 said uh in fact and I'm going to quote
00:38:23.040 development of superhuman machine
00:38:25.200 intelligence is probably the greatest
00:38:27.480 threat to the continued existence of
00:38:29.700 humanity end quote you may have had in
00:38:33.000 mind the effect on on jobs which is
00:38:37.560 really my biggest nightmare
00:38:39.660 in the long term uh let me ask you uh
00:38:43.520 what your biggest nightmare is and
00:38:46.380 whether you share that concern
00:38:51.599 like with all technological revolutions
00:38:53.579 I expect there to be significant impact
00:38:56.460 on jobs but exactly what that impact
00:38:58.619 looks like is very difficult to predict
00:39:00.240 if we went back to the the other side of
00:39:02.640 a previous technological Revolution
00:39:04.020 talking about the jobs that exist on the
00:39:06.300 other side
00:39:07.619 um you know you can go back and read
00:39:09.359 books of this it's what people said at
00:39:11.220 the time it's difficult
00:39:13.140 I believe that there will be far greater
00:39:15.900 jobs on the other side of this and the
00:39:17.700 jobs of today will get better I think
00:39:19.680 it's important
00:39:21.300 first of all I think it's important to
00:39:22.740 understand and think about gpt4 as a
00:39:24.900 tool not a creature which is easy to get
00:39:27.720 confused and it's a tool that people
00:39:29.280 have a great deal of control over and
00:39:32.220 how they use it and second gpt4 and
00:39:36.000 things other systems like it are good at
00:39:39.180 doing tasks not jobs and so you see
00:39:41.940 already people that are using
00:39:44.220 gpt4 to do their job much more
00:39:46.079 efficiently by helping them with tasks
00:39:48.960 now gbt4 will I think entirely automate
00:39:53.460 away some jobs and it will create new
00:39:55.980 ones that we believe will be much better
00:39:57.960 this happens
00:39:59.400 again my understanding of the history of
00:40:01.500 technology is one long technological
00:40:03.780 Revolution not a bunch of different ones
00:40:05.220 put together but this has been
00:40:06.599 continually happening we as our quality
00:40:09.240 of life raises and as machines and tools
00:40:12.119 that we create can help us live better
00:40:13.619 lives uh the bar raises for what we do
00:40:16.560 and and our human ability and what we
00:40:18.960 spend our time going after uh goes after
00:40:21.060 more ambitious more satisfying projects
00:40:22.859 so there there will be an impact on jobs
00:40:25.680 uh we try to be very clear about that
00:40:27.780 and I think it will require
00:40:30.420 partnership between the industry and
00:40:32.040 government but mostly action by
00:40:33.359 government to figure out how we want to
00:40:34.740 mitigate that
00:40:36.119 um
00:40:36.960 but I'm very optimistic about how great
00:40:39.180 the jobs of the future will be thank you
00:40:40.980 let me ask Ms Montgomery and Professor
00:40:43.859 Marcus for your reactions those
00:40:45.900 questions as well Ms Montgomery on the
00:40:48.720 jobs Point yeah I mean well it's a
00:40:51.839 hugely important question
00:40:54.300 um and it's one that we've been talking
00:40:55.800 about for a really long time at IBM you
00:40:58.980 know we do believe that Ai and we've
00:41:01.260 said it for a long time is going to
00:41:02.940 change every job new jobs will be
00:41:04.920 created many more jobs will be
00:41:07.619 transformed and some jobs will
00:41:09.540 transition away I'm a personal example
00:41:12.599 of a job that didn't exist when I joined
00:41:15.300 IBM and I have a team of AI governance
00:41:17.760 professionals who are in new roles that
00:41:20.579 we created you know as early as three
00:41:22.740 years ago I mean they're new and they're
00:41:24.839 growing so I think the most important
00:41:27.060 thing that we could be doing and Canon
00:41:29.160 should be doing now is to prepare the
00:41:32.400 workforce of today and the workforce of
00:41:34.680 tomorrow for partnering with death AI
00:41:38.040 Technologies and using them and we've
00:41:40.020 been very involved for for years now in
00:41:42.780 doing that in focusing on skills-based
00:41:46.020 hiring
00:41:46.980 in educating for the skills of the
00:41:50.160 future our skills build platform has
00:41:53.280 seven million Learners and over a
00:41:55.079 thousand courses worldwide focused on
00:41:57.359 skills and we've pledged to train 30
00:42:00.839 million individuals by 2030 in the
00:42:04.020 skills that are needed for society today
00:42:06.300 thank you Professor Marcus man go back
00:42:08.880 to the first question as well absolutely
00:42:11.339 on on the subject of nutrition labels I
00:42:13.800 think we absolutely need to do that I
00:42:16.380 think that there's some technical
00:42:17.400 challenges in that building proper
00:42:19.140 nutrition labels goes hand in hand with
00:42:20.700 transparency the biggest scientific
00:42:23.160 challenge in understanding these models
00:42:24.660 is how they generalize what do they
00:42:26.460 memorize and what new things do they do
00:42:28.500 the more that there's in the data set
00:42:30.599 for example the thing that you want to
00:42:32.220 test accuracy on the less you can get a
00:42:34.500 proper read on that so it's important
00:42:36.300 first of all that scientists be part of
00:42:37.920 that process and second that we have
00:42:40.020 much greater transparency about what
00:42:41.460 actually goes into these systems if we
00:42:43.320 don't know what's in them then we don't
00:42:45.060 know exactly how well they're doing when
00:42:46.800 we give something new and we don't know
00:42:48.660 how good a benchmark that will be for
00:42:50.160 something that's entirely novel so I
00:42:52.200 could go into that more but I want to
00:42:53.579 flag that
00:42:54.740 second is on jobs past performance
00:42:58.020 history is not a guarantee of the future
00:43:00.000 it has always been the case in the past
00:43:02.520 that we have had more jobs that new jobs
00:43:05.040 new professions come in as new
00:43:07.200 technologies come in I think this one's
00:43:09.300 going to be different and the real
00:43:10.380 question is over what time time scale is
00:43:12.540 it going to be 10 years is it going to
00:43:13.920 be 100 years and I don't think anybody
00:43:15.359 knows the answer to that question I
00:43:17.339 think in the long run so-called
00:43:19.440 artificial general intelligence really
00:43:21.540 will replace a large fraction of human
00:43:23.579 jobs we're not that close to artificial
00:43:25.560 general intelligence despite all of the
00:43:27.720 media hype and so forth I would say that
00:43:29.640 what we have right now is just a small
00:43:31.680 sampling of the AI that we will build in
00:43:33.900 20 years people will laugh at this as I
00:43:36.240 think it was Senator Hawley made the um
00:43:38.040 but maybe Senator Durbin made the
00:43:39.480 example about this it was Senator Durbin
00:43:41.040 made the example about cell phones when
00:43:42.720 we look back at the AI of today 20 years
00:43:46.079 ago we'll be like wow that stuff was
00:43:47.880 really unreliable it couldn't really do
00:43:50.160 planning which is important technical
00:43:51.660 aspect it's reasoning wasability and
00:43:54.420 reasoning abilities were limited but
00:43:56.880 when we get to AGI artificial general
00:43:58.920 intelligence mainly let's say it's 50
00:44:00.359 years that really is going to have I
00:44:02.220 think profound effects on labor and
00:44:05.099 there's just no way around that and last
00:44:06.540 I don't know if I'm allowed to do this
00:44:07.740 but I will note that Sam's worst fear I
00:44:10.140 do not think is employment and he never
00:44:11.940 told us what his worst fear actually is
00:44:14.400 and I think it's germane to find out
00:44:17.880 thank you I'm going to ask
00:44:22.140 Mr Altman if he cares to respond
00:44:25.020 look we have tried to be very clear
00:44:27.420 about the magnitude of the risks here I
00:44:31.440 I think jobs and employment and what
00:44:34.680 we're all going to do with our time
00:44:35.700 really matters I agree that when we get
00:44:38.040 to very powerful systems the landscape
00:44:40.319 will change I think I'm just more
00:44:41.460 optimistic that we are incredibly
00:44:44.160 creative and we find new things to do
00:44:45.839 with better tools and that will keep
00:44:47.400 happening
00:44:48.599 um
00:44:49.680 my worst fears are that we cause
00:44:51.540 significant we the field the technology
00:44:53.940 the industry caused significant harm to
00:44:56.220 the world
00:44:57.960 I think that could happen a lot of
00:44:59.099 different ways it's why we started the
00:45:00.660 company it's a big part of why I'm here
00:45:03.599 today and why we've been here in the
00:45:05.280 past and we've been able to spend some
00:45:06.900 time with you I think if this technology
00:45:09.060 goes wrong it can go quite wrong and we
00:45:12.839 want to be vocal about that we want to
00:45:14.640 work with the government to prevent that
00:45:16.619 from happening but we we try to be very
00:45:18.780 clear-eyed about what the downside case
00:45:21.000 is and the work that we have to do to
00:45:22.740 mitigate that
00:45:24.240 thank you and and our hope is that the
00:45:27.720 rest of the industry will follow the
00:45:29.160 example that you and
00:45:31.740 IBM Ms Montgomery have set by coming
00:45:35.700 today
00:45:36.960 and meeting with us as you have done
00:45:39.260 privately in helping to guide what we're
00:45:41.880 going to do so that we can Target the
00:45:43.980 harms and avoid unintended consequences
00:45:46.859 to the good
00:45:48.240 thanks
00:45:50.640 Senator Hawley
00:45:52.680 thank you again Mr chairman thanks to
00:45:54.180 the witnesses for being here Mr Altman I
00:45:55.920 think you grew up in St Louis if I did
00:45:57.720 not mistaken it's great to see it's a
00:45:59.579 great place Missouri in here it is thank
00:46:00.900 you I want to I want that noted
00:46:02.579 especially underlying on the record
00:46:03.660 Missouri is a great place that is the
00:46:05.520 takeaway from today's hearing maybe
00:46:07.200 we'll just stop there Mr chairman
00:46:09.300 um let me ask you Mr Altman I think I'll
00:46:11.280 start with you and I'll just preface by
00:46:13.440 saying my questions here are an attempt
00:46:14.880 to get my head around and to ask all of
00:46:17.160 you to help us to get our heads around
00:46:19.500 what these this generative AI
00:46:21.839 particularly the large language models
00:46:23.220 what it can do is I'm trying to
00:46:24.780 understand its capacities and then it's
00:46:26.880 its significance so I'm looking at a
00:46:28.800 paper here entitled large language
00:46:30.900 models trained on media diets can
00:46:32.880 predict public opinion this is just
00:46:34.859 posted about a month ago the authors are
00:46:37.500 two Andreas
00:46:39.319 and Roy and their conclusion of this
00:46:42.839 work was done at MIT and then also at
00:46:44.640 Google the conclusion is that large
00:46:47.220 language models can indeed predict
00:46:49.980 public opinion and they go through and
00:46:52.440 and model why this is the case and they
00:46:54.480 they conclude ultimately that an AI
00:46:56.460 system can predict human survey
00:46:58.140 responses by adapting a pre-trained
00:47:00.000 language model to subpopulation specific
00:47:02.579 media diets in other words you can feed
00:47:04.260 the model a particular set of media
00:47:06.359 inputs and it can with remarkable
00:47:09.000 accuracy in the paper goes into this
00:47:10.380 predict then what people's opinions will
00:47:12.839 be I I want to think about this in the
00:47:14.700 context of Elections
00:47:17.460 if these large language models can even
00:47:21.060 now based on the information we put into
00:47:23.400 them quite accurately predict public
00:47:26.400 opinion
00:47:27.720 you know ahead of time I mean predict
00:47:29.640 it's before you even ask the public
00:47:30.900 these questions
00:47:32.700 what will happen when entities whether
00:47:36.119 it's corporate entities or whether it's
00:47:37.740 governmental entities or whether it's
00:47:39.300 campaigns or whether it's foreign actors
00:47:41.880 take this survey information these
00:47:44.579 predictions about public opinion and
00:47:46.200 then fine-tune strategies to elicit
00:47:48.660 certain responses certain behavioral
00:47:50.579 responses I mean we already know this
00:47:52.200 committee has her testimony I think
00:47:53.640 three years ago now about the effect of
00:47:56.760 something as prosaic it now seems as
00:47:59.220 Google search
00:48:00.660 the effect that this has on voters in an
00:48:03.420 election particularly undecided voters
00:48:05.400 in the final days of an election who may
00:48:07.020 try to get information from Google
00:48:08.280 search and what an enormous effect the
00:48:11.339 ranking of the Google search the
00:48:13.079 articles that it returns has come an
00:48:14.520 enormous effect on an undecided voter
00:48:16.260 this of course is orders of magnitude
00:48:18.599 far more powerful far more significant a
00:48:22.200 far more directive if you like so Mr
00:48:24.300 Altman maybe you can
00:48:26.099 help me understand here what some of the
00:48:28.140 significance of this is should we be
00:48:29.579 concerned about models that can large
00:48:31.680 language models that can predict
00:48:34.500 survey opinion and then can help
00:48:36.480 organizations entities fine-tune
00:48:38.520 strategies to illicit behaviors from
00:48:40.319 voters should we be worried about this
00:48:41.700 for our elections yeah uh thank you
00:48:43.920 Senator Hawley for the question it's one
00:48:45.540 of my areas of greatest concern the the
00:48:47.880 the more General ability of these models
00:48:50.040 to manipulate to persuade uh to provide
00:48:53.280 sort of one-on-one uh you know
00:48:55.140 interactive disinformation I think
00:48:56.880 that's like a broader version of what
00:48:58.260 you're talking about but given that
00:49:00.960 we're going to face an election next
00:49:02.760 year and these models are getting better
00:49:03.960 I think this is a significant area of
00:49:06.300 concern I think there's a lot there's a
00:49:09.240 lot of policies that companies can
00:49:10.680 voluntarily adopt and I'm happy to talk
00:49:12.420 about what we do there
00:49:13.880 I do think some regulation would be
00:49:16.920 quite wise on this topic uh
00:49:19.800 someone mentioned earlier it's something
00:49:21.119 we really agree with people need to know
00:49:22.680 if they're talking to an AI if content
00:49:25.200 that they're looking at might be
00:49:26.520 generated or might not I think it's a
00:49:28.260 great thing to do is to make that clear
00:49:30.560 I think we also will need rules
00:49:33.780 guidelines about what
00:49:37.560 what's expected in terms of disclosure
00:49:40.220 from a company providing a model that
00:49:44.280 could have these sorts of abilities that
00:49:47.520 you talk about so I'm nervous about it
00:49:51.599 I think people are able to adapt quite
00:49:54.119 quickly when Photoshop came onto the
00:49:56.579 scene a long time ago you know for a
00:49:58.560 while people were really quite fooled by
00:50:01.220 photoshopped images and then pretty
00:50:02.940 quickly developed an understanding that
00:50:05.280 images might be photoshopped this will
00:50:07.619 be like that but on steroids and the the
00:50:11.400 interactivity the ability to really
00:50:14.339 model predict humans well as you talked
00:50:16.800 about I think is going to require
00:50:20.880 combination of companies doing the right
00:50:23.220 thing regulation and public education do
00:50:25.680 you Mr Professor Marcus do you want to
00:50:27.839 address this yeah I'd like to add two
00:50:29.339 things one is in the appendix to my
00:50:31.319 remarks I have two papers to make you
00:50:33.359 even more concerned
00:50:35.280 um one is in the Wall Street Journal
00:50:37.619 just a couple days ago called help my
00:50:39.359 political beliefs were altered by a chat
00:50:41.160 bot and I think the scenario you raised
00:50:44.040 was that we might basically observe
00:50:46.800 people and use surveys to figure out
00:50:48.599 what they're saying but as Sam just
00:50:50.640 acknowledged the risk is actually worse
00:50:52.020 that the systems will directly maybe not
00:50:54.420 even intentionally manipulate people and
00:50:56.460 that was the thrust of the Wall Street
00:50:57.599 Journal article and it links to an
00:50:59.880 article that I've also linked to called
00:51:01.319 interacting and it's not yet published
00:51:03.240 not yet peer-reviewed interacting with
00:51:05.520 opinionated language models changes
00:51:07.260 users views and this comes back
00:51:09.119 ultimately to data one of the things
00:51:11.640 that I'm most concerned about with gpt4
00:51:13.740 is that we don't know what it's trained
00:51:15.180 on I guess Sam knows but the rest of us
00:51:17.400 do not and what it is trained on has
00:51:20.220 consequences for essentially the biases
00:51:22.859 of the system we could talk about that
00:51:24.240 in technical terms but how these systems
00:51:26.520 might lead people about depends very
00:51:28.619 heavily on what data is trained on them
00:51:30.540 and so we need transparency about that
00:51:33.000 and we probably need scientists in there
00:51:34.980 doing analysis in order to understand
00:51:37.260 what the political influences of for
00:51:39.720 example of these systems might be and
00:51:41.339 it's not just about politics it can be
00:51:42.960 about health it could be about anything
00:51:45.300 these systems absorb a lot of data and
00:51:47.760 then what they say reflects that data
00:51:49.680 and they're going to do it differently
00:51:51.599 depending on what what's in that data so
00:51:53.280 it makes a difference if they're trained
00:51:55.079 on the Wall Street Journal as opposed to
00:51:57.119 the New York Times or or Reddit I mean
00:51:59.220 actually they're largely trained on all
00:52:00.780 of this stuff but we don't really
00:52:01.920 understand the composition of that and
00:52:03.900 so we have this issue of potential
00:52:06.000 manipulation and it's even more complex
00:52:08.760 than that because it's subtle
00:52:10.079 manipulation people may not be aware of
00:52:12.359 what's going on that was the point of
00:52:13.920 both the Wall Street Journal article and
00:52:15.660 the other article that I called your
00:52:17.160 attention to let me ask you about
00:52:19.680 AI systems trained on personal data the
00:52:22.559 kind of data that for instance the
00:52:24.780 social media companies the major
00:52:25.920 platforms Google meta Etc collect on all
00:52:29.579 of us routinely we've had many a chat
00:52:31.859 about this in this committee over many a
00:52:34.440 year now but the massive amounts of data
00:52:36.660 personal data that the companies have on
00:52:38.819 each one of us
00:52:40.680 an AI system that is that is trained on
00:52:43.440 that individual data that knows each of
00:52:45.540 us better than ourselves and also knows
00:52:47.819 the billions of data points about human
00:52:50.220 behavior human language interaction
00:52:52.200 generally wouldn't we be able wouldn't
00:52:54.540 we can't we foresee an AI system that is
00:52:58.160 extraordinarily good at determining what
00:53:01.079 will grab human attention and what will
00:53:03.119 keep an individual's attention and so
00:53:05.520 for the war for attention the war for uh
00:53:09.960 clicks that is currently going on on all
00:53:12.180 of these platforms is how they make
00:53:13.260 their money I'm just imagining a an AI
00:53:16.319 system these these AI models
00:53:18.059 supercharging that war for attention
00:53:19.740 such that we now have technology that
00:53:22.079 will allow individual targeting of a
00:53:24.359 kind we have never even imagined before
00:53:25.980 where the AI will know exactly what Sam
00:53:27.900 Altman finds uh attention grabbing will
00:53:30.900 know exactly what Josh Hawley finds
00:53:32.520 attention grabbing will be able to
00:53:33.780 elicit to grab our attention and then
00:53:35.819 elicit responses from us in a way that
00:53:38.099 we have heretofore not even been able to
00:53:40.020 imagine should we be concerned about
00:53:41.700 that for its corporate applications for
00:53:43.859 the monetary applications for the
00:53:45.960 manipulation that that could come from
00:53:47.460 that Mr almond
00:53:48.960 uh yes we should be concerned about that
00:53:51.359 to be clear uh openai does not we're not
00:53:55.440 off you know we wouldn't have an ad
00:53:57.119 based business model so we're not trying
00:53:58.319 to build up these profiles of our users
00:54:00.059 we're not we're not trying to get them
00:54:01.319 to use it more actually we'd love it if
00:54:03.240 they use it less because we don't have
00:54:04.319 enough gpus
00:54:05.940 um but I think other companies are
00:54:08.579 already and certainly will in the future
00:54:10.440 use AI models to create
00:54:13.680 you know very good ad predictions of
00:54:15.540 what a user will like I think that's
00:54:17.099 already happening in many ways Mr Marcus
00:54:19.800 anything you want to add hyperton yes um
00:54:22.140 and perhaps Ms Montgomery will want to
00:54:24.240 to as well I don't but um hyper
00:54:26.579 targeting of advertising is definitely
00:54:28.619 going to come I agree that that's not
00:54:30.420 been open ai's business model
00:54:32.819 um of course now they're working for
00:54:33.839 Microsoft and I don't know what's in
00:54:35.640 Microsoft's thoughts but we will
00:54:38.040 definitely see it maybe it will be with
00:54:39.599 open source language models I don't know
00:54:41.940 but the technology there is let's say
00:54:44.880 part way there to being able to do that
00:54:46.500 and we'll certainly get there
00:54:52.319 so we're an Enterprise technology
00:54:54.660 company not consumer focused so the
00:54:56.579 space isn't one that we necessarily
00:54:58.079 operate in in terms of but these issues
00:55:00.960 are hugely important issues and it's why
00:55:04.380 we've been out ahead in developing the
00:55:07.319 technology that will help to ensure that
00:55:12.480 you can do things like produce a fact
00:55:15.839 sheet that has the ingredients of what
00:55:18.240 your data is trained on data sheets
00:55:21.119 model cards all those types of things
00:55:23.220 and calling for as I've mentioned today
00:55:25.520 transparency so you know what the
00:55:29.400 algorithm was trained on and then you
00:55:31.800 also know and can manage and monitor
00:55:34.260 continuously over the life cycle of an
00:55:36.359 AI model the behavior and the
00:55:38.280 performance of that model
00:55:41.099 Senator Durbin
00:55:42.960 thank you I think what's happening today
00:55:45.420 in this hearing room is historic
00:55:47.880 I can't recall when we've had people
00:55:51.059 representing
00:55:52.800 large corporations or private sector
00:55:55.800 entities come before us and plead with
00:55:58.740 us to regulate them
00:56:00.660 in fact many people in the Senate have
00:56:04.020 based their careers on the opposite that
00:56:07.200 the economy will Thrive if Government
00:56:09.900 gets the hell out of the way
00:56:12.359 and what I'm hearing instead today is
00:56:15.119 that stop me before I innovate again
00:56:18.599 uh message and I'm just curious as to
00:56:23.520 how we're going to achieve this
00:56:25.319 as I mentioned section 230 in my opening
00:56:27.960 remarks
00:56:29.099 we learned something there
00:56:31.380 we decided that in section 230 that we
00:56:34.559 were basically going to absolve the
00:56:36.960 industry from liability
00:56:39.000 for a period of time as it came into
00:56:41.520 being
00:56:43.020 well
00:56:44.160 Mr Alderman on the podcast earlier this
00:56:46.619 year you agreed with host Kara Swisher
00:56:49.140 that section 230 doesn't apply to
00:56:51.780 generative AI
00:56:54.059 and that developers like open AI should
00:56:56.220 not be entitled to full immunity for
00:56:57.900 harms caused by their products
00:57:00.240 so what have we learned from 230 that
00:57:03.059 applies to your situation with AI
00:57:07.020 thank you for the question Senator I I
00:57:09.300 don't know yet exactly what the right
00:57:11.339 answer here is I'd love to collaborate
00:57:13.140 with you to figure it out I do think for
00:57:15.780 a very new technology we need a new
00:57:18.000 framework
00:57:19.020 certainly companies like ours bear a lot
00:57:21.420 of responsibility for the tools that we
00:57:23.099 put out in the world but tool users do
00:57:24.780 as well and how we want and also people
00:57:28.079 that will build on top of it between
00:57:29.520 them and the the end consumer
00:57:31.740 um and how we want to come up with a
00:57:33.180 live a liability framework there
00:57:35.640 is a super important question
00:57:38.160 um and we'd love to work together
00:57:40.559 the point I want to make is this when it
00:57:42.420 came to online platforms the inclination
00:57:44.400 of the government was get out of the way
00:57:46.380 this is a new industry don't over
00:57:48.660 regulate it in fact give them some
00:57:50.819 breathing space and see what happens I'm
00:57:53.760 not sure I'm happy with the outcome as I
00:57:55.680 look at online platforms me either and
00:57:57.839 the harms that they have created
00:58:00.079 problems that we've seen demonstrated in
00:58:03.119 this committee child exploitation cyber
00:58:05.880 bullying online drug sales and more
00:58:09.119 I don't want to repeat that mistake
00:58:10.740 again and what I hear is the opposite
00:58:12.960 suggestion from the private sector and
00:58:15.599 that is come in and the front end of
00:58:17.160 this thing and establish some liability
00:58:18.900 standards Precision regulation
00:58:22.559 for a major company like IBM to come
00:58:24.660 before this committee and say to the
00:58:26.339 government please regulate us
00:58:29.640 can you explain the difference in
00:58:31.740 thinking from the past and now
00:58:34.680 yeah absolutely so for us this comes
00:58:38.040 back to the issue of trust and Trust in
00:58:40.260 the technology trust is our license to
00:58:42.180 operate as I mentioned in my remarks and
00:58:45.240 so we firmly believe and we've been
00:58:47.040 calling for precision regulation of
00:58:48.900 artificial intelligence for years now
00:58:50.760 this is not a new position we think that
00:58:53.880 technology needs to be deployed in a
00:58:56.280 risk responsible and clear way that
00:58:59.339 people we've taken principles around
00:59:01.740 that trust and transparency we call them
00:59:04.200 are principles that were articulated
00:59:05.640 years ago and build them into practices
00:59:07.260 that's why we're here advocating for
00:59:09.359 precision regulatory approach so we
00:59:12.000 think that AI should be regulated at the
00:59:14.520 point of risk essentially and that's the
00:59:17.220 point at which technology meets Society
00:59:20.339 let's take a look at what that might
00:59:22.559 appear to be
00:59:23.940 members of Congress are pretty smart a
00:59:26.400 lot of people maybe not as smart as we
00:59:28.260 think we are many times and government
00:59:30.420 certainly has a capacity to do amazing
00:59:33.420 things
00:59:34.380 but when you talk about our ability to
00:59:37.559 respond to the current Challenge and
00:59:39.540 perceive challenge of the future
00:59:41.359 challenges which you all have described
00:59:44.099 in terms which are
00:59:45.599 hard to
00:59:47.460 forget
00:59:48.660 as you said Mr Altman things can go
00:59:50.880 quite wrong she said Mr Marcus democracy
00:59:54.059 is threatened I mean
00:59:56.579 the magnitude of the challenge you're
00:59:58.380 giving us is substantial I'm not sure
01:00:01.020 that we respond quickly and with enough
01:00:03.059 expertise to deal with it
01:00:05.460 Professor Marcus you made a reference to
01:00:08.520 CERN
01:00:09.720 the international Arbiter of nuclear
01:00:12.540 research I suppose I don't know if
01:00:14.520 that's a fair characterization but it's
01:00:16.260 a characterization I'll start with
01:00:19.380 what is it what agency of this
01:00:21.480 government do you think exists that
01:00:23.700 could respond to the challenge that
01:00:25.559 you've laid down today
01:00:27.599 we have many agencies that can respond
01:00:31.020 in some ways for example the FTC
01:00:35.040 um we have CC and there are many
01:00:36.900 agencies that can but my view is that we
01:00:39.960 probably need a cabinet level uh
01:00:42.619 organization within the United States in
01:00:45.359 order to address this and my reasoning
01:00:48.119 for that is that the number of risks is
01:00:52.079 large the amount of information to keep
01:00:55.140 up on is so much I think we need a lot
01:00:58.500 of technical expertise I think we need a
01:01:00.540 lot of coordination of these efforts so
01:01:02.819 there is one model here where we stick
01:01:04.740 to only existing law and try to shape
01:01:08.160 all of what we need to do and each
01:01:10.440 agency does their own thing but I think
01:01:12.299 that AI is going to be such a large part
01:01:14.579 of our future and is so complicated and
01:01:17.220 moving so fast this does not fully solve
01:01:19.980 your problem about a dynamic world but
01:01:23.280 it's a step in that direction to have an
01:01:24.900 agency that's full-time job is to do
01:01:27.240 this I personally have suggested in fact
01:01:29.460 that we should want to do this at a
01:01:31.200 global way I wrote an article on The
01:01:33.180 Economist I have a link in here an
01:01:35.400 invited essay for the economist
01:01:37.099 suggesting we might want an
01:01:38.819 international Agency for AI That's what
01:01:41.339 I wanted to go to next and that is the
01:01:43.140 fact that I'll get it aside from the
01:01:45.660 CERN and nuclear examples because
01:01:47.819 government was involved in that from day
01:01:50.220 one at least in the United States but
01:01:52.500 now we're dealing with Innovation which
01:01:54.660 doesn't necessarily have a boundary
01:01:57.559 we may create a great U.S agency and I
01:02:00.960 hope that we do that may have
01:02:02.819 jurisdiction over U.S corporations and
01:02:05.040 U.S activity but doesn't have a thing to
01:02:07.440 do with what's going to bombard us from
01:02:09.720 outside the United States how do you
01:02:12.059 give this International Authority
01:02:14.339 the authority to regulate in a fair way
01:02:17.819 for all entities involved in AI I think
01:02:20.640 that's probably over my pay grade I
01:02:23.400 would like to see it happen and I think
01:02:25.079 it may be inevitable that we push there
01:02:27.059 I mean I think the politics behind it
01:02:29.460 are obviously complicated I'm really
01:02:31.859 heartened by the degree to which this
01:02:33.480 room is bipartisan and supporting the
01:02:36.000 same things and that makes me feel like
01:02:37.980 it might be possible I would like to see
01:02:40.319 the United States take leadership in
01:02:41.880 such organization it has to involve the
01:02:44.579 whole world and not just the US to work
01:02:46.559 properly I think even from the
01:02:48.839 perspective of the companies it would be
01:02:50.280 a good thing so the companies themselves
01:02:52.440 do not want a situation where you take
01:02:54.240 these models which are expensive to
01:02:56.040 train and you have to have 190 some of
01:02:58.619 them you know one for every country that
01:03:01.200 that wouldn't be a good way of operating
01:03:02.819 when you think about the energy costs
01:03:04.380 alone just for training these systems it
01:03:06.839 would not be a good model if every
01:03:08.280 country has its own policies and each
01:03:10.619 for each jurisdiction every company has
01:03:13.440 to train another model and maybe you
01:03:15.780 know different states are different so
01:03:17.220 Missouri and California have different
01:03:18.900 rules and so then that requires even
01:03:21.599 more training of these expensive models
01:03:23.339 with huge climate impact
01:03:25.500 um and I mean just it would be very
01:03:27.000 difficult for the companies to operate
01:03:28.500 if there was no Global coordination and
01:03:30.359 so I think that we might get the
01:03:32.160 companies on board if there's bipartisan
01:03:34.380 support here and I think there's support
01:03:36.420 around the world that is entirely
01:03:38.280 possible that we could develop such a
01:03:40.020 thing but obviously there are many you
01:03:42.000 know nuances here of diplomacy that are
01:03:43.980 over my pay grade I I would love to
01:03:46.020 learn from you all to try to help make
01:03:47.460 that happen Mr Altman can I win just
01:03:50.280 briefly briefly please uh I want to Echo
01:03:52.799 support for what Mr Marcus said I think
01:03:54.599 the U.S should lead here and do things
01:03:56.880 first but to be effective uh we do need
01:04:00.599 something Global as you mentioned this
01:04:02.099 can this can happen everywhere there is
01:04:03.960 precedent I know it sounds naive to call
01:04:06.240 for something like this and it sounds
01:04:07.319 really hard there is precedent we've
01:04:09.000 done it before with the iaea we've
01:04:12.000 talked about doing it for other
01:04:13.020 Technologies
01:04:14.420 they're given what it takes to make
01:04:16.980 these models the chip supply chain the
01:04:19.680 sort of limited number of competitive
01:04:21.059 gpus the power the US has over these
01:04:23.579 companies I think there are paths to
01:04:26.819 the U.S setting some International
01:04:28.319 standards that other countries would
01:04:30.839 need to collaborate with and be part of
01:04:32.700 that are actually workable even though
01:04:34.319 it sounds on its face like a impractical
01:04:36.900 idea and I think it would be great for
01:04:39.299 the world thank you Mr chairman thank
01:04:40.920 you thanks Senator German and in fact I
01:04:43.260 think we're going to hear more about
01:04:44.460 what Europe is doing European Parliament
01:04:47.700 already is acting on an AI act uh on
01:04:52.619 social media Europe is ahead of us uh we
01:04:56.220 need to be in the lead I think
01:04:59.819 your your point is very well taken let
01:05:02.520 me turn to Senator Graham
01:05:05.940 Senator Blackburn thank you Mr chairman
01:05:08.880 and thank you all for being here with us
01:05:11.460 today I put into my chat GPT account
01:05:14.579 should Congress regulate AI chat GPT and
01:05:18.780 it gave me four Pros four cons and says
01:05:21.960 ultimately the decision rests with
01:05:24.180 Congress and deserves careful
01:05:26.220 consideration so on that you know it was
01:05:29.819 uh very balanced I recently visited with
01:05:33.240 the Nashville Technology Council I
01:05:35.640 represent Tennessee and of course you
01:05:38.460 had people there from Health Care
01:05:39.900 Financial Services Logistics educational
01:05:43.260 entities and they're concerned about
01:05:45.900 what they see happening with AI with the
01:05:50.099 utilizations for their companies Miss
01:05:52.140 Montgomery you know some similar to you
01:05:54.240 they've got Health Care people are
01:05:55.980 looking at disease analytics they're
01:05:58.980 looking at predictive diagnosis how this
01:06:01.980 can better the outcomes for patients la
01:06:05.880 Logistics industry looking at ways to
01:06:08.520 save time and money and yield
01:06:10.380 efficiencies
01:06:11.839 you've got financial services that are
01:06:15.480 saying how does this work with Quantum
01:06:17.280 how does it work with blockchain how can
01:06:19.920 we use this but uh it I think as we have
01:06:25.559 talked with them Mr chairman one of the
01:06:28.079 things that continues to come up is yes
01:06:31.039 Professor Marcus as you were saying the
01:06:33.420 EU different entities are ahead of us in
01:06:36.420 this but we have never established a
01:06:40.020 federally preem given preemption for
01:06:43.500 online privacy for data security and put
01:06:46.619 some of those foundational elements in
01:06:49.500 place which is something that we need to
01:06:52.020 do as we look at this and it will
01:06:56.520 require that Commerce Committee
01:06:58.920 Judiciary Committee decide how we move
01:07:01.859 forward so that people own their virtual
01:07:05.039 you and um
01:07:08.460 Mr Altman I was glad to see last week
01:07:11.700 that your open AI models are not going
01:07:16.140 to be trained using consumer data I
01:07:19.859 think that that is important and if we
01:07:21.660 have a second round I've got a host of
01:07:23.640 questions for you on data security and
01:07:27.480 privacy
01:07:28.819 but I think it's important to let people
01:07:32.039 control their virtual you their
01:07:35.280 information in these settings and I want
01:07:37.380 to come to you on music and content
01:07:40.319 creation because we've got a lot of
01:07:42.299 songwriters and artists and a I think we
01:07:45.720 have the best creative community on the
01:07:48.180 face of the Earth they're in Tennessee
01:07:51.440 and they should be able to decide if
01:07:55.260 they're copyrighted songs and images are
01:07:58.619 going to be used to train these models
01:08:01.819 and I'm concerned about open ai's
01:08:06.000 jukebox
01:08:07.500 it offers some re-renditions in the
01:08:11.099 style of Garth Brooks which suggests
01:08:14.339 that open AI is trained on Garth Brooks
01:08:17.819 songs I went in this weekend and I said
01:08:20.880 write me a song that sounds like Garth
01:08:23.040 Brooks and it gave me a different
01:08:25.259 version of Simple Man
01:08:27.960 so it's interesting that it would do
01:08:31.500 that but you're training it on these
01:08:33.359 copyrighted songs these MIDI files these
01:08:36.238 sound Technologies so as you do this who
01:08:41.339 owns the rights to that AI generated
01:08:45.238 material and using your technology could
01:08:50.040 I
01:08:51.359 remake a song insert content from my
01:08:56.399 favorite artist and then on the creative
01:09:00.060 rights to that song
01:09:02.939 thank you Senator uh this is an area of
01:09:05.580 great interest to us
01:09:08.279 I would say first of all we think that
01:09:10.140 creators deserve control over how their
01:09:13.319 Creations are used and
01:09:16.380 what happens sort of beyond the point of
01:09:19.080 them releasing it into the world
01:09:21.420 um second I think that we need to figure
01:09:24.060 out new ways with this new technology
01:09:25.979 that creators can win succeed have a
01:09:28.920 vibrant life and I'm optimistic that
01:09:31.738 this will present it then let me ask you
01:09:33.600 this how do you compensate the art the
01:09:36.060 artist that's exactly what I was going
01:09:37.560 to say okay
01:09:38.460 um we'd like to we're working with
01:09:40.140 artists now visual artists musicians uh
01:09:42.660 to figure out what people want there's a
01:09:44.880 lot of different opinions unfortunately
01:09:46.140 and at some point we'll have let me ask
01:09:47.819 you this do you favor something like
01:09:49.738 Sound Exchange that has worked in the
01:09:53.100 area of
01:09:54.540 radio I'm not familiar with live
01:09:57.060 exchange I'm sorry okay you've got your
01:09:59.699 team behind you get back to me on that
01:10:01.679 that would be a third party entity okay
01:10:04.199 so let's discuss that let me move on
01:10:08.520 um can you commit as you've done with
01:10:12.600 consumer data not to train chat GPT open
01:10:16.380 AI jukebox or other AI models on artists
01:10:21.179 and songwriters copyrighted works or use
01:10:24.480 their voices and their likenesses
01:10:27.900 without first receiving their consent so
01:10:32.040 first of all jukebox is not a product we
01:10:33.780 offer that was a research release but
01:10:35.580 it's not you know unlike chatgpt or
01:10:37.860 dollar but we've lived through Napster
01:10:41.280 yes but that was something that really
01:10:45.360 cost a lot of artists a lot of money
01:10:48.480 oh I understand yeah for sure digital
01:10:50.340 distribution era I don't I don't know
01:10:53.040 the numbers on jukebox on the top of my
01:10:54.900 head as a research release I can I can
01:10:56.460 follow up with your office but it's not
01:10:57.900 jukebox is not something that gets much
01:11:00.120 attention or usage it was put out to to
01:11:02.100 show that something's possible well
01:11:03.480 Senator Durbin just said you know and I
01:11:06.120 think it's a fair warning to you all if
01:11:09.540 we're not involved in this from the
01:11:12.540 get-go and you all already are a long
01:11:15.179 way down the path on this but if we
01:11:17.520 don't step in then this gets away from
01:11:21.300 you so are you working with a copyright
01:11:23.580 office are you considering protections
01:11:27.179 for Content generators and creators in
01:11:31.500 generative AI yes we are absolutely
01:11:33.780 engaged on that again to reiterate my
01:11:35.940 earlier point we think that content
01:11:37.380 creators content owners need to benefit
01:11:39.659 from this technology exactly what the
01:11:41.880 economic model is we're still talking to
01:11:43.620 artists and content owners about what
01:11:45.600 they want I think there's a lot of ways
01:11:47.280 this can happen but very clearly no
01:11:49.320 matter what the law is the right thing
01:11:50.699 to do is to make sure people get
01:11:52.260 significant upside benefit from this new
01:11:54.540 technology and we believe that it's
01:11:57.000 really going to deliver that but that
01:11:58.920 content owners likenesses people totally
01:12:02.219 deserve control over how that's used and
01:12:04.140 to benefit from it okay so on privacy
01:12:06.840 the and how do you plan to account for
01:12:09.480 the collection of voice and other
01:12:13.280 user-specific data things that are
01:12:16.640 copyrighted user-specific data through
01:12:20.159 your AI applications because if I can go
01:12:23.159 in and say write me a song that sounds
01:12:25.020 like Garth Brooks and it takes part of
01:12:27.659 an existing song there has to be a
01:12:31.380 compensation to that artist for that
01:12:35.640 utilization and that use if it was radio
01:12:38.219 play it would be there if it was
01:12:40.020 streaming it would be there so if you're
01:12:43.679 going to do that what is your policy for
01:12:48.420 making certain you're accounting for
01:12:50.340 that and you're protecting that
01:12:53.159 individual's right to privacy and their
01:12:56.940 right to secure that data and that
01:13:00.480 created work so a few thoughts about
01:13:02.940 this uh number one we think that people
01:13:05.159 should be able to say I don't want my
01:13:07.080 personal data trained on
01:13:08.760 that's I think that's right that gets to
01:13:11.040 a national Privacy Law which many of us
01:13:13.860 here on the Deus are working toward
01:13:16.260 getting something that we can use yeah I
01:13:19.140 think a strong privacy my time's expired
01:13:20.940 like I yelled back thank you Mr chip
01:13:23.340 thanks Senator Blackburn Senator thank
01:13:25.980 you very much Mr chairman um and uh
01:13:28.460 Senator Blackburn I love Nashville I
01:13:31.080 love Tennessee love your music but I
01:13:33.360 will say I use chat GPT and just ask
01:13:36.000 what are the top creative song artists
01:13:38.880 of all time and two of the top three
01:13:40.860 were from Minnesota that would be uh
01:13:43.920 Prince I'm sure they moved Prince and
01:13:45.900 Bob Dylan okay all right so let us let
01:13:48.600 us continue one thing AI won't change
01:13:50.880 and you're seeing it here all right so
01:13:54.120 on a more serious note though my staff
01:13:56.880 and I
01:13:58.020 um in my role as chair of the rules
01:13:59.580 committee and leading a lot of the
01:14:00.900 election bill and we just introduced a
01:14:03.000 bill that's
01:14:04.640 representative Yvette Clark from New
01:14:06.719 York introduced over the house Senator
01:14:08.400 Booker and Bennett and I did
01:14:10.159 on political advertisements but that is
01:14:13.140 just of course the tip of the iceberg
01:14:14.760 you know this from your discussions with
01:14:16.440 Senator Hawley and others about the
01:14:18.300 images and my own view it was Senator
01:14:21.659 grams of section 230 is is that we just
01:14:25.080 can't let people make stuff up and then
01:14:27.179 not have any consequence but I'm going
01:14:29.219 to focus in on what my job one of my
01:14:31.320 jobs will be on the rules committee and
01:14:32.880 that is election misinformation and we
01:14:36.060 just asked Chef GPT to do a tweet about
01:14:40.140 a polling location in Bloomington
01:14:42.420 Minnesota and said there are long lines
01:14:44.580 at this polling location
01:14:46.219 at atonement Lutheran Church of where
01:14:49.500 should we go now albeit it's not an
01:14:51.719 election right now but the answer the
01:14:53.880 tweet that was drafted was a completely
01:14:56.100 fake thing go to one two three four Elm
01:14:59.460 Street and so you can imagine what I'm
01:15:02.580 concerned about here with an election
01:15:04.860 upon us with primary elections upon us
01:15:07.980 that we're going to have all kinds of
01:15:09.900 misinformation and I just want to know
01:15:12.179 what you're planning on doing it doing
01:15:14.040 about it I know we're going to have to
01:15:15.600 do something soon not just for the
01:15:17.699 images of the candidates but also for
01:15:20.460 misinformation about the actual polling
01:15:23.040 place is and election rules
01:15:25.980 thank you Senator that
01:15:28.080 we we talked about this a little bit
01:15:29.880 earlier we are quite concerned about the
01:15:32.280 impact this can have on elections I
01:15:34.260 think this is an area where hopefully
01:15:35.880 the entire industry and the government
01:15:37.080 can work together quickly
01:15:39.120 there's there's many approaches and I'll
01:15:42.000 talk about some of the things we do but
01:15:43.199 before that
01:15:44.820 I think it's tempting to use the frame
01:15:46.800 of social media
01:15:48.679 but this is not social media this is
01:15:51.239 different and so the the response that
01:15:53.219 we need is different you know this is a
01:15:55.679 tool that a user is using to help
01:15:58.260 generate content more efficiently than
01:15:59.880 before they can change it they can test
01:16:01.980 the accuracy of it if they don't like it
01:16:03.960 they can get another version
01:16:05.880 um but it still then spreads through
01:16:08.460 social media or other ways like chat gbt
01:16:10.860 is a you know single player experience
01:16:13.199 where you're just using this
01:16:15.600 um
01:16:16.500 and so I think as we think about what to
01:16:18.300 do that's that's important to understand
01:16:20.300 that there's a lot that we can and do do
01:16:23.400 there
01:16:24.360 um there's things that the model refuses
01:16:26.760 to generate we have policies uh we also
01:16:29.460 importantly have monitoring so at scale
01:16:31.940 uh we can detect someone generating a
01:16:34.860 lot of those tweets even if generating
01:16:36.540 one tweet is okay yeah and of course
01:16:38.460 there's going to be other platforms and
01:16:40.020 if they're all spouting out fake
01:16:42.000 election information I just that I think
01:16:44.760 what happened in the past with Russian
01:16:47.100 interference and like it's just going to
01:16:49.140 be a tip of the iceberg with some of
01:16:51.900 those fake ads so that's number one
01:16:53.940 number two is the impact on intellectual
01:16:58.199 property and Senator Blackburn was
01:17:00.360 getting at some of this with song rights
01:17:03.060 um and I have serious concerns about
01:17:05.460 that but news content so Senator Kennedy
01:17:09.000 and I have a bill that was really quite
01:17:11.100 straightforward that would simply
01:17:13.040 allowed the
01:17:15.420 um the new news organizations an
01:17:18.000 exemption to be able to negotiate with
01:17:20.340 basically Google and Facebook Microsoft
01:17:22.560 was supportive of the bill but basically
01:17:24.659 negotiate with them to get better rates
01:17:27.960 and be able to not have some leverage
01:17:32.040 and other countries are doing this
01:17:33.780 Australia and the like and so my
01:17:36.120 question is when we already have a study
01:17:37.920 by Northwestern predicting that
01:17:39.420 one-third of the U.S newspapers are that
01:17:41.940 roughly existed two decades are going to
01:17:44.460 go are going to be gone by 2025 unless
01:17:47.580 you start compensating for everything
01:17:50.040 from book movies books yes but also news
01:17:53.820 content we're going to lose any
01:17:56.640 realistic content producers and so I'd
01:17:59.940 like your response to that and of course
01:18:01.620 there is an exemption for copyright in
01:18:03.900 section 230 but I think asking little
01:18:07.739 newspapers to go out and Sue all the
01:18:09.659 time just can't be the answer they're
01:18:11.280 not going to be able to keep up yeah
01:18:13.440 like
01:18:14.940 it is my hope that tools like what we're
01:18:17.880 creating can help news organizations do
01:18:20.100 better I think having a vibrant
01:18:22.739 having a vibrant National media is
01:18:24.960 critically important and let's call it
01:18:27.000 round one of the internet has not been
01:18:28.320 great for that right we're talking here
01:18:30.239 about local that you know report on your
01:18:32.219 high school football scores and a
01:18:33.840 scandal in your city council those kinds
01:18:35.699 of things for sure they're the ones that
01:18:36.960 are actually getting the worst the
01:18:39.120 little radio stations and broadcast but
01:18:42.120 do you understand that this could be
01:18:43.679 exponentially worse in terms of local
01:18:45.960 news content if they're not compensated
01:18:49.020 well because what they need is to be
01:18:51.060 compensated for their content and not
01:18:52.800 have it stolen yeah again our our model
01:18:56.040 you know our the current version of uh
01:18:58.860 gpt4 ended training in 2021 it's not
01:19:01.560 it's not it's not a good way to find
01:19:03.600 recent news uh and it's I don't think
01:19:05.820 it's a service that can do a great job
01:19:08.100 of linking out although maybe with our
01:19:09.420 plugins it's it's possible uh if there
01:19:12.239 are things that we can do to help local
01:19:13.679 news we would certainly like to again I
01:19:15.719 think it's it's critically important
01:19:18.120 um May I add something there yeah but
01:19:19.980 let me just ask you a question you can
01:19:21.420 combine them quick more transparency on
01:19:23.760 the platforms
01:19:25.560 um Senator Coons and Senator Cassidy and
01:19:27.780 I have the platform accountability
01:19:29.659 transparency act to give researchers
01:19:32.820 access to this information of the
01:19:34.920 algorithms and the like on social media
01:19:37.020 data would that be helpful and then why
01:19:39.000 don't you just say yes or no and then go
01:19:41.159 at his uh the transparency is absolutely
01:19:43.560 critical here to understand the
01:19:45.600 political ramifications the bias
01:19:47.520 ramifications and so forth we need
01:19:48.900 transparency about the data we need to
01:19:51.060 know more about how the models work we
01:19:52.620 need to have scientists have access to
01:19:54.480 them I was just going to amplify your
01:19:56.219 earlier point about local news a lot of
01:19:58.980 news is going to be generated by these
01:20:00.540 systems they're not reliable News Guard
01:20:02.219 already is a study I'm sorry it's not in
01:20:03.780 my appendix but I will get it to your
01:20:05.580 office showing that something like 50
01:20:07.800 websites are already generated by Bots
01:20:11.540 we're going to see much much more of
01:20:13.860 that and it's going to make it even more
01:20:15.179 competitive for the local news
01:20:16.560 organizations and so the quality of the
01:20:19.620 sort of overall news Market is going to
01:20:21.420 decline as we have more generated
01:20:23.400 content by systems that aren't actually
01:20:25.080 reliable in the content they're
01:20:26.280 generated thank you and thank you at a
01:20:28.440 very timely basis to make the argument
01:20:30.120 why we have to mark up this bill again
01:20:31.800 in June I appreciate it thank you
01:20:34.320 Senator Graham
01:20:36.659 thank you Mr chairman and Senator Hawley
01:20:38.580 for having this I'm trying to find out
01:20:40.440 how it is different than social media
01:20:42.300 and learn
01:20:43.320 from the mistakes we made with social
01:20:45.120 media the idea of not suing social media
01:20:48.659 companies is to allow the internet to
01:20:50.760 flourish because
01:20:52.280 if I slander you you can sue me
01:20:56.960 if you're a billboard company and you
01:20:59.400 put up the slander can you sue the
01:21:01.080 billboard company we said no basically
01:21:04.620 section 230 is being used by social
01:21:06.900 media companies to high to avoid
01:21:09.659 liability uh for activity that other
01:21:12.719 people generate
01:21:14.219 when they refuse to comply with their
01:21:17.100 terms of use a mother calls up the
01:21:19.260 company and says this app is being used
01:21:22.140 to bully my child to death you promise
01:21:24.840 in the terms of use you would prevent
01:21:26.699 bullying
01:21:27.900 and she calls three times she gives no
01:21:31.380 response the child kills herself and
01:21:33.600 they can't sue
01:21:35.340 do you all agree we don't want to do
01:21:37.440 that again
01:21:39.239 yes
01:21:43.080 if I may speak for one second there's a
01:21:44.820 fundamental distinction between
01:21:46.640 reproducing content and generating
01:21:48.780 content
01:21:50.159 yeah but you you would like liability
01:21:52.080 where people are harmed
01:21:53.640 absolutely
01:21:55.739 yes in fact IBM has been publicly
01:21:59.280 advocating to condition liability on a
01:22:01.739 reasonable Care standard so let me just
01:22:03.719 make sure I understand the law as it
01:22:05.219 exists today Mr Solomon thank you for
01:22:07.380 coming your company is not claiming that
01:22:10.560 section 230 applies to the tool you have
01:22:13.679 created
01:22:15.300 yeah we're claiming we need to work
01:22:16.920 together to find a totally new approach
01:22:18.780 I don't think section 230 is the even
01:22:21.780 the right framework okay so under the
01:22:25.080 law it exists today this tool you create
01:22:28.080 if I'm harmed by it can I sue you
01:22:30.540 that is beyond my area of legalization
01:22:33.080 not for that no have you ever been sued
01:22:36.179 at all uh the your company yeah open AI
01:22:39.120 gets sued uh yeah we've gotten sued
01:22:42.060 before okay and what for
01:22:45.659 um
01:22:46.560 I mean they've mostly been like pretty
01:22:48.540 frivolous things like I think happens to
01:22:50.219 any company but like the examples my
01:22:52.980 colleagues have given from artificial
01:22:55.320 intelligence that could literally ruin
01:22:57.300 our lives
01:22:58.679 can we go to the company that created
01:23:00.900 that tool in suom is that your
01:23:02.520 understanding yeah I think there needs
01:23:04.739 to be clear responsibility by the
01:23:07.320 companies but you're not claiming any
01:23:09.780 kind of legal protection
01:23:12.000 like section 2 the 230 applies to your
01:23:14.820 industry is that correct no I don't
01:23:16.860 think we're I don't I don't think we're
01:23:18.060 saying anything Mr Marcus when it comes
01:23:20.580 to Consumers there seems to be like
01:23:22.260 three time tested ways to protect
01:23:24.060 consumers against any product
01:23:26.640 um statutory schemes which are
01:23:29.580 non-existent here
01:23:32.100 um
01:23:32.820 legal systems which may be appear here
01:23:36.179 but not social media and agencies
01:23:39.659 go back to
01:23:41.460 Senator Hawley
01:23:42.900 the atom bomb
01:23:44.880 has put a cloud over Humanity
01:23:47.820 but nuclear power could be one of the
01:23:49.739 solutions to climate change
01:23:53.280 so what I'm trying to do is make sure
01:23:55.320 that you just can't go build a nuclear
01:23:57.780 power plant hey Bob what would you like
01:23:59.280 to do today let's go build a nuclear
01:24:00.719 power plant you have a nuclear
01:24:02.219 Regulatory Commission that governs
01:24:04.679 how you build a plant
01:24:06.540 and it's licensed do you agree Mr
01:24:08.699 Alderman that these tools you're
01:24:10.260 creating should be licensed yeah we've
01:24:12.480 been calling for this we think any
01:24:13.980 that's the simplest way you get a
01:24:16.560 license and do you agree with me the the
01:24:19.140 simplest way and the most effective way
01:24:21.239 is have an agency that is more Nimble
01:24:24.060 and smarter than Congress which should
01:24:25.620 be easy to create
01:24:27.780 overlooking what you do yes we'd be
01:24:30.000 enthusiastic about that you agree with
01:24:31.320 that Mr Marcus absolutely do you agree
01:24:34.080 with that that's Montgomery
01:24:36.300 I would have some nuances I think we
01:24:38.699 need to build on what we have in place
01:24:40.199 already today we don't have an agency
01:24:42.120 Regulators wait a minute nope nope nope
01:24:45.000 we don't have an agency that regulates
01:24:46.679 the technology so should we have one but
01:24:49.679 a lot of the issues I I don't think so a
01:24:53.760 lot of these wait a minute so IBM says
01:24:56.520 we don't need an agency uh
01:24:59.640 interesting should we have a license
01:25:01.320 required for these tools so so what we
01:25:04.260 believe is that we need to write a
01:25:06.120 simple question should you get a license
01:25:08.219 to produce one of these tools I think it
01:25:10.920 comes back to some of them potentially
01:25:13.199 yes so what I said at the onset is that
01:25:16.199 we need to um clearly Define risks
01:25:19.560 you claim section 230 applies in this
01:25:22.679 area at all we are not a platform
01:25:24.420 company and we've again long advocated
01:25:26.520 for a reasonable Care standard in
01:25:28.199 section I just don't understand how you
01:25:30.239 could say
01:25:31.500 that you don't need an agency to deal
01:25:33.780 with the most transformative technology
01:25:35.760 maybe ever
01:25:37.500 well
01:25:42.600 transformative technology that can
01:25:44.760 disrupt Life as we know it good and bad
01:25:47.340 I think it's a transformative technology
01:25:49.560 certainly and the conversations that
01:25:51.719 we're having here today have been really
01:25:53.880 bringing to light the fact that this is
01:25:55.920 the domains and the issues this one with
01:25:58.199 you has been very enlightening to me Mr
01:26:00.960 Allman why are you so willing to have an
01:26:04.320 agency
01:26:07.080 Senator we've been clear about what we
01:26:09.000 think the upsides are and I think you
01:26:10.199 can see from users how much they enjoyed
01:26:12.239 how much value they're getting out of it
01:26:13.380 but we've also been clear about what the
01:26:14.880 downsides are and so that's why we think
01:26:16.739 we need an agency system it's a major
01:26:18.719 tool to be used by a lot it's a major
01:26:20.580 new technology
01:26:21.860 yeah if you make a ladder and the ladder
01:26:25.020 doesn't work you can see the people made
01:26:26.520 the letter but there are some standards
01:26:28.020 out there to make a letter so that's why
01:26:30.060 we're agreeing with you yeah that's
01:26:31.380 right I think you're on the right track
01:26:32.880 so here's what my two cents worth for
01:26:35.460 the committee is that we need to empower
01:26:37.560 an agency
01:26:39.060 that issues in a license and can take it
01:26:41.699 away
01:26:43.560 wouldn't that be some incentive to do it
01:26:47.159 right if you could actually be taken out
01:26:49.800 of business clearly that should be part
01:26:51.659 of what an agency can do now
01:26:53.460 and you also agree that China is doing
01:26:55.699 AI research is that right correct
01:26:58.860 this world organization that doesn't
01:27:00.840 exist maybe it will but if you don't do
01:27:03.659 something about the China part of it
01:27:05.760 you'll never quite get this right do you
01:27:07.620 agree
01:27:09.000 well that that's why I think it doesn't
01:27:11.040 necessarily have to be a world
01:27:12.360 organization but there has to be some
01:27:14.280 sort of and there's a lot of options
01:27:15.600 here there has to be some sort of
01:27:17.300 standard some sort of set of controls
01:27:19.440 that do have Global effects yeah because
01:27:20.940 you know other people doing this I got
01:27:22.739 15 military application
01:27:25.320 how can AI change the Warfare
01:27:30.120 and you got one minute I got one minute
01:27:32.520 yeah all right this is that's a tough
01:27:34.800 question for one minute
01:27:36.480 um this is very far out of my area of
01:27:38.760 expertise uh but I give you one example
01:27:40.860 a drone can a drone you program you can
01:27:44.040 plug into a drone the coordinates and it
01:27:46.139 can fly out it goes over this Target and
01:27:48.900 it drops a missile on this car moving
01:27:51.239 down the road and somebody's watching it
01:27:53.400 could AI create a situation where a
01:27:56.340 drone can select the target itself
01:27:58.800 I think we shouldn't allow that well can
01:28:00.960 it be done sure thanks
01:28:04.860 thanks underground Thunder
01:28:07.560 thank you Senator Blumenthal Senator
01:28:09.239 Hawley for convening this hearing for
01:28:11.100 working closely together to come up with
01:28:13.199 this compelling panel of witnesses and
01:28:16.440 beginning a series of hearings on this
01:28:18.960 transformational technology but we
01:28:21.360 recognize the immense promise and
01:28:24.120 substantial risks associated with
01:28:26.699 generative AI Technologies we know these
01:28:28.920 models can make us more efficient help
01:28:31.020 us learn new skills open whole new
01:28:33.480 vistas of creativity but we also know
01:28:36.780 that generative AI can authoritatively
01:28:40.260 deliver wildly incorrect information it
01:28:43.679 can hallucinate as is often described it
01:28:47.340 can impersonate loved ones it can
01:28:48.840 encourage self-destructive behaviors and
01:28:51.600 it can shape public opinion and the
01:28:54.239 outcome of Elections Congress thus far
01:28:56.880 has demonstrably failed to responsibly
01:28:59.340 enact meaningful regulation of social
01:29:01.860 media companies with serious harms that
01:29:04.500 have resulted that we don't fully
01:29:06.060 understand Senator Clovis are referenced
01:29:08.400 in her questioning a bipartisan bill
01:29:10.800 that would open up social media
01:29:13.320 platforms underlying algorithms we have
01:29:16.260 struggled to even do that to understand
01:29:18.300 the underlying technology and then to
01:29:20.940 move towards responsible regulation we
01:29:23.400 cannot afford to be as late to
01:29:26.280 responsibly regulating generative AI as
01:29:29.340 we have been to social media because the
01:29:31.920 consequences both positive and negative
01:29:33.960 will exceed those of social media by
01:29:36.540 orders of magnitude so let me ask a few
01:29:39.480 questions designed to get at both how we
01:29:42.300 assess the risk what's the role of
01:29:44.280 international regulation and how does
01:29:46.800 this impact AI Mr Altman I appreciate
01:29:49.620 your testimony about the ways in which
01:29:51.420 open AI assesses the safety of your
01:29:54.300 models through a process of iterative
01:29:55.980 deployment the fundamental question
01:29:57.900 embedded in that process though is how
01:29:59.580 you decide whether or not a model is
01:30:01.860 safe enough to deploy and safe enough to
01:30:04.860 have been built and then let go into the
01:30:07.560 wild I understand one way to prevent
01:30:10.440 generative AI models from providing
01:30:12.420 harmful content is to have humans
01:30:15.060 identify that content and then train the
01:30:17.580 algorithm to avoid it there's another
01:30:19.620 approach that's called constitutional AI
01:30:21.960 that gives the model a set of values or
01:30:24.540 principles to guide its decision making
01:30:27.179 would it be more effective to give
01:30:28.980 models these kinds of rules instead of
01:30:31.260 trying to require or compel training the
01:30:34.080 model on all the different potentials
01:30:36.060 for harmful content
01:30:38.040 thank you Senator it's it's a great
01:30:39.540 question uh I like to frame it by
01:30:41.699 talking about why we deploy at all like
01:30:43.800 why we put these systems out into the
01:30:45.420 world there's the obvious answer about
01:30:47.880 there's benefits and people are using it
01:30:49.739 for all sorts of wonderful things and
01:30:51.120 getting great value and that makes us
01:30:53.040 happy but a big part of why we do it is
01:30:55.980 that we believe that iterative
01:30:58.739 deployment and giving people in our
01:31:00.600 institutions and you all time to come to
01:31:03.120 grips with this technology to understand
01:31:04.560 it to find its limitations and benefits
01:31:06.960 that the regulations we need around it
01:31:08.760 what it takes to make it safe that's
01:31:10.800 really important going off to build a
01:31:13.380 super powerful AI system in secret and
01:31:15.780 then dropping it on the world all at
01:31:17.159 once I think would not go well so a big
01:31:20.100 part of our strategy is while these
01:31:22.320 systems are still relatively weak and
01:31:24.600 deeply imperfect to find ways to get
01:31:27.239 people to have experience with them to
01:31:29.699 have contact with reality and to figure
01:31:32.400 out what we need to do to make it safer
01:31:34.260 and better and that is the only way that
01:31:36.900 I've seen in the history of new
01:31:39.360 technology and products of this
01:31:40.860 magnitude to get to a very good outcome
01:31:42.960 and so that that interaction with the
01:31:45.300 world is very important now of course
01:31:47.159 before we put something out uh it needs
01:31:50.219 to meet a bar of safety and uh and again
01:31:53.460 we spent well over six months with gpt4
01:31:56.100 after we finished training it going
01:31:58.020 through all of these different things
01:31:59.760 and deciding also what the standards
01:32:01.920 were going to be before we put something
01:32:03.600 out there trying to find the harms that
01:32:05.760 we knew about uh put it and and how to
01:32:07.980 address those one of the things that's
01:32:09.420 been gratifying to us is even some of
01:32:10.980 our biggest critics have looked at gpt4
01:32:13.080 and said wow open AI made huge progress
01:32:15.000 on I could focus briefly on whether or
01:32:17.400 not a constitutional model that gives
01:32:19.020 values would be worth it I was just
01:32:20.639 about to get there all right sorry about
01:32:22.020 that
01:32:22.800 um
01:32:23.760 yeah I think giving the models values up
01:32:26.580 front uh is an extremely important
01:32:30.179 Set uh you know rlhf is another way of
01:32:32.820 doing that same thing but somehow or
01:32:34.500 other you are with synthetic data or
01:32:36.360 human generated data you're saying here
01:32:38.340 are the values here's what I want you to
01:32:40.260 reflect or here are the wide bounds of
01:32:42.900 everything that Society will allow and
01:32:44.280 then within there you pick as the user
01:32:45.960 you know if you want value system over
01:32:47.520 here or value system over there
01:32:49.560 um we think that's very important
01:32:51.000 there's multiple technical approaches
01:32:52.920 but
01:32:54.000 we need to give policy makers and the
01:32:56.940 world as a whole the tools to say here's
01:32:59.400 the values and Implement them thank you
01:33:01.380 Ms Montgomery you serve on an AI ethics
01:33:03.900 Board of a long established company that
01:33:06.900 has a lot of experience with AI I'm
01:33:09.420 really concerned that generative AI
01:33:10.980 Technologies can undermine the faith of
01:33:13.500 democratic values and the institutions
01:33:15.179 that we have the Chinese are insisting
01:33:17.760 that AI as being developed in China
01:33:20.420 reinforce the core values of the Chinese
01:33:22.679 Communist Party in the Chinese system
01:33:24.440 and I'm concerned about how we
01:33:27.659 promote AI that reinforces and
01:33:29.820 strengthens open markets open societies
01:33:31.920 and democracy in your testimony you're
01:33:34.080 advocating for AI regulation tailored to
01:33:36.780 the specific way the technology is being
01:33:38.940 used not the underlying technology
01:33:40.920 itself and the EU is moving ahead with
01:33:43.500 an AI act which categorizes AI products
01:33:47.159 based on level of risk you all in
01:33:49.199 different ways have said that you view
01:33:50.460 elections and the shaping of election
01:33:52.620 outcomes and disinformation that can
01:33:55.139 influence elections as one of the
01:33:56.940 highest risk cases one that's entirely
01:33:59.159 predictable we have attempted so far
01:34:01.739 unsuccessfully to regulate social media
01:34:04.199 after the demonstrably harmful impacts
01:34:06.480 of social media on our last several
01:34:08.699 elections what advice do you have for us
01:34:11.100 about what kind of approach we should
01:34:12.780 follow and whether or not the EU
01:34:14.580 direction is the right one to pursue
01:34:17.460 I mean the conception of the EU AI Act
01:34:20.580 is very consistent with this concept of
01:34:23.940 precision regulation where you're
01:34:25.679 regulating the use of the technology in
01:34:28.139 context so absolutely that approach
01:34:31.020 makes a ton of sense it's what
01:34:33.600 um I advocated for at the onset
01:34:35.540 different rules for different risks so
01:34:38.040 in the case of Elections absolutely any
01:34:40.500 algorithm being used in that context
01:34:42.360 should be required to have disclosure
01:34:45.600 around the data being used the
01:34:48.239 performance of the model anything along
01:34:50.820 those lines is really important guard
01:34:53.400 rails need to be in place and and on the
01:34:55.440 point just come back to the question of
01:34:57.060 of whether we need an independent agency
01:34:59.400 I mean I think we don't want to slow
01:35:03.060 down regulation to address real risks
01:35:06.360 right now right so we have existing
01:35:09.360 regulatory authorities in place who have
01:35:11.880 been clear that they have the ability to
01:35:14.940 regulate in their respective domains a
01:35:16.980 lot of the issues we're talking about
01:35:18.239 today span multiple domains elections
01:35:20.760 and the like so if I could I'll just
01:35:23.340 assert that those existing regulatory
01:35:25.320 bodies and authorities are
01:35:26.460 under-resourced and lack many of the
01:35:28.860 statutes regulatory powers that they
01:35:30.960 need correct we have failed to deliver a
01:35:33.360 data privacy even though industry has
01:35:35.400 asking us to regulate data privacy if I
01:35:38.580 might Mr Marcus I'm I'm interested also
01:35:41.340 what international bodies are best
01:35:43.860 positioned to convene multilateral
01:35:46.139 discussions to promote responsible
01:35:47.820 standards we've talked about a model
01:35:49.920 being CERN and nuclear energy I'm
01:35:52.980 concerned about proliferation and
01:35:55.020 non-proliferation we've also talked I
01:35:57.360 would suggest that the ipcc a U.N body
01:36:00.320 helped at least provide a scientific
01:36:02.639 Baseline of what's happening in climate
01:36:05.100 change so that even though we may
01:36:06.659 disagree about strategies globally we've
01:36:09.480 come to a common understanding of what's
01:36:11.760 happening and what should be the
01:36:13.440 direction of intervention I'd be
01:36:15.420 interested Mr Marcus if you could just
01:36:16.860 give us
01:36:17.880 um your thoughts on who's the right body
01:36:20.040 internationally to convene a
01:36:21.659 conversation and one that could also
01:36:23.580 reflect our values I'm still feeling my
01:36:26.340 way on that issue I think global
01:36:28.620 politics is not my specialty I'm an AI
01:36:31.560 researcher but I I have moved towards
01:36:34.320 policy in in recent months really
01:36:36.360 because of my great concern about all of
01:36:38.760 these risks I think certainly the UN
01:36:40.739 UNESCO has its guidelines should be
01:36:44.520 involved and at the table and maybe
01:36:46.320 things work under them and maybe they
01:36:47.880 don't but they should have a strong
01:36:49.139 voice and and help to develop this the
01:36:51.600 oacd has also been thinking greatly
01:36:54.300 about this number of organizations have
01:36:56.520 internationally I I don't feel like I
01:36:58.800 personally am qualified to say exactly
01:37:00.480 what the right model is there well thank
01:37:02.940 you I think we need to pursue this both
01:37:05.159 at the national level and the
01:37:06.480 international level I'm the chair of the
01:37:08.219 IP subcommittee of the Judiciary
01:37:10.020 Committee in June and July we will be
01:37:12.179 having hearings on the impact of AI on
01:37:15.060 patents and copyrights you can already
01:37:17.040 tell from the questions of others there
01:37:18.780 will be a lot of interest I look forward
01:37:20.159 to following up with you about that
01:37:21.960 topic I know Mr chairman I look forward
01:37:23.760 to helping as much as possible thank you
01:37:25.500 very much thanks Senator Coons Senator
01:37:27.420 Kennedy
01:37:29.820 thank you thank you all for being here
01:37:33.300 permit me to share with you three
01:37:36.080 hypotheses that I would like you
01:37:39.300 to assume for the moment to be true
01:37:43.280 hypothesis number one
01:37:45.960 many members of Congress do not
01:37:48.900 understand
01:37:49.940 artificial intelligence
01:37:54.440 hypothesis number two
01:37:58.139 that absence of understanding
01:38:02.060 may not prevent Congress
01:38:06.179 from plunging in with enthusiasm
01:38:09.620 and trying to regulate this technology
01:38:13.139 in a way that could hurt this technology
01:38:18.260 hypothesis number three that I would
01:38:20.820 like you to assume
01:38:23.820 there is likely
01:38:26.760 a berserk wing of the artificial
01:38:29.940 intelligence community
01:38:32.940 that intentionally or unintentionally
01:38:36.960 could use artificial intelligence
01:38:41.040 to kill all of us and hurt us the entire
01:38:44.580 time that we are dying
01:38:47.760 soon all of those to be true
01:38:51.659 please tell me in plain English
01:38:55.080 two or three reforms regulations if any
01:39:00.120 that you would you would Implement if
01:39:03.060 you were queen or King for a Day
01:39:07.800 Ms Montgomery
01:39:12.900 I think it comes back again to
01:39:14.580 transparency and explainability in AI we
01:39:18.120 absolutely need to know and have
01:39:19.679 companies attest what do you mean by
01:39:21.659 transparency
01:39:23.159 so disclosure of the data that's used to
01:39:27.480 train AI disclosure of the model and how
01:39:31.620 it performs and making sure that there's
01:39:34.620 continuous governance over these models
01:39:37.320 that we are the Leading Edge into
01:39:39.179 governance Foundation
01:39:41.520 technology governance organizational
01:39:43.860 governance
01:39:45.199 rules and clarification that are needed
01:39:48.540 that this which Congress I mean this is
01:39:50.940 your chance folks to tell us how to get
01:39:53.280 this right please use it
01:39:55.560 all right I mean I think again the rules
01:39:57.360 should be focused on the use of AI in
01:40:00.780 certain contexts so if you look at for
01:40:02.760 example so if you look at the EU AIS it
01:40:06.840 has certain
01:40:08.580 um uses of AI that it says are just
01:40:11.760 simply too dangerous and will be
01:40:13.739 outlawed in these okay so we ought to
01:40:15.780 first pass along that says you can use a
01:40:17.880 uh for these uses but not others is that
01:40:21.179 is that what you're saying we need to
01:40:22.800 define the highest risk usage is there
01:40:24.540 anything else
01:40:26.520 um and then of course uh requiring
01:40:28.920 things like impact assessments and
01:40:30.659 transparency requiring companies to show
01:40:34.620 their work
01:40:35.840 protecting data that's used to train AI
01:40:39.120 in the first place as well Professor
01:40:41.580 Marcus if you could be specific
01:40:43.860 this was your shot man
01:40:46.500 talking plain English and tell me what
01:40:49.440 if any rules we ought to implement and
01:40:53.400 police don't just use Concepts I'm
01:40:55.920 looking for specificity number one a
01:40:59.520 safety review like we use with the FDA
01:41:02.040 prior to widespread deployment if you
01:41:04.320 can introduce something to 100 million
01:41:07.020 people somebody has to have their
01:41:08.760 eyeballs on it there you go okay that's
01:41:11.580 a good one number sure I agree with it
01:41:13.620 but that's a good one what else you
01:41:15.600 didn't ask for three that you would
01:41:16.620 agree with number two
01:41:18.719 a Nimble monitoring agency to follow
01:41:21.600 what's going on not just pre-review but
01:41:24.300 also post as things are out there in the
01:41:26.340 world with authority to call things back
01:41:28.500 which we've discussed today and number
01:41:30.540 three would be funding geared towards
01:41:32.520 things like AI Constitution AI that can
01:41:36.060 reason about what it's doing I would not
01:41:38.580 leave things entirely to current
01:41:40.139 technology which I think is poor at
01:41:42.540 behaving in ethical fashion and behaving
01:41:44.940 in honest fashion and so I would have
01:41:47.159 funding to try to basically focus on AI
01:41:49.800 Safety Research that term has a lot of
01:41:52.199 complications in my field there's both
01:41:54.960 safety let's say short term and long
01:41:56.520 term and I think we need to look at both
01:41:58.619 rather than just funding models to be
01:42:00.900 bigger which is the popular thing to do
01:42:02.639 we need to find projects to be more
01:42:04.560 trustworthy because I want to hear from
01:42:06.239 Mr Alton Mr Altman here's your shot
01:42:08.940 thank you Senator uh number one I would
01:42:10.920 form a new agency that licenses any
01:42:12.780 effort above a certain scale of
01:42:14.280 capabilities and can take that license
01:42:16.260 away and ensure compliance with safety
01:42:17.940 standards number two I would create a
01:42:19.860 set of safety standards focused on what
01:42:21.960 you said in your third hypothesis as the
01:42:23.940 dangerous capability evaluations one
01:42:26.280 example that we've used in the past is
01:42:27.659 looking to see if a model can
01:42:28.860 self-replicate and self-exfiltrate into
01:42:31.500 the wild we can give your office a long
01:42:33.300 other list of the things that we think
01:42:34.500 are important there but specific tests
01:42:36.540 that a model has to pass before it can
01:42:38.280 be deployed into the world and then
01:42:40.080 third I would require independent audits
01:42:42.540 so not just from the company or the
01:42:44.159 agency but experts who can say the model
01:42:46.139 is or isn't in compliance with these
01:42:47.880 State and safety thresholds and these
01:42:49.260 percentages of performance on question X
01:42:51.119 or Y
01:42:52.320 can you send me that information we will
01:42:54.659 do that
01:42:56.159 um would you be qualified
01:42:58.560 to uh to to uh if we promulgated those
01:43:02.400 rules to administer those rules I love
01:43:04.619 my current job
01:43:07.739 cool are there people out there that
01:43:09.540 would be qualified we'd be happy to send
01:43:10.980 you recommendations for people out there
01:43:12.360 yes okay
01:43:14.400 you make a lot of money do you I make no
01:43:16.800 I paid enough for health insurance I
01:43:18.719 have no equity in open AI really yeah
01:43:20.460 that's interesting you need a lawyer I
01:43:23.219 need a what you need a lawyer or an
01:43:25.380 agent I I'm doing this because I love it
01:43:29.219 thank you Mr chairman thanks Senator
01:43:32.100 Kennedy uh senator hirono
01:43:37.080 thank you Mr chairman
01:43:39.480 chairman listening to to all of you
01:43:41.159 testifying thank you very much for being
01:43:42.780 here uh clearly AI truly is a
01:43:47.520 game-changing tool and uh we need to get
01:43:52.739 the regulation of this tool right
01:43:55.139 because my staff for example asks AI it
01:43:59.760 might have been gpt4 it might have been
01:44:02.760 I don't know one of the other
01:44:04.460 entities to create a song that my
01:44:08.280 favorite band BTS
01:44:10.739 a favorite a song that they would sing
01:44:13.440 somebody else's song but you know
01:44:15.420 neither of the artists were involved in
01:44:18.000 creating what sounded like a really
01:44:19.860 genuine song so you can do a lot we also
01:44:23.580 asked can
01:44:25.500 um can there be a speech created talking
01:44:28.440 about the the Supreme Court decision in
01:44:30.840 Dobbs and the chaos that it created
01:44:32.580 using my voice my kind of voice and it
01:44:35.580 created a speech that was really good it
01:44:37.619 almost made me think about you know what
01:44:39.060 do I need my staff for so
01:44:41.880 don't worry that's not very nervous
01:44:43.980 laughter behind you oh
01:44:45.780 their jobs are safe but there's so much
01:44:48.060 that can be done and one of the things
01:44:50.100 that you mentioned Mr May Altman that
01:44:52.320 intrigued me was you said gpt4 can
01:44:54.900 refuse harmful requests so you must have
01:44:58.139 put some thought into how your system if
01:45:01.139 I can call it that can refuse harmful
01:45:04.500 requests what what do you consider a
01:45:06.060 harmful request you can just keep it
01:45:08.280 short yeah uh
01:45:10.560 I'll give a few examples one would be
01:45:12.659 about violent content another would be
01:45:14.880 about content that's encouraging
01:45:15.900 self-harm another is adult content not
01:45:18.300 that we think adult content is
01:45:19.560 inherently harmful but there's things
01:45:21.540 that could be associated with that that
01:45:22.980 we cannot reliably enough differentiate
01:45:24.780 so we refuse all of it so those are some
01:45:26.699 of the more obvious harmful kinds of of
01:45:29.219 information but in the election context
01:45:31.860 for for example I saw a picture of a
01:45:35.460 former president Trump being arrested by
01:45:37.679 NY PD and that went viral I don't know
01:45:40.380 is that considered harmful I've seen all
01:45:43.199 kinds of statements attributed to any
01:45:45.600 one of us that could be put out there
01:45:47.159 that may not be that may not rise to
01:45:49.739 your level of harmful content but there
01:45:52.020 you have it so two of two of you said
01:45:54.900 that we should have a licensing scheme I
01:45:57.540 can't Envision or imagine right now what
01:45:59.940 kind of our licensing scheme we would be
01:46:02.400 able to create two pretty much regulate
01:46:05.280 the vastness of of the this game to
01:46:10.440 changing tools so
01:46:13.460 are you thinking of an FTC kind of a
01:46:16.860 system an FCC kind of a system what do
01:46:19.860 the two of you even Envision as a
01:46:21.960 potential licensing scheme that would
01:46:24.300 provide the kind of guard Wheels that we
01:46:27.300 need to protect our literally our
01:46:30.300 country from harmful content to touch on
01:46:32.699 the first part of what what you said
01:46:34.020 there are things besides uh you know
01:46:36.960 should this content be generated or not
01:46:38.340 that I think are also important so
01:46:40.260 that image that you mentioned was
01:46:41.520 generated I think it'd be a great policy
01:46:43.500 to say generated images need to be made
01:46:45.780 clear in all contexts that they were
01:46:47.280 generated and you know then we still
01:46:49.560 have the image out there but it's we're
01:46:51.480 at least requiring people to say this
01:46:52.920 was a generated image
01:47:01.280 where I think the licensing scheme comes
01:47:04.020 in is uh not with not for what these
01:47:06.659 models are capable of today because as
01:47:08.880 you pointed out you don't need a new
01:47:10.980 licensing agency to do that but as we as
01:47:13.619 we head and you know this may take a
01:47:15.540 long time I'm not sure as we head
01:47:17.159 towards artificial general intelligence
01:47:19.500 and the impact that will have and the
01:47:22.139 power of that technology I think we need
01:47:24.119 to treat that as seriously as we treat
01:47:26.280 other very powerful Technologies and
01:47:27.960 that's where I personally think we need
01:47:29.340 such a such a scheme I agree and that is
01:47:31.619 why by the time we're talking about AGI
01:47:34.260 we're talking about about major harms
01:47:36.719 that can occur through the use of AGR so
01:47:40.340 Professor Marcus having what kind of a
01:47:42.719 regulatory scheme would you envision and
01:47:44.520 what and we can't just come up with
01:47:46.380 something you know that is going to be
01:47:49.080 of uh take care of the issues that will
01:47:52.080 arise in the future especially with AGI
01:47:54.119 so what what kind of a scheme would you
01:47:56.340 contemplate well first if I can rewind
01:47:58.739 just a moment um I think you really put
01:48:00.960 your finger on the central scientific
01:48:02.400 issue in terms of the challenges in
01:48:04.860 building artificial intelligence we
01:48:06.719 don't know how to build a system that
01:48:08.280 understands harm in the full breadth of
01:48:10.080 its meaning so what we do right now is
01:48:12.060 We Gather examples and we say is this
01:48:13.920 like the examples that we have labeled
01:48:15.960 before but that's not broad enough and
01:48:18.179 so I thought you're questioning
01:48:19.320 beautifully outlined the challenge that
01:48:22.320 AI itself has to face in order to to
01:48:24.780 Really deal with this we want AI itself
01:48:26.580 to understand harm and that may require
01:48:28.679 new technology so I think that's very
01:48:30.480 important on this second part of your
01:48:32.580 question the model that I tend to
01:48:35.460 gravitate towards but I am not an expert
01:48:37.139 here is the FDA at least as part of it
01:48:39.420 in terms of you have to make a safety
01:48:41.820 case and say why the benefits out weigh
01:48:45.000 the Harms in order to get that license
01:48:46.619 probably we need elements of multiple
01:48:48.780 agencies I'm not an expert there but I
01:48:50.880 think that the safety case part of it is
01:48:53.100 incredibly important you have to be able
01:48:54.840 to have external reviewers that are
01:48:57.060 scientifically qualified look at this
01:48:58.920 and say you have you addressed enough so
01:49:00.360 I'll just give one specific example Auto
01:49:02.520 GPT frightens me that's not something
01:49:04.739 that open AI made but something that
01:49:06.480 openai did make call called chat GPD
01:49:09.119 plugins led a few weeks later to some
01:49:11.460 Building open source software called
01:49:13.500 Auto GPT and what Auto GPT does is it
01:49:16.739 allows systems to access source code
01:49:18.719 access the internet and so forth and
01:49:20.820 there are a lot of potential let's say
01:49:22.440 cyber security risks there there should
01:49:24.659 be an external agency that says well we
01:49:26.940 need to be reassured if you're going to
01:49:28.500 release this product that there aren't
01:49:30.960 going to be cyber security problems or
01:49:32.400 there are ways of addressing it so
01:49:33.780 Professor I am running
01:49:39.540 your
01:49:41.400 um is the the a used model similar to
01:49:44.159 what the EU has come up with but the vas
01:49:46.280 vastness of AI and the complexities
01:49:49.380 involve I think would require more than
01:49:52.080 looking at the use use of it and I think
01:49:55.440 that based on what I'm hearing today
01:49:57.840 don't you think that that uh we're
01:50:00.780 probably going to need to do a heck of a
01:50:02.340 lot more than to focus on what use it as
01:50:04.800 being AI is being used for for example
01:50:07.260 you can ask AI to come up with a funny
01:50:10.380 joke or something but you can use the
01:50:12.239 same you can ask the same AI tool to
01:50:15.179 generate something that is like an
01:50:16.920 election fraud kind of a situation so I
01:50:19.080 don't know how you will make a
01:50:20.699 determination based on where you're
01:50:22.679 going with the use model how to
01:50:24.780 distinguish those kinds of uses of this
01:50:27.900 tool so I think that if we're going to
01:50:30.900 go toward a licensing kind of a scheme
01:50:32.699 we're going to need to put a lot of
01:50:34.260 thought into how we're going to come up
01:50:36.119 with an appropriate scheme that is going
01:50:38.940 to provide the kind of future reference
01:50:42.780 that we need to put in place so I I
01:50:45.960 thank all of you for coming in and
01:50:48.320 providing further food for thought thank
01:50:51.480 you Mr chairman
01:50:53.100 thanks very much Senator hirono uh
01:50:55.920 Senator Padilla
01:50:57.540 thank you Mr chairman I appreciate the
01:51:00.119 flexibility as I've been uh back and
01:51:02.639 forth to uh maintain this committee and
01:51:05.400 Homeland Security committee where
01:51:06.900 there's a hearing going on right now on
01:51:09.480 the use of AI in government so it's AI
01:51:12.600 day on the hill right this is the Senate
01:51:14.219 apparently now for folks watching at
01:51:16.920 home if you never thought about AI until
01:51:19.260 the recent emergence of generative AI
01:51:22.440 tools the developments in this space may
01:51:25.920 feel like they've just happened all of a
01:51:27.719 sudden
01:51:28.500 but the fact of the matter is Mr chairs
01:51:30.600 that they haven't AI is not new not for
01:51:33.119 government not for business not for
01:51:35.280 public not for the public in fact the
01:51:38.219 public uses AI all the time and just for
01:51:40.679 folks to be able to relate want to offer
01:51:42.600 the example of anybody with a smartphone
01:51:45.020 uh many features on your device leverage
01:51:48.480 AI including suggested replies right
01:51:51.719 over text messaging or even to email
01:51:54.420 autocorrect features including but
01:51:57.119 unlimited to spelling in their email and
01:51:59.520 text applications so I'm frankly excited
01:52:03.840 to explore how we can facilitate
01:52:06.000 positive AI Innovation that benefits
01:52:09.600 Society while addressing some of the
01:52:13.139 already known harms and biases that stem
01:52:15.540 from the development and use of the
01:52:17.460 tools today
01:52:19.199 now with language models becoming
01:52:21.540 increasingly ubiquitous I want to make
01:52:24.179 sure that there's a focus on ensuring
01:52:25.739 Equitable treatment of diverse
01:52:28.440 demographic groups my understanding is
01:52:30.960 that most research in to evaluating and
01:52:34.500 mitigating fairness harms has been
01:52:37.020 concentrated
01:52:38.460 on the English language while
01:52:40.920 non-english languages have received
01:52:43.320 comparatively little attention or
01:52:45.960 investment and we've seen this problem
01:52:47.580 before I'll tell you why I raised this
01:52:49.739 social media companies for example have
01:52:52.199 not adequately invested in content
01:52:54.780 moderation tools and resources for their
01:52:57.900 non-english uh in in non-english
01:53:01.739 language
01:53:02.760 and I share this not just out of concern
01:53:04.760 for non-us-based users but
01:53:09.080 so many us-based users prefer a language
01:53:12.420 other than English in their
01:53:13.920 communication
01:53:15.420 so I'm deeply concerned about repeating
01:53:17.340 social media's failure in AI tools and
01:53:21.239 applications question Mr Altman and Miss
01:53:23.940 Montgomery how are open Ai and IBM
01:53:26.520 ensuring Language and Cultural
01:53:29.040 inclusivity that they're in their uh
01:53:32.400 large language models
01:53:35.639 and it's even an area of focus in the
01:53:37.920 development of your products
01:53:40.440 so bias and equity in technology is a
01:53:44.820 focus of ours and always has been I
01:53:46.800 think diversity in terms of the
01:53:48.480 development of the tools in terms of
01:53:50.340 their deployment so having diverse
01:53:52.199 people that are actually training those
01:53:54.300 tools considering the downstream effects
01:53:57.060 as well we're also very cautious uh very
01:54:02.040 aware of the fact that we can't just be
01:54:03.840 articulating and calling for these types
01:54:06.719 of things without having the tools and
01:54:08.219 the technology to test for bias and to
01:54:11.100 apply governance across the life cycle
01:54:13.139 of AI so we were one of the first teams
01:54:15.840 and companies to put tool kits on the
01:54:19.440 market deploy them contribute them to
01:54:21.960 open source that will do things like
01:54:23.580 help to address you know be the
01:54:25.440 technical aspects in which we help to
01:54:27.780 address issues like bias
01:54:29.820 can you speak just for a second
01:54:31.679 specifically to language inclusivity
01:54:34.679 yeah I mean language so we don't have a
01:54:37.139 consumer platform but we are very
01:54:39.960 actively involved with ensuring that the
01:54:42.540 technology we hope to deploy in the
01:54:44.400 large language models that we use in
01:54:46.860 helping our clients to deploy technology
01:54:49.580 is focused on and available in many
01:54:53.520 languages thank you the shop we think
01:54:56.219 this is really important one example is
01:54:58.500 that we worked with the government of
01:54:59.760 Iceland which is a language with fewer
01:55:01.980 speakers than many of the languages that
01:55:03.540 are well represented on the internet to
01:55:06.540 ensure that their language was included
01:55:08.100 in our model and we've had many similar
01:55:10.679 conversations and I look forward to many
01:55:12.960 similar Partnerships with lower resource
01:55:15.179 languages to get them into our models
01:55:17.960 gpt4 is uh unlike previous models of
01:55:21.659 ours which were good at English and not
01:55:23.460 very good at other languages uh now
01:55:25.679 pretty good at a large number of
01:55:27.600 languages you can go pretty far down the
01:55:29.520 list ranked by number of speakers and
01:55:31.679 still get good performance but for these
01:55:33.659 very small languages we're excited about
01:55:35.280 custom Partnerships to include that
01:55:37.560 language into our model run and the part
01:55:40.560 of the question you asked about values
01:55:42.060 and making sure that cultures are
01:55:43.619 included we're equally focused on that
01:55:45.739 excited to work with people who have
01:55:48.000 particular data sets and to work to
01:55:50.760 collect a representative set of values
01:55:52.380 from around the world to draw these wide
01:55:54.420 bounds of what the system can do I also
01:55:56.580 appreciate what you said about the the
01:55:58.440 benefits of these systems and wanting to
01:56:00.060 make sure we get those to as wide of a
01:56:01.679 group as possible I think this will
01:56:03.840 these systems will have lots of positive
01:56:05.760 impact on a lot of people but in
01:56:07.679 particular underrepresented
01:56:10.739 historically underpresented groups in
01:56:12.360 technology people who have not had as
01:56:14.159 much technology around the world this
01:56:16.199 technology seems like it can be a big
01:56:17.820 lift up
01:56:19.580 and uh I know my
01:56:22.199 question was specific to language
01:56:24.480 inclusivity but uh glad there's
01:56:26.820 agreement on the broader uh commitment
01:56:29.580 to uh diversity and inclusion and I'll
01:56:32.280 just give a couple more reasons why they
01:56:34.320 think it's so critical you know the
01:56:36.000 largest actors in this space can afford
01:56:39.119 the massive amount of data the computing
01:56:41.460 power and they have the financial
01:56:43.500 resources necessary to develop complex
01:56:45.659 AI systems
01:56:49.260 but in this space we haven't seen from a
01:56:51.540 Workforce standpoint the racial and
01:56:54.239 gender diversity reflective of the
01:56:56.400 United States of America and we risk if
01:56:58.739 we're not thoughtful about it uh
01:57:00.679 contributing to the development of tools
01:57:03.060 and approaches that only exacerbate uh
01:57:06.840 the bias and inequities that exist in
01:57:09.719 our society so a lot of follow-up work
01:57:11.340 to do there in my time remaining I do
01:57:13.440 want to ask one more question this
01:57:16.500 committee and the public are right to
01:57:18.239 pay attention to the emergence of
01:57:19.739 generative AI now this technology has a
01:57:22.739 different opportunity and risk profile
01:57:24.840 than other AI tools and these
01:57:27.360 applications have felt very tangible for
01:57:29.159 the public due to the nature of the user
01:57:31.860 interface and the outputs that they
01:57:34.139 produce
01:57:35.159 but I don't think we should lose that of
01:57:36.719 the broader AI ecosystem as we consider
01:57:39.119 ai's broader impact on society as well
01:57:42.000 as the design of appropriate safeguards
01:57:44.280 so miss Montgomery in your testimony uh
01:57:48.060 as you noted AI is not you can you
01:57:50.219 highlight some of the different
01:57:51.380 applications that the public and policy
01:57:54.300 makers should also keep in mind as we
01:57:56.520 consider possible regulations
01:58:00.599 yeah I mean I think the generative AI
01:58:03.300 systems that are available today are
01:58:05.400 creating new issues that necess that
01:58:07.619 need to be studied new issues around the
01:58:10.139 potential to generate content that could
01:58:12.000 be extremely misleading deceptive and
01:58:14.820 alike so those issues absolutely need to
01:58:17.280 be studied but we shouldn't also ignore
01:58:20.040 the fact that AI is a tool it's been
01:58:21.960 around for a long time it has
01:58:23.699 capabilities Beyond just generative
01:58:25.980 capabilities and again that's why I
01:58:28.020 think going back to this approach where
01:58:29.940 we're regulating AI where it's touching
01:58:32.040 people and Society is a really important
01:58:34.260 way to address it
01:58:37.260 thank you Mr
01:58:39.000 chair thanks Senator uh Senator Booker
01:58:41.880 is next but I think he's going to defer
01:58:44.040 to senator ossoff
01:58:46.400 is a very big deal I don't know
01:58:50.219 I have a meeting at noon and I'm
01:58:52.199 grateful to you Senator Booker for
01:58:54.840 yielding your time you are as always
01:58:57.599 brilliant and handsome and thank you to
01:59:01.080 the panelists for joining us thank you
01:59:04.080 to the subcommittee leadership for
01:59:05.880 opening this up to all committee members
01:59:08.280 if we're going to contemplate a
01:59:10.139 regulatory framework we're gonna have to
01:59:12.540 Define what it is that we're regulating
01:59:14.219 so you know Mr Alban any such law will
01:59:17.580 have to include a section that defines
01:59:19.560 the scope of regulated activities
01:59:21.420 Technologies tools products
01:59:23.760 just take a stab at it
01:59:25.619 yeah thanks for asking Senator also I
01:59:27.360 think I think it's super important I
01:59:29.280 think there are very different levels
01:59:30.599 here and I think it's important that any
01:59:33.179 any new approach any new law does not
01:59:36.540 stop the Innovation from happening with
01:59:38.280 smaller companies open source models
01:59:39.960 researchers that are doing work at a
01:59:42.420 smaller scale uh that's a wonderful part
01:59:45.480 of this ecosystem and of America we
01:59:47.040 don't want to slow that down there still
01:59:48.900 may need to be some rules there but I
01:59:51.960 think we could draw
01:59:53.580 a line at systems that need to be
01:59:56.099 licensed in a very intense way the
01:59:59.159 easiest way to do it I'm not sure if
02:00:00.540 it's the best but the easiest would be
02:00:01.800 to talk about the amount of compute that
02:00:03.840 goes into such a model so we could
02:00:05.520 divide you know we could Define a
02:00:07.080 threshold of compute and it'll have to
02:00:08.400 go it'll have to change it could go up
02:00:10.679 or down I could down as we discover more
02:00:12.719 efficient algorithms that says above
02:00:15.119 this amount of compute you are in this
02:00:17.219 regime what I would prefer it's hard to
02:00:20.639 do but I think more accurate is to
02:00:22.619 Define some capability thresholds and
02:00:25.199 say a model that can do things X Y and Z
02:00:27.480 up to all to decide that's now in this
02:00:30.060 licensing regime but models that are
02:00:31.800 less capable you know we don't want to
02:00:33.360 stop our open source Community we don't
02:00:35.040 want to stop individual researchers we
02:00:36.840 don't want to stop new startups can
02:00:38.159 proceed you know with a different
02:00:39.420 framework thank you as concisely as you
02:00:41.280 can please state which capabilities
02:00:43.080 you'd propose we'd consider for the
02:00:44.820 purposes of this definition
02:00:47.400 uh I would love rather than to do that
02:00:49.560 off the cuff to follow up with your
02:00:50.940 office with like well perhaps openings
02:00:53.060 opine understanding that you're just
02:00:55.619 responding uh and you're not making law
02:00:57.900 all right in the spirit of just a pining
02:01:00.119 um I think a model that can uh persuade
02:01:03.119 manipulate influence
02:01:04.739 person's Behavior or a person's beliefs
02:01:06.960 that would be a good threshold I think a
02:01:09.060 model that could help create novel
02:01:11.099 biological agents would be a great
02:01:12.780 threshold
02:01:13.920 things like that I want to talk about
02:01:15.900 the predictive capabilities of the
02:01:18.119 technology and we have to think about a
02:01:21.179 lot of very complicated constitutional
02:01:22.920 questions that arise from it
02:01:24.900 with with massive data sets
02:01:29.760 the integrity and accuracy with which
02:01:33.360 such technology can predict future human
02:01:35.940 behaviors potentially pretty significant
02:01:38.520 at the individual level correct
02:01:40.500 I think we don't know the answer to that
02:01:41.940 for sure but let's say it can at least
02:01:43.320 have some impact there okay so we may be
02:01:45.840 confronted by situations where for
02:01:47.760 example a law enforcement agency
02:01:49.920 deploying such technology seeks some
02:01:52.679 kind of judicial consent to execute a
02:01:54.659 search
02:01:55.500 or to take some other police action on
02:01:57.599 the basis of a modeled prediction about
02:02:00.119 some individual's Behavior uh
02:02:04.500 but that's very different from the kind
02:02:06.119 of evidentiary predicate that normally
02:02:08.099 police would take to a judge in order to
02:02:10.320 get a warrant
02:02:11.400 talk me through how you think about that
02:02:13.080 issue
02:02:14.699 yeah I think it's very important that we
02:02:17.159 continue to understand that these are
02:02:19.320 tools that humans use to make human
02:02:20.940 judgments and that we don't take away
02:02:22.320 human judgment I don't think that people
02:02:24.960 should be prosecuted based off of the
02:02:26.760 output of an AI system for example we
02:02:29.219 have no uh National Privacy Law
02:02:33.679 the Europe has has rolled one out to
02:02:38.239 mixed reviews do you think we need one I
02:02:41.580 think it'd be good and what would be the
02:02:43.860 qualities or purposes of such a law that
02:02:46.560 you think would make the most sense
02:02:47.520 based on your experience
02:02:49.320 again this is very far out of my of
02:02:51.179 expertise I think there's many many
02:02:52.800 people that could that are privacy
02:02:54.179 experts that could weigh on what a law
02:02:55.800 needs I'd still like I'd still like you
02:02:57.060 to weigh in
02:02:58.020 um
02:02:58.800 I mean I think a minimum is that users
02:03:01.320 should be able to to sort of opt out
02:03:03.480 from having their data used by companies
02:03:05.520 like ours or the social media companies
02:03:07.739 it should be easy to delete your data I
02:03:10.320 think those are it should but the thing
02:03:12.659 that I think is
02:03:13.739 important from my perspective running an
02:03:15.420 AI company is that if you don't want
02:03:17.099 your data used for training these
02:03:18.599 systems uh you have the right to do that
02:03:20.880 so let's think about how that will be
02:03:22.440 practically implemented I mean uh
02:03:24.360 as I understand it your tool and
02:03:26.760 certainly similar tools one of the
02:03:29.219 inputs will be
02:03:31.800 um
02:03:33.179 scraping for lack of a better word data
02:03:35.940 off of the open web right as a low-cost
02:03:38.880 way of of gathering information and
02:03:41.099 there's a vast amount of information out
02:03:45.000 there about all of us how would such a
02:03:47.880 restriction on the access or use or
02:03:51.179 analysis of such data be practically
02:03:52.739 implemented so I was speaking about
02:03:55.080 something a little bit different which
02:03:56.280 is the data that someone generates the
02:03:57.840 questions they ask our system things
02:03:59.219 that they input their training on that
02:04:00.900 data that's on the public web that's
02:04:02.820 accessible even if we don't train on
02:04:04.560 that the models can certainly link out
02:04:06.119 to it so that was not what I was
02:04:08.040 referring to I think that
02:04:10.440 you know there's ways to have your data
02:04:12.060 or there should be more ways to have
02:04:13.020 your data taken down from the public web
02:04:14.639 but certainly models with web browsing
02:04:17.040 capabilities will be able to search the
02:04:18.900 web and Link out to it
02:04:21.119 when you think about implementing a
02:04:22.920 safety or a regulatory regime to
02:04:26.040 constrain such software and to mitigate
02:04:29.699 some risk
02:04:31.980 is your view that
02:04:34.500 the federal government
02:04:36.239 would make laws such that certain
02:04:38.520 capabilities or functionalities
02:04:40.460 themselves are forbidden in potential in
02:04:43.739 other words one cannot
02:04:45.420 uh
02:04:46.679 deploy or execute code capable of X yes
02:04:50.460 or is it the act itself X only when
02:04:53.159 actually executed well I think both I'm
02:04:56.580 a Believer in defense in depth I think
02:04:58.619 that there should be limits on what a
02:05:01.380 deployed model is capable of and then
02:05:03.119 what it actually does too how are you
02:05:04.800 thinking about how kids use your product
02:05:07.679 we well you have to be I mean you have
02:05:10.500 to be 18 or up or have your parents
02:05:12.360 permission at 13 and up to use a product
02:05:14.099 but we understand that people get around
02:05:15.960 those safeguards all the time and so
02:05:17.280 what we try to do is just design a safe
02:05:19.380 product and there are decisions that we
02:05:21.119 make that we would allow if we knew only
02:05:23.099 adults were using it that we just don't
02:05:24.659 allow in the product because we know
02:05:25.980 children will use it some way or other
02:05:27.780 too in particular given how much these
02:05:30.360 systems are being used in education we
02:05:33.719 like want to be aware that that's
02:05:35.400 happening I think what and and Senator
02:05:37.800 Blumenthal has done extensive work
02:05:40.020 investigating this what we've seen
02:05:41.280 repeatedly is that companies whose
02:05:44.699 revenues depend upon volume of use
02:05:47.820 screen time intensity of use
02:05:50.360 design these systems in order to
02:05:53.520 maximize the engagement of all users
02:05:56.219 including children with with perverse
02:05:58.800 results in many cases and what I would
02:06:00.719 humbly advise you is that you get way
02:06:05.159 ahead of this issue the safety for
02:06:07.199 children of your product or I think
02:06:10.440 you're going to find that Senator
02:06:11.820 Blumenthal Senator Holly others on the
02:06:14.219 subcommittee and I
02:06:16.080 um are will look very harshly on the
02:06:18.719 deployment of technology that harms
02:06:20.400 children we couldn't agree more I think
02:06:22.260 we're out of time but I'm happy to talk
02:06:23.340 about that if I can respond go ahead
02:06:25.199 well it's up to the chairman
02:06:27.300 okay
02:06:28.619 um
02:06:29.400 I first of all I think we try to design
02:06:32.520 systems that do not maximize for
02:06:34.739 engagement in fact we're so short on
02:06:36.480 gpus uh the less people use our products
02:06:38.760 the better but we're not an Advertising
02:06:40.440 based model we're not trying to get
02:06:41.880 people to use it more and more
02:06:43.920 um
02:06:44.699 and I think that's that's a different
02:06:46.500 shape than ad supported social media
02:06:49.020 second uh
02:06:51.179 these systems do have the capability to
02:06:53.880 to influence in obvious and in very
02:06:56.880 nuanced ways and I think that's
02:06:59.280 particularly important for the safety of
02:07:00.780 children but that will that will impact
02:07:02.460 all of us one of the things that we'll
02:07:04.500 do ourselves uh regulation or not but I
02:07:06.840 think a regulatory approach would be
02:07:08.040 good for also is requirements about
02:07:11.159 how the values of these systems are set
02:07:14.340 and how these systems respond to
02:07:16.380 questions that can cause influence so
02:07:18.960 we'd love to partner with you couldn't
02:07:20.099 agree more on the importance thank you
02:07:22.739 Mr chairman for the record I just want
02:07:24.239 to say that the senator from Georgia is
02:07:26.400 also very handsome and Brilliant too
02:07:29.760 but I will uh allow that comment to
02:07:32.760 stand with without objection without
02:07:35.280 objection okay
02:07:38.699 um Mr chairman are now recognized thank
02:07:41.699 you very much thank you it's nice that
02:07:43.560 we finally got down to the ball guys
02:07:45.000 down here at the end
02:07:46.619 um I just want to thank you both this
02:07:47.760 has been one of the best hearings I've
02:07:48.780 had this Congress and uh just a
02:07:50.699 testimony to you to seeing uh the the
02:07:53.460 challenges and the opportunities that AI
02:07:55.260 presents so I appreciate you both I want
02:07:57.480 to just jump in I think very broadly and
02:07:59.400 then I'll get a little more narrow uh
02:08:01.500 Sam you said very broadly technology has
02:08:03.900 been moving like this and we are a lot
02:08:07.380 of people have been talking about
02:08:08.159 regulation and so I use the example of
02:08:10.500 the automobile what an extraordinary uh
02:08:13.980 piece of technology I mean New York City
02:08:15.960 did not know what to do with horse
02:08:17.400 manure they were having crises forming
02:08:19.619 commissions and the automobile comes
02:08:21.119 along ends that problem but at the same
02:08:23.460 time we have tens of thousands of people
02:08:25.199 dying on highways every day we have
02:08:27.060 emissions crises and the like there are
02:08:29.400 multiple federal agencies multiple
02:08:31.860 federal agencies that were created uh or
02:08:34.440 are specifically focused on regulating
02:08:37.260 cars
02:08:38.940 um and and so this idea that this
02:08:41.699 equally transforming technology is
02:08:44.340 coming and for Congress to do nothing
02:08:46.679 which is not what anybody hears is is uh
02:08:49.860 calling for little or nothing is is
02:08:51.719 obviously unacceptable uh I really
02:08:54.360 appreciate uh Senator Welsh and I have
02:08:56.040 been going back and forth uh during this
02:08:58.139 hearing and him and Bennett have a bill
02:09:00.179 talking about trying to regulate in this
02:09:02.760 space not doing so for uh social media
02:09:06.560 uh has been I think very destructive and
02:09:09.960 allowed a lot of things uh to go on that
02:09:12.599 are really causing a lot of harm and so
02:09:14.699 the question is is what kind of
02:09:15.900 Regulation you all have spoken about to
02:09:17.520 a lot of my colleagues
02:09:19.260 um and and I want to say that Miss
02:09:21.239 Montgomery and I have to give full
02:09:23.400 disclosure I'm the child of two IBM
02:09:25.139 parents
02:09:26.400 um uh but I I you know you talked about
02:09:28.820 uh defining the highest risk uses we
02:09:32.460 don't know all of them we really don't
02:09:34.560 we can't see where this is going uh
02:09:38.159 regulating at the point of risk and you
02:09:40.560 you sort of
02:09:42.119 called not for an agency and I think
02:09:43.860 when you when somebody else asks you to
02:09:45.300 specify because you don't want to slow
02:09:47.040 things down we should build on what we
02:09:48.599 have in place but you can Envision that
02:09:50.820 we can try to work on two different ways
02:09:53.760 that ultimately a specific like we have
02:09:57.420 in uh in cars EPA Nitza the federal
02:10:01.560 motor car carrier safety administration
02:10:03.780 all of these things you can imagine
02:10:05.520 something specific that is uh as Mr
02:10:08.520 Marcus points out a Nimble agency that
02:10:12.000 could do monitoring other things you
02:10:13.199 could imagine the need for something
02:10:14.639 like that correct oh absolutely yeah and
02:10:17.280 and so uh just for the record then in
02:10:20.460 addition to trying to regulate with what
02:10:22.260 we have now you would encourage Congress
02:10:24.900 and my colleague Senator Welsh to move
02:10:26.940 forward in trying to figure out the
02:10:28.380 right tailored agency to deal with what
02:10:31.020 we know and perhaps things that might
02:10:32.820 come up in the future
02:10:34.139 I would encourage Congress to make sure
02:10:36.420 it understands the technology has the
02:10:38.400 skills and resources in place to impose
02:10:41.159 regulatory requirements on the uses of
02:10:44.460 the technology and to understand
02:10:45.900 emerging risks as well so yes Mr Marcus
02:10:48.420 there's no way no way to put this genie
02:10:49.860 in the bottle globally this is It's
02:10:52.500 exploding I appreciate your thoughts and
02:10:55.020 I shared some with my staff about your
02:10:56.940 ideas what the international context is
02:10:59.400 but there's there's no way to stop this
02:11:01.679 moving forward so with that
02:11:03.540 understanding just building on what what
02:11:05.880 Miss Montgomery said what kind of
02:11:07.980 encouragement do you have as
02:11:09.239 specifically as possible to forming an
02:11:11.460 agency to using current rules and
02:11:13.260 regulations can you just put some
02:11:14.639 clarity on what you've already stated
02:11:16.080 let me just insert there are more Genies
02:11:18.780 yet to come from more bottles some
02:11:20.280 Genies are already out but we don't have
02:11:22.500 machines that can really for example
02:11:23.940 self-improve themselves we don't really
02:11:26.760 have machines that have self-awareness
02:11:28.440 and we might not ever want to go there
02:11:30.060 so there are other Genies to be
02:11:31.679 concerned about on to the main part of
02:11:34.080 your question
02:11:35.659 I think that we need to have some
02:11:38.099 International meetings very quickly with
02:11:39.840 people who have expertise in how you
02:11:41.639 grow agencies in the history of growing
02:11:44.099 agencies we need to do that in the
02:11:45.780 federal level we need to do that in the
02:11:47.099 international level I'll just emphasize
02:11:49.080 one thing I haven't as much as I would
02:11:50.639 like to which is that I think science
02:11:52.500 has to be a really important part of it
02:11:54.119 and I'll give an example we've talked
02:11:55.440 about misinformation we don't really
02:11:57.420 have the tools right now to detect and
02:11:59.400 label misinformation with nutrition
02:12:01.380 labels that we would like to we have to
02:12:03.119 build new technologies for that we don't
02:12:05.639 really have tools yet to detect a wide
02:12:08.520 uptick in cyber crime probably we
02:12:10.739 probably need new tools there we need
02:12:12.840 science to probably help us to figure
02:12:15.000 out what we need to build and also what
02:12:17.520 it is that we need to have transparency
02:12:19.260 around and understood understood see I'm
02:12:21.540 just going to you uh for the little bit
02:12:23.040 of time I have left real quick
02:12:25.260 um first of all you're a bit of a
02:12:26.699 unicorn when I sat down with you first
02:12:28.199 could you explain why non-profit in
02:12:31.139 other words you're you're you're you're
02:12:33.119 not looking this and you've even capped
02:12:35.219 the vce people just really quickly I
02:12:37.080 want folks to understand that
02:12:39.840 we started as a non-profit uh really
02:12:42.420 focused on how this technology was going
02:12:44.520 to be built at the time it was very
02:12:46.440 outside the Overton window that
02:12:48.000 something like AGI was even possible
02:12:49.619 that's that shifted a lot um we didn't
02:12:51.719 know at the time how important scale was
02:12:53.639 going to be but we did know that we
02:12:55.980 wanted to build this
02:12:57.659 um with Humanity's best interests at
02:12:59.400 heart and a belief that this technology
02:13:01.679 could if it goes the way we we want if
02:13:03.900 we can do some of those things for
02:13:05.159 Professor Marcus mentioned uh really
02:13:08.639 deeply transformed the world and we
02:13:10.020 wanted to be as much of a force for
02:13:12.179 getting to a positive I'm going to
02:13:13.800 interrupt you I think that's all good I
02:13:15.000 hope more of that gets on the record the
02:13:16.860 second part of my question as well
02:13:19.079 um I found it fascinating uh but are you
02:13:22.139 ever gonna for a revenue model for
02:13:24.000 return on your investors are you ever
02:13:25.560 going to do ads or something like that
02:13:28.920 I wouldn't say never I don't think like
02:13:31.800 I think there may be people that we want
02:13:33.659 to offer services to and there's no
02:13:35.099 other model that works but I really like
02:13:36.780 having a subscription-based model uh we
02:13:39.119 have API developers pay us and we have
02:13:45.320 one of my biggest concerns about this
02:13:47.520 space is what I've already seen in the
02:13:49.500 space of uh web 2 web 3 is this massive
02:13:52.920 corporate concentration it is really
02:13:55.199 terrifying to see how few companies now
02:13:57.480 control and affect the lives of so many
02:13:59.820 of us and these companies are getting
02:14:01.320 bigger and more powerful and I see you
02:14:03.659 know open AI back by Microsoft uh
02:14:06.000 anthropic is backed by Google Google has
02:14:08.520 its own in-house products We Know bar so
02:14:10.619 I'm really worried about that and I'm
02:14:13.199 wondering uh if Sam you can give me a
02:14:15.420 quick uh acknowledgment are you worried
02:14:17.820 about the corporate concentration in
02:14:19.079 this space and what effect it might have
02:14:20.460 uh um uh in the associated risks perhaps
02:14:24.239 with Market concentration in Ai and the
02:14:26.340 Mr Market Marcus can you answer that as
02:14:28.139 well I think there will be many people
02:14:30.300 that develop models uh what's happening
02:14:32.579 on the open source Community is amazing
02:14:34.199 but there will be a relatively small
02:14:35.699 number of providers that can make models
02:14:37.980 at the at the church
02:14:40.739 um I think there is
02:14:42.780 benefits and danger to that because
02:14:44.699 we're talking about all the dangers with
02:14:46.020 AI the fewer of us that you really have
02:14:48.300 to keep a careful eye on on the absolute
02:14:50.280 like bleeding edge of capabilities
02:14:51.719 there's benefits there but then I think
02:14:54.599 there needs to be enough in their will
02:14:55.739 because there's so much value that
02:14:57.360 consumers have choice that we have
02:14:58.619 different ideas Mr Marcus real quick
02:15:00.840 there there is a real risk of a kind of
02:15:02.940 technical technocracy combined with
02:15:05.099 oligarchy where a small number of
02:15:06.780 companies influence people's beliefs
02:15:09.179 through the nature of these systems
02:15:10.800 again I put something in the wall in the
02:15:12.480 record about the Wall Street Journal
02:15:14.040 about how these systems can subtly shape
02:15:16.380 our beliefs and that's enormous
02:15:17.820 influence on how we live our lives and
02:15:20.579 having a small number of players do that
02:15:22.380 with data that we don't even know about
02:15:24.179 that scares me Sam I'm sorry one more
02:15:26.579 thing I wanted to add one thing that I
02:15:28.380 think is very important is that that
02:15:30.119 what these systems get aligned to
02:15:32.400 whose values what those bounds are that
02:15:34.920 that is somehow set by society as a
02:15:37.560 whole by governments as a whole and so
02:15:39.060 creating that data set the Align that
02:15:41.880 our alignment data set it could be you
02:15:43.440 know an AI Constitution whatever it is
02:15:44.880 that that has got to come very broadly
02:15:47.880 from society
02:15:49.260 thank you very much Mr chairman Thomas
02:15:50.760 expired and I guess the best for last
02:15:53.219 thank you Senator Booker Senator well
02:15:56.480 first of all I want to thank you Senator
02:15:59.460 Blumenthal and you Senator Hawley this
02:16:01.739 has been a tremendous hearing uh
02:16:04.020 senators are noted for their short
02:16:06.300 attention spans but I've sat through
02:16:08.280 this entire hearing and enjoyed every
02:16:10.560 minute of it you have one of our longer
02:16:13.739 attention spans in the United States to
02:16:16.500 your great credit well we've had good
02:16:18.420 Witnesses and it's an incredibly
02:16:19.739 important issue and here's just I don't
02:16:22.980 all the questions I have have been asked
02:16:25.560 really but here's a kind of a takeaway
02:16:28.159 and what I think is the major question
02:16:31.500 that we're going to have to answer as a
02:16:33.898 congress
02:16:34.799 and number one you're here because AI is
02:16:38.638 this extraordinary new technology that
02:16:42.240 everyone says can be transformative as
02:16:45.959 much as the printing press
02:16:48.120 number two is really unknown what's
02:16:50.459 going to happen but there's a big fear
02:16:52.679 you've expressed to all of you about
02:16:54.558 what Bad actors can do and will do if
02:16:58.679 there's no rules of the road
02:17:01.939 uh number three
02:17:04.740 as a member who served in the house and
02:17:07.439 now in the Senate
02:17:09.058 I've come to the conclusion that it's
02:17:11.760 impossible for Congress to keep up with
02:17:13.978 the speed of Technology
02:17:15.660 and there have been concerns expressed
02:17:18.420 uh and so about social media and now
02:17:21.599 about a AI that relate to fundamental
02:17:24.780 privacy rights bias rights intellectual
02:17:28.320 property uh the spread of disinformation
02:17:31.500 which in many ways for me is the biggest
02:17:33.359 threat because that goes to the core of
02:17:35.160 our capacity for self-governing
02:17:37.920 uh there's the economic transformation
02:17:40.459 which can be profound through safety
02:17:44.519 concerns
02:17:45.660 and and I've come to the conclusion that
02:17:48.718 we absolutely have to have an agency
02:17:51.080 what it's
02:17:53.519 scope of Engagement is it has to be
02:17:56.760 defined by us
02:17:58.879 but
02:18:00.478 I believe that unless we have an agency
02:18:02.898 that is going to address these questions
02:18:05.638 from social media in AI
02:18:08.599 we really don't have much of a defense
02:18:11.218 against the bad stuff
02:18:13.138 and the bad stuff will come so uh last
02:18:16.920 year I introduced in the house side and
02:18:19.200 Senator Bennett did incentives it was
02:18:21.120 the end of the year digital commission
02:18:22.740 act and we're going to be reintroducing
02:18:24.599 that this year
02:18:26.218 and the two things that I want to ask
02:18:28.859 one you've somewhat answered because I
02:18:31.138 think two of the three of you have said
02:18:33.000 you think we do need an independent
02:18:34.799 commission you know and Congress
02:18:36.420 establishing into independent commission
02:18:38.638 when railroads were running rampant over
02:18:40.859 the interest of farmers when Wall Street
02:18:43.500 had no rules of the road and we had the
02:18:45.718 SEC and I think we're at that point now
02:18:49.500 but
02:18:51.200 what the commission does would have to
02:18:54.179 be defined and circumscribed but also
02:18:58.200 there's always a question about the use
02:19:00.660 of regulatory Authority
02:19:02.580 and the recognition that it can be used
02:19:05.340 for good JD Vance actually mentioned
02:19:07.679 that when we were considering his and
02:19:09.540 Senator Brown's Bill about railroads in
02:19:12.179 that event in East Palestine
02:19:15.179 regulation for the public health but
02:19:17.580 there's also a legitimate concern about
02:19:19.740 regulation getting in the way of things
02:19:21.478 being too cumbersome
02:19:23.120 and being a negative influence so a
02:19:28.260 two of the three of you have said you
02:19:30.120 think we do need an agency
02:19:32.580 uh
02:19:33.959 what are some of the Perils of an agency
02:19:36.240 that we would have to be mindful of in
02:19:38.638 order to make certain that its goals of
02:19:41.519 protecting many of those interests I
02:19:43.080 just mentioned privacy bias intellectual
02:19:45.299 property disinformation
02:19:47.040 would be the winners and not the losers
02:19:49.620 and I'll start with you Mr Altman
02:19:52.020 thank you Senator uh
02:19:53.939 thank you
02:19:55.020 one I think America has got to continue
02:19:57.240 to lead
02:19:58.280 this happened in America I'm very proud
02:20:00.960 that it happened in America by the way I
02:20:02.820 I think that's right and that's why I'd
02:20:04.560 be much more confident if we had our
02:20:07.740 agency as opposed to got involved in
02:20:10.260 international discussions ultimately you
02:20:12.660 want the rules of the road but I think
02:20:14.100 if we lead and get rules of the road
02:20:15.899 that work for us that is probably a more
02:20:18.600 effective way to proceed I I personally
02:20:21.479 believe there's a way to do both and I
02:20:23.280 think it is important to have the global
02:20:25.200 view on this because this technology
02:20:27.359 will impact Americans and all of us
02:20:30.120 wherever it's developed but I think we
02:20:32.939 want America to lead we want we want so
02:20:35.939 get to the perils issue though because I
02:20:37.620 know well that's one I mean that is
02:20:39.000 apparel which is you slow down American
02:20:40.500 industry in such a way that China or
02:20:42.899 somebody else makes faster progress
02:20:44.819 a second and I think this can happen
02:20:46.920 with like
02:20:48.479 the regulatory pressure should be on us
02:20:50.220 it should be on Google it should be on
02:20:51.899 the other small set of people in the
02:20:53.399 lead the most we don't want to slow down
02:20:55.140 smaller startups we don't want to slow
02:20:56.580 down uh open source efforts we still
02:20:58.560 need them to comply with things they can
02:21:00.359 still you can still cause great harm
02:21:01.800 with a smaller model but leaving the
02:21:05.100 room and the space for new ideas and new
02:21:08.939 companies uh and independent researchers
02:21:11.340 to do their work and not put in a
02:21:13.920 regulatory burden to say a company like
02:21:15.420 us could handle but a smaller one
02:21:16.800 couldn't I think that's another Peril
02:21:19.140 and it's clearly a way that regulation
02:21:20.819 has gone
02:21:21.840 Mr Marcus or Professor Marcus the the
02:21:24.359 other obvious Peril is regulatory
02:21:26.640 capture if we make it as appear as if we
02:21:30.000 are doing something but it's more like
02:21:31.500 green washing and nothing really happens
02:21:34.319 we just keep out the little players
02:21:36.300 because we put so much burden that only
02:21:38.640 the big players can do it so there are
02:21:40.740 also those kinds of perils I fully agree
02:21:42.300 with everything that that Mr Altman said
02:21:44.100 and I would add that to the list
02:21:46.620 James Montgomery
02:21:48.540 one of the things I would add to the
02:21:50.160 list is the risk of not holding
02:21:52.080 companies accountable for the harms that
02:21:54.240 they're causing today right so we talk
02:21:57.180 about misinformation in electoral
02:21:59.220 systems so no agency or no agency we
02:22:03.240 need to hold companies responsible today
02:22:05.340 and accountable for AI that they're
02:22:07.620 deploying that disseminates
02:22:09.899 misinformation on things like elections
02:22:12.479 and where the where the risk you know
02:22:14.340 regulatory agency would do a lot of the
02:22:16.140 things that Senator Graham was talking
02:22:17.520 about you know you don't build a nuclear
02:22:19.260 reactor without getting a license you
02:22:20.939 don't build an AI system without getting
02:22:22.680 a license that gets tested independently
02:22:25.020 I think it's a great analogy we need
02:22:27.180 both pre-deployment pre-deployment and
02:22:29.399 post deployment okay thank you all very
02:22:31.979 much I yield back Mr chairman thanks
02:22:34.319 thanks Senator Welsh let me
02:22:36.240 ask a few more questions you've all been
02:22:38.180 very very patient and the turnout today
02:22:41.479 which is beyond our subcommittee I think
02:22:44.280 reflects both your
02:22:46.620 value in what you're contributing as
02:22:49.740 well as the interest in this topic uh
02:22:52.859 there are a number of subjects that we
02:22:54.960 haven't covered
02:22:56.700 at all but because one was uh just
02:23:01.140 alluded to
02:23:02.700 by
02:23:05.340 Professor Marcus which is the
02:23:07.080 monopolization danger
02:23:09.240 the dominance of markets that excludes
02:23:14.340 new competition and thereby inhibits or
02:23:18.180 prevents Innovation and invention which
02:23:21.540 we have seen in social media as well as
02:23:25.380 some of the old Industries Airlines
02:23:28.819 Automobiles and others where
02:23:30.960 consolidation has narrowed competition
02:23:34.260 and so uh I think we need to to focus on
02:23:40.319 kind of an old area of antitrust which
02:23:43.319 dates more than a century still
02:23:46.040 inadequate to deal with the challenges
02:23:48.060 we have right now in our economy and
02:23:49.979 certainly
02:23:51.180 we need to be mindful of the way that
02:23:53.760 rules can
02:23:55.319 enable the big guys to get bigger and
02:23:57.479 exclude Innovation and competition and
02:23:59.460 responsible
02:24:01.080 good guys
02:24:02.520 such as are represented
02:24:05.760 in this industry right now we haven't
02:24:08.700 dealt with National Security
02:24:10.680 there are huge implications for National
02:24:12.780 Security I will tell you as a member of
02:24:14.280 the armed services committee uh
02:24:16.500 classified briefings on this issue have
02:24:20.040 abounded
02:24:21.479 and the threats that are posed by some
02:24:24.180 of our
02:24:25.220 adversaries China has been mentioned
02:24:27.840 here
02:24:28.800 but the sources of threats to this
02:24:31.979 nation
02:24:32.880 in this space are very real and urgent
02:24:36.060 uh we're not going to deal with them
02:24:37.979 today but we do need to deal with them
02:24:40.319 and we will hopefully in this committee
02:24:43.740 and then uh on the issue of a new agency
02:24:46.560 you know I've been doing this stuff for
02:24:48.540 a while
02:24:49.319 I was Attorney General of Connecticut
02:24:50.939 for 20 years I was a federal prosecutor
02:24:53.640 U.S attorney most of my career has been
02:24:55.979 an enforcement
02:24:57.359 and I will tell you something you can
02:24:58.859 create 10 new agencies
02:25:00.720 but if you don't give them the resources
02:25:03.780 and I'm talking not just about dollars
02:25:05.760 I'm talking about scientific expertise
02:25:09.780 you guys will run circles around them
02:25:12.780 and it isn't just the
02:25:15.479 the models or the generative AI that
02:25:18.300 will run models around run circles
02:25:20.520 around them but it is
02:25:22.800 the scientists in your company
02:25:27.260 for every success story in government
02:25:29.760 regulation you can think of five
02:25:31.800 failures
02:25:32.880 that's true of the FDA it's true of the
02:25:36.080 iaea it's true of the SEC it's true of
02:25:39.540 the whole alphabet list of government
02:25:41.580 agencies and I hope our experience here
02:25:44.700 will be different but the Pandora's Box
02:25:48.780 requires more than just
02:25:52.500 the words or the concepts licensing
02:25:56.160 new agency
02:25:58.140 there's some real hard decision making
02:26:00.000 as as Montgomery has alluded to about
02:26:03.300 how to frame the rules to fit the risk
02:26:09.240 first Do no harm
02:26:12.000 make it effective make it enforceable
02:26:15.120 make it real
02:26:17.040 I think we need to Grapple with
02:26:19.800 the the hard questions here that you
02:26:23.280 know frankly this initial hearing I
02:26:25.620 think is raised very successfully but
02:26:27.899 not answered and I I thank our
02:26:31.080 colleagues who have participated and and
02:26:33.899 made these uh very creative suggestions
02:26:36.899 I'm very interested in uh enforcement I
02:26:41.460 you know literally 15 years ago I think
02:26:43.740 abdicated abolishing section 230.
02:26:46.800 what's old is New Again
02:26:49.380 you know now people are talking about
02:26:52.280 abolishing section 230 back then it was
02:26:55.140 considered completely unrealistic but uh
02:26:59.160 enforcement really does matter I want to
02:27:01.500 ask uh Mr Altman because of the the
02:27:04.859 Privacy issue and you suggested uh that
02:27:08.580 you have an interest in protecting the
02:27:10.859 privacy of the data that may come to you
02:27:14.160 or be available how do you What specific
02:27:16.380 steps you take
02:27:18.260 uh to protect privacy
02:27:23.040 uh one is that we don't train on any
02:27:25.380 data submitted to our API so if you're a
02:27:28.020 business customer of ours and submit
02:27:29.520 data we don't train it at all we do
02:27:31.560 retain it for 30 days solely for the
02:27:33.479 purpose of trust and safety enforcement
02:27:36.180 um but that's different than training
02:27:37.319 and I if you use chat GPT you can opt
02:27:40.319 out of us training on your data you can
02:27:42.840 also delete your conversation history or
02:27:44.340 your whole account
02:27:47.280 um Miss Montgomery I know you don't deal
02:27:49.620 directly with consumers but do you take
02:27:51.899 steps to protect privacy as well
02:27:54.600 absolutely and we even filter our large
02:27:57.720 language models for Content that
02:28:00.180 includes personal information that may
02:28:01.680 have been pulled from public data sets
02:28:03.240 as well so we apply additional level of
02:28:05.340 filtering
02:28:07.560 um
02:28:08.160 Professor Marcus you made reference to
02:28:11.700 self-awareness
02:28:13.680 self-learning uh already we're talking
02:28:16.140 about the potential for jailbreaks
02:28:20.040 um
02:28:21.180 how soon do you think that new kind of
02:28:24.359 generative
02:28:25.560 AI will be
02:28:28.920 usable will be practical
02:28:31.859 new AI that is self-aware and so forth
02:28:34.680 where yes I mean I have no idea on that
02:28:37.740 one I think we don't really understand
02:28:39.000 what self-awareness is and so it's hard
02:28:41.280 to put a date on it in terms of
02:28:43.740 self-improvement there's some modest
02:28:45.300 self-improvement in current systems but
02:28:46.920 one could imagine a lot more and that
02:28:48.960 could happen in two years it could
02:28:50.280 happen in 20 years the basic paradigms
02:28:53.939 that haven't been invented yet some of
02:28:55.800 them we might want to discourage but
02:28:58.140 it's a bit hard to put timelines on them
02:29:00.300 and just going back to enforcement for
02:29:01.740 one second one thing that is absolutely
02:29:03.720 Paramount I think is far greater
02:29:05.760 transparency about what the models are
02:29:08.100 and what the data are that doesn't
02:29:09.359 necessarily mean everybody in the
02:29:10.740 general public has to know exactly
02:29:12.660 what's in one of these systems but I
02:29:14.700 think it means that there needs to be
02:29:15.840 some enforcement arm that can look at
02:29:17.939 these systems can look at the data can
02:29:20.520 perform tests and so forth
02:29:23.280 um let me ask you uh all of you uh I
02:29:27.180 think there has been a reference to
02:29:30.120 um elections and banning
02:29:34.740 um
02:29:35.700 outputs involving elections are there
02:29:38.460 other areas where you think what are the
02:29:40.560 what are the other high risk or highest
02:29:43.260 risk areas where you would either ban or
02:29:48.180 establish especially strict rules
02:29:51.540 is Montgomery
02:29:54.660 the space around misinformation I think
02:29:57.540 is hugely important one and coming back
02:30:00.420 to the points of transparency you know
02:30:02.580 knowing what content was generated by AI
02:30:06.060 is going to be a really critical area
02:30:08.580 that we need to address
02:30:11.700 any others I think medical
02:30:13.680 misinformation is something to really
02:30:15.420 worry about we have systems that
02:30:16.800 hallucinate things they're going to
02:30:18.060 hallucinate medical advice some of the
02:30:19.560 advice they'll give is good some of it's
02:30:20.939 bad we need really tight regulation
02:30:22.920 around that same with psychiatric
02:30:25.160 advice people using these things as as
02:30:27.840 kind of erzot's therapists I think we
02:30:30.300 need to be very concerned about that I
02:30:32.040 think we need to be concerned about
02:30:33.540 internet access for these tools when
02:30:36.060 they can start making requests both of
02:30:38.340 people and and internet things is
02:30:40.200 probably okay if they just do search but
02:30:42.420 as they do more intrusive things on the
02:30:44.580 internet like do we want them to be able
02:30:46.020 to order equipment or order
02:30:48.479 um chemistry and so forth so as they as
02:30:51.420 we Empower these systems more by giving
02:30:53.760 them internet access I think we need to
02:30:55.380 be concerned about that and then we've
02:30:57.060 hardly talked at all about long-term
02:30:58.620 risk Sam alluded to it briefly I don't
02:31:01.260 think that's where we are right now but
02:31:03.000 as we start to approach machines that
02:31:05.280 have a larger footprint on the world
02:31:07.080 Beyond just having a conversation we
02:31:09.479 need to worry about that and think about
02:31:10.920 how we're going to write regulate that
02:31:12.479 and monitor it and so forth in a sense
02:31:15.479 we've been talking about
02:31:17.819 bad guys or
02:31:21.060 um
02:31:21.660 certain Bad actors manipulating
02:31:24.660 AI to do harm manipulating people and
02:31:28.380 manipulating people but also
02:31:30.899 generative AI can manipulate the
02:31:33.540 manipulators
02:31:34.920 it it can I mean there's there's many
02:31:36.840 layers of manipulation that are possible
02:31:38.880 and I think we don't yet really
02:31:40.979 understand the consequences Dan Dennett
02:31:43.260 just sent me a manuscript last night
02:31:45.000 that will be in the Atlantic in a few
02:31:46.500 days on what he calls counterfeit people
02:31:48.620 it's a wonderful metaphor these systems
02:31:51.660 are almost like counterfeit people and
02:31:53.880 we don't really honestly understand what
02:31:56.160 the consequence of that is they're not
02:31:58.439 perfectly human like yet but they're
02:32:00.660 good enough to fool a lot of the people
02:32:02.160 a lot of the time and that introduces
02:32:04.439 lots of problems for example cyber crime
02:32:06.960 and how people might try to manipulate
02:32:08.640 markets and so forth so it's a serious
02:32:10.920 concern in my opening I
02:32:14.460 suggested three principles
02:32:17.040 transparency
02:32:20.040 accountability and limits on use
02:32:24.600 would you agree that those are a good
02:32:27.000 starting point
02:32:28.680 is Montgomery 100 and as you also
02:32:32.220 mentioned industry shouldn't wait for
02:32:34.200 congress that's what we're doing here at
02:32:35.640 IBM there's no reason that absolutely
02:32:37.979 wait for congress yep Professor Marcus I
02:32:41.280 think those three would be a great start
02:32:42.660 I mean there are there are things like
02:32:44.399 the White House Bill of Rights for
02:32:45.840 example that show I think a large
02:32:47.640 consensus the UNESCO guidelines and so
02:32:49.680 forth so a large consensus around what
02:32:51.960 it is we need and the real question is
02:32:53.580 definitely now how are we going to put
02:32:55.500 some teeth in it try to make these
02:32:57.120 things actually enforce so for example
02:32:58.620 we don't have transparency yet we all
02:33:01.080 know we want it but we're not doing
02:33:02.460 enough to enforce it
02:33:04.439 Mr Altman
02:33:05.819 I certainly agree that those are
02:33:07.500 important points I would add that
02:33:10.260 and Professor Marcus touched on this I
02:33:12.180 would add that as we
02:33:13.620 we spend most of the time today on
02:33:15.120 current risks and I think that's
02:33:16.560 appropriate I'm very glad we have done
02:33:18.240 it as these systems do become more
02:33:21.000 capable and I'm not sure how far away
02:33:22.979 that is but maybe not not super far I
02:33:26.160 think it's important that we also spend
02:33:27.479 time talking about how we're going to
02:33:29.100 confront those challenges I mean talk to
02:33:31.439 you privately you know how much I care I
02:33:34.260 that you care deeply and intensely but
02:33:36.960 also
02:33:37.800 that Prospect of increased danger or
02:33:41.880 risk resulting from even more
02:33:45.840 complex and capable
02:33:48.479 AI mechanisms certainly maybe closer
02:33:51.840 than a lot of people appreciate let me
02:33:54.240 just add for the record that I'm sitting
02:33:55.979 next to Sam closer than I've ever sat to
02:33:58.620 him except once before in my life and
02:34:01.319 that his sincerity in talking about
02:34:03.000 those fears is very apparent physically
02:34:05.880 in a way that just doesn't communicate
02:34:08.340 on the television screen thank you
02:34:09.720 indicates from here thank you Senator
02:34:12.359 Hawley thank you again Mr chairman for a
02:34:14.700 great hearing thanks to the witnesses so
02:34:16.020 I've been keeping a little list here of
02:34:17.939 the potential downsides or harms risks
02:34:21.180 of generative AI even in its current
02:34:23.580 form let's just run through it loss of
02:34:25.500 jobs
02:34:26.520 and this isn't speculative I think your
02:34:28.140 company Miss Montgomery has announced
02:34:29.460 that it's it's potentially laying off 7
02:34:31.260 800 people a third of your non-consumer
02:34:33.780 facing Workforce for because of AI so
02:34:36.780 loss of jobs invasion of privacy
02:34:38.580 personal privacy on a scale we've never
02:34:40.740 before seen manipulation of personal
02:34:43.200 Behavior manipulation of personal
02:34:45.420 opinions and potentially the degradation
02:34:47.520 of free elections in America did I miss
02:34:49.080 anything
02:34:49.979 I mean this is this is quite a list
02:34:53.220 I noticed that an Eclectic group of
02:34:55.680 about a thousand technology and AI
02:34:58.140 leaders everybody from Andrew Yang to
02:35:00.960 Elon Musk recently called for a
02:35:03.000 six-month moratorium on any further AI
02:35:06.540 development
02:35:08.220 were they right do you join those calls
02:35:10.200 are they right to do that should we
02:35:11.280 should we pause for six months
02:35:12.840 characterization is not quite correct um
02:35:15.359 I actually signed that letter about 27
02:35:17.640 000 people signed it
02:35:19.319 um it did not call for a ban on all AI
02:35:22.500 research it only caught in nor on all AI
02:35:25.500 but only on a very specific thing which
02:35:28.140 would be systems like gpt5
02:35:30.960 um every other piece of research that's
02:35:33.060 ever been done it was actually
02:35:34.500 supportive or neutral about and
02:35:36.660 specifically called for more AI sorry
02:35:39.359 specifically called for more research on
02:35:41.460 trustworthy and safe AI so you think
02:35:43.800 just so you think that we should take a
02:35:46.200 moratorium a six-month moratorium or
02:35:48.180 more on anything beyond chat gpt4 I took
02:35:51.840 the letter what is the famous phrase uh
02:35:54.319 spiritually not literally what was the
02:35:56.460 famous phrase
02:35:57.660 um well I'm asking for your opinion now
02:35:59.040 though so my opinion is that the
02:36:02.880 moratorium that we should focus on is
02:36:04.560 actually deployment until we have good
02:36:06.240 safety cases I'd don't know that we need
02:36:09.420 to pause that particular project but I
02:36:11.880 do think it's emphasis on focusing more
02:36:14.880 on AI safety on trustworthy reliable AI
02:36:18.000 is exactly right deployment means not
02:36:20.399 making it available to public yeah my
02:36:23.280 concern is about things that are
02:36:25.020 deployed at a scale of let's say 100
02:36:26.460 million people without any external
02:36:28.740 review I think that we should think very
02:36:30.780 carefully about doing that
02:36:32.700 what about you Mr Altman do you agree
02:36:34.319 with that would you would you pause any
02:36:35.880 further development for six months or
02:36:37.439 longer uh so first of all we after we
02:36:40.080 finish training gpt4 we waited more than
02:36:42.060 six months to deploy it um we are not
02:36:44.160 currently training what will be gpt5 we
02:36:46.439 don't have plans to do it in the next
02:36:47.520 six months but I think the frame of the
02:36:50.160 letter is wrong what matters is
02:36:52.920 audits
02:36:54.359 red teaming safety standards that a
02:36:56.880 model needs to pass before training if
02:36:58.620 we pause for six months then I'm not
02:37:00.060 what we sure sure what we do then do we
02:37:01.620 pause for another six do we kind of come
02:37:03.840 up with some rules then the standards
02:37:06.180 that we have developed and that we've
02:37:07.740 used for gpt4 deployment uh we want to
02:37:11.160 build on those but we think that's the
02:37:12.600 right direction uh not a calendar clock
02:37:14.880 pause
02:37:16.680 there may be times I expect there will
02:37:18.479 be times when we find something that we
02:37:20.399 don't understand and we really do need
02:37:21.780 to take a pause but we don't see that
02:37:24.000 yet
02:37:25.560 never mind all the benefits you don't
02:37:28.080 see what you're comfortable with all of
02:37:30.000 the potential ramifications from the
02:37:31.979 current existing technique I'm sorry I
02:37:33.359 don't see the reasons to not train a new
02:37:35.100 one for deploying as I mentioned I think
02:37:36.600 there's all sorts of risky behavior and
02:37:38.280 there's limits we put we have to pull
02:37:40.140 things back sometimes add new ones and
02:37:42.060 then we don't see something that would
02:37:43.020 stop us from training the next model uh
02:37:45.660 where we'd be so worried that we'd
02:37:46.800 create something dangerous even in that
02:37:48.540 process let alone the deployment what
02:37:51.240 about you miss Montgomery
02:37:53.100 I think we need to use the time to
02:37:54.960 prioritize ethics and responsible
02:37:57.060 technology as opposed to posing
02:37:59.100 development
02:38:00.240 well wouldn't a pause and development
02:38:02.220 help the development of protocols for
02:38:04.680 safety standards and ethics I'm not sure
02:38:07.020 how practical it is to pause but we
02:38:09.660 absolutely should be prioritizing safety
02:38:11.880 protocols okay the the point about
02:38:14.100 practicality leads me to this I'm
02:38:15.960 interested in this talk about an agency
02:38:17.760 and you know maybe that would work
02:38:19.500 although
02:38:21.060 having seen how agencies work in this
02:38:23.040 government they usually get captured by
02:38:25.920 the interests that they're supposed to
02:38:27.240 regulate they usually get controlled by
02:38:30.780 the people who they're supposed to be
02:38:32.520 watching I mean that's just been our
02:38:34.020 history for 100 years maybe this agency
02:38:35.460 would be different I have a little
02:38:36.899 different idea why don't we just let
02:38:37.920 people sue you
02:38:39.840 why don't we just make you liable in
02:38:41.760 court we knew that we know how to do
02:38:43.920 that we can pass a statute we can create
02:38:45.780 a federal right of action that will
02:38:47.100 allow private individuals who are harmed
02:38:48.899 by this technology to get into court
02:38:51.180 and to bring evidence into court
02:38:53.399 and it can be anybody I mean we want to
02:38:55.800 talk about crowdsourcing we'll just open
02:38:57.780 the courthouse doors
02:39:00.000 will Define a broad right of action
02:39:01.859 private right of action private citizens
02:39:03.780 be class actions we'll just open it up
02:39:06.300 we'll allow people to go into court
02:39:07.680 we'll allow them present evidence they
02:39:09.000 say that they were harmed by they were
02:39:12.000 given medical misinformation they were
02:39:13.740 given election of misinformation
02:39:15.540 whatever
02:39:16.979 why not do that Mr Altman
02:39:19.680 I mean please forgive my ignorance can't
02:39:21.600 can't people sue us
02:39:23.220 well you're not protection by protected
02:39:25.080 by section 230 but there's not currently
02:39:27.660 a I don't think a federal right of
02:39:30.060 action private right of action that says
02:39:31.979 that if you are harmed by generative AI
02:39:34.200 technology we will guarantee you the
02:39:35.880 ability to get into court oh well I
02:39:37.859 think there's like a lot of other laws
02:39:39.120 where if you know technology harms you
02:39:41.819 uh there's standards that we could be
02:39:43.800 sued under unless I'm really
02:39:44.939 misunderstanding how things work uh if
02:39:47.040 the question is are more are clearer
02:39:50.160 laws about the specifics that this
02:39:52.560 technology and consumer protection is a
02:39:54.960 good thing I would say definitely yes
02:39:57.120 the laws that we have today were
02:39:59.520 designed long before we had artificial
02:40:01.319 intelligence and I do not think they
02:40:03.000 give us enough coverage uh the plan that
02:40:05.340 you propose I think is a hypothetical
02:40:06.899 would certainly make a lot of lawyers
02:40:08.340 wealthy but I think it would be too slow
02:40:10.439 to affect a lot of the things that we
02:40:12.000 care about and there are gaps in the law
02:40:14.040 for example we don't really wait you
02:40:15.600 think it'd be slower than Congress
02:40:17.640 yes I do in some ways really you know
02:40:21.359 litigation can take a decade or more oh
02:40:23.880 I think the threat litigation is a
02:40:25.859 powerful tool I mean how would IBM like
02:40:27.960 to be successful in no way asking to
02:40:30.600 take litigation off the table among the
02:40:32.520 tools but I think for example if I can
02:40:35.160 uh continue
02:40:37.020 um we there are areas like copyright
02:40:38.760 where we don't really have laws we don't
02:40:40.439 really have a way of thinking about
02:40:42.120 wholesale misinformation as opposed to
02:40:44.220 individual pieces of it where say a
02:40:46.560 foreign actor might make billions of
02:40:48.359 pieces of misinformation or a local
02:40:49.859 actor we have some laws around Market
02:40:52.140 manipulation we could apply but we get
02:40:53.939 in a lot of situations where we don't
02:40:55.740 really know which laws apply there would
02:40:57.720 be loopholes this system is really not
02:40:59.819 thought through in fact we don't even
02:41:02.399 know that 230 does or does not apply
02:41:04.920 here as far as I know I think that
02:41:06.420 that's something a lot of people
02:41:07.560 speculated about this afternoon but it's
02:41:09.420 not something we could fix that
02:41:11.479 the question is how
02:41:14.100 oh easy you just you just it would be
02:41:17.640 easy for us to say that section 230
02:41:19.439 doesn't apply to generative AI
02:41:23.819 um Miss Montgomery a duty of care
02:41:27.240 which I think fits the idea of a private
02:41:30.359 right of action now that's exactly right
02:41:32.340 and also AI is not a shield right so so
02:41:35.160 if a company discriminates in granting
02:41:38.760 credit for example or in the hiring
02:41:41.340 process the by virtue of the fact that
02:41:44.520 they relied too significantly on an AI
02:41:46.979 tool they're responsible for that today
02:41:49.080 regardless of whether they used a tool
02:41:51.000 or a human to make that decision
02:41:53.700 I'm going to turn to Senator Booker for
02:41:56.760 some final questions but I just uh want
02:41:59.580 to make a quick Point here on the on the
02:42:02.100 issue of the moratorium
02:42:04.020 I think we need to be careful The World
02:42:05.939 Won't Wait
02:42:07.260 the rest of
02:42:08.720 the global scientific Community isn't
02:42:11.640 going to pause we have adversaries that
02:42:13.859 are moving ahead
02:42:15.240 and sticking our head in the sand is not
02:42:18.060 the answer safeguards and protections
02:42:21.780 yes
02:42:23.640 but a flat stop sign sticking our head
02:42:28.260 in the sand
02:42:29.580 I would be very very worried without
02:42:32.640 militating for any sort of pause I would
02:42:35.760 just again emphasize there is a
02:42:37.140 difference between research which surely
02:42:39.420 we need to do to keep Pace with our
02:42:41.340 foreign Rivals and deployment at really
02:42:43.800 massive scale you know you could deploy
02:42:46.439 things at a scale of a million people or
02:42:48.300 10 million people but not 100 million
02:42:50.220 people or a billion people and if there
02:42:52.080 are risks you might find them out sooner
02:42:53.760 and be able to close the barn doors
02:42:56.280 before the horses leave rather than
02:42:57.899 after
02:42:58.920 Senator Booker yeah I I just there will
02:43:01.560 be no pause I mean there's no
02:43:03.240 enforcement body to force appall it's
02:43:04.920 just not not going to happen it's nice
02:43:06.420 to call for it for any just reasons or
02:43:09.180 whatsoever but I'm just forgive me for
02:43:11.520 sounding skeptical nobody's pausing this
02:43:13.260 thing is crazy
02:43:14.660 I would agree and I don't think it's a
02:43:17.220 realistic thing in the world the reason
02:43:18.540 I personally signed the letter was to
02:43:20.220 call attention to how serious the
02:43:22.080 problems were and to emphasize spending
02:43:24.120 more of our efforts on trustworthy and
02:43:26.040 safe AI rather than just making a bigger
02:43:28.620 version of something we already know to
02:43:30.060 be unreliable yeah so I'm I'm uh
02:43:33.120 futurist I love the exciting about the
02:43:35.580 future and I guess there's a famous
02:43:37.040 question uh if you couldn't control for
02:43:39.720 your race your gender where you would
02:43:41.520 land on the planet Earth or what time in
02:43:43.080 humanity would you want to be born
02:43:44.280 everyone would say right now it's the
02:43:46.740 it's still the best time to be alive
02:43:48.840 because of Technology Innovation and
02:43:51.120 everything and I'm excited about what
02:43:52.979 the future holds but the destructiveness
02:43:55.319 that I've also seen as a person that's
02:43:57.120 seen the transformative Technologies
02:43:59.100 of uh of of of a lot of the Technologies
02:44:02.760 of the last 25 years is is what really
02:44:05.100 concerns me and one of the things
02:44:07.319 especially with
02:44:09.479 um companies that are designed to want
02:44:11.340 to keep my attention on screens and I'm
02:44:14.100 not just talking about New Media a
02:44:16.080 24-hour cable news is a great example of
02:44:18.660 people that want to keep your eyes on
02:44:20.819 screens I have a lot of concerns about
02:44:24.120 the corporate intention and and Sam this
02:44:26.880 is again why I find your story so
02:44:28.979 fascinating to me and your values that I
02:44:31.740 believe in from our conversations so
02:44:34.620 compelling to me but but perhaps in that
02:44:37.620 I really want to just explore what
02:44:40.200 happens when these companies that are
02:44:42.899 already controlling so much of our lives
02:44:44.580 a lot has been written about the Fang
02:44:46.439 companies what happens when they are the
02:44:49.319 ones that are dominating this technology
02:44:51.720 as they did before so Professor Marcus
02:44:54.000 does that have any concern the role that
02:44:57.359 corporate power corporate concentrate
02:44:59.040 nutrition has in this realm that a few
02:45:02.220 companies might might control this whole
02:45:03.960 area I radically change the shape of my
02:45:06.780 own life in the last few months and it
02:45:08.819 was because of what happened with
02:45:10.319 Microsoft releasing Sydney and it didn't
02:45:13.080 go the way I thought it would in one way
02:45:15.000 it did which as I anticipated the
02:45:17.040 hallucinations I wrote an essay which I
02:45:19.319 have in the appendix What to Expect When
02:45:20.819 You're Expecting gpt4 and I said that it
02:45:23.280 would still be a good tool for
02:45:24.840 misinformation that it would still have
02:45:26.220 trouble with physical reasoning
02:45:27.420 psychological reasoning that would
02:45:28.740 hallucinate and then Along Came Sydney
02:45:30.899 and the initial press reports were quite
02:45:32.760 favorable and then there was the famous
02:45:34.800 article by Kevin Roos in which it
02:45:36.660 recommended he get a divorce and I had
02:45:39.479 seen Tay and I had seen Galactica for
02:45:41.939 meta and those had been pulled after
02:45:43.800 they had problems and Sydney clearly had
02:45:46.500 problems what I would have done had I
02:45:48.540 run Microsoft which clearly I do not
02:45:50.340 would have been to temporarily withdraw
02:45:52.500 it from the market and they didn't and
02:45:55.020 that was a wake-up call to me and a
02:45:56.760 reminder that even if you have a company
02:45:58.680 like open AI that is a non-profit and
02:46:01.380 Sam's values I think have come clear
02:46:03.420 today other people can buy those
02:46:05.760 companies and do what they like with
02:46:07.140 them and you know maybe we have a stable
02:46:09.420 set of actors now but the amount of
02:46:11.819 power that these systems have to shape
02:46:14.160 our views and our lives is really really
02:46:17.100 significant and that doesn't even get
02:46:18.600 into the risks that someone might
02:46:20.340 repurpose them deliberately for all
02:46:21.960 kinds of bad purposes and so in the
02:46:24.479 middle of February I stopped writing
02:46:26.700 much about technical issues in AI which
02:46:28.979 is most of what I've written about for
02:46:30.660 the last decade and said I need to work
02:46:32.460 on policy this is frightening
02:46:34.920 Sam I want to give you an opportunity
02:46:36.840 it's my sort of last
02:46:38.720 a question or so
02:46:41.420 don't you have concerns about I mean you
02:46:44.100 you I I graduated from Stanford the I
02:46:47.160 know so many of the players in the
02:46:48.359 valley uh from VC people folks Angel
02:46:50.700 folks to a lot of founders of companies
02:46:52.740 that we all know do you have some
02:46:55.500 concern about about a few players with
02:46:58.260 with extraordinary resources and power
02:47:01.020 power to influence Washington I mean I I
02:47:03.479 see us I love I'm a big believer in the
02:47:05.880 free market but the reason why I walk
02:47:08.220 into a bodega and a Twinkie is cheaper
02:47:10.560 than an apple or a Happy Meal costs less
02:47:13.200 than a bucket of salad is because of the
02:47:15.660 way the government tips the scales to
02:47:17.760 pick winners and losers so the free
02:47:19.560 market is is is not what it should be
02:47:21.479 when you have large corporate power that
02:47:23.460 can even influence the game here do you
02:47:25.020 have some concerns about that in this in
02:47:27.600 this next era of technological
02:47:29.520 innovation yeah I mean again that's
02:47:31.680 that's so much of why we started open AI
02:47:33.420 we have huge concerns about that uh I
02:47:35.880 think it's important to democratize the
02:47:38.580 inputs to these systems the values that
02:47:40.319 we're going to align to
02:47:42.240 um and I think it's also important to
02:47:43.800 give people wide use of these tools when
02:47:45.540 we started the API strategy uh which is
02:47:47.939 a big part of how we make our systems
02:47:49.380 available for anyone to use there was a
02:47:51.420 huge amount of skepticism over that and
02:47:52.859 it does come with challenges that's for
02:47:54.420 sure but we think putting this in the
02:47:57.120 hands of a lot of people and not in the
02:47:58.979 hands of a few companies uh is really
02:48:02.280 quite important and we are seeing the
02:48:03.720 resultant Innovation boom from that
02:48:06.240 um but but it is absolutely true that
02:48:08.939 the number of companies that can train
02:48:10.439 the true Frontier models is going to be
02:48:13.439 small just because of the resources
02:48:14.939 required and so I think there needs to
02:48:16.500 be incredible scrutiny on us and our
02:48:18.180 competitors uh I think there is a rich
02:48:21.720 and exciting industry happening of
02:48:23.520 incredibly good research and new
02:48:25.020 startups that are not just using our
02:48:26.640 models but creating their own and I
02:48:28.380 think it's important to make sure that
02:48:29.880 whatever regulatory stuff happens
02:48:31.380 whatever new agencies may or may not
02:48:32.760 happen we we preserve that fire because
02:48:35.220 that's critical I'm a big believer in
02:48:37.740 the democratizing potential of
02:48:39.359 Technology but I've seen the promise of
02:48:42.240 that fail time and time again uh where
02:48:45.780 people say oh this is going to have a
02:48:46.920 big democratizing force my team works on
02:48:49.439 a lot of issues about the reinforcing of
02:48:52.020 of of bias through algorithms the
02:48:55.020 failure to advertise certain
02:48:56.640 opportunities in certain zip codes
02:48:59.580 um but you seem to be saying and I heard
02:49:01.560 this with web3 that this is going to be
02:49:03.420 die fide uh decentralized Finance all
02:49:05.939 these things are going to happen
02:49:07.500 but this is not this seems to me not
02:49:09.600 even to offer that promise because the
02:49:11.939 people who are designing these it takes
02:49:13.500 so much power energy resources are you
02:49:17.280 saying that this that my dreams of
02:49:19.560 Technology further democratizing
02:49:21.300 opportunity and more
02:49:23.460 um are are possible within a technology
02:49:26.520 that is ultimately I think going to be
02:49:28.380 very centralized to a few players who
02:49:30.600 already control so much
02:49:32.880 so
02:49:34.280 this point that I made about use of use
02:49:37.140 of the model and building on top of it
02:49:38.700 as a this is really a new platform right
02:49:40.859 it is definitely important to talk about
02:49:42.540 who's going to create the models I want
02:49:44.399 to do that I also think it's really
02:49:45.840 important to decide to whose values
02:49:47.760 we're going to align these models but in
02:49:49.979 terms of using the models the people
02:49:52.500 that build on top of the open AI API do
02:49:54.960 incredible things and it's you know
02:49:57.479 people frequently comment like I can't
02:49:59.460 believe you get this much technology for
02:50:00.960 this little money and so what people are
02:50:03.180 the companies people are building
02:50:04.439 putting AI everywhere using our API
02:50:07.020 which does let us put safeguards in
02:50:08.819 place uh I think that's quite exciting
02:50:11.520 and I think that is how it is being
02:50:12.960 democracy not not how it's going to be
02:50:14.460 but how it is being democratized right
02:50:16.020 now there is a whole new Cambrian
02:50:18.479 explosion of new businesses new products
02:50:21.180 new Services Happening by lots of
02:50:23.580 different companies on top of these
02:50:24.600 models so I'll say chairman as I close
02:50:26.220 that
02:50:27.359 I have most Industries resist even
02:50:30.000 reasonable regulation from seat belt
02:50:31.979 laws to we've been talking a lot
02:50:34.260 recently about rail safety that the only
02:50:36.960 way we're going to see the
02:50:37.859 democratization of values I think and
02:50:40.140 while there are noble companies out
02:50:41.760 there is if we create rules of the road
02:50:43.800 that enforce certain safety measures
02:50:46.560 like we've seen with other technology
02:50:47.880 thank you
02:50:49.560 thanks Senator Booker uh and I I
02:50:52.380 couldn't agree more that
02:50:54.660 um in terms of consumer protection which
02:50:57.120 I've been doing for a while uh
02:51:00.240 participation by the industry is
02:51:02.840 tremendously important and not just
02:51:05.600 rhetorically but in real terms because
02:51:08.280 we have a lot of industries that come
02:51:10.319 before us and say oh we're all in favor
02:51:12.479 of rules but not those rules those rules
02:51:15.540 we don't like and it's every rule in
02:51:19.740 fact that they don't like and I sense
02:51:22.200 that there is a willingness to
02:51:23.520 participate here that is genuine and
02:51:26.640 authentic I thought about asking chat
02:51:31.080 GPT to
02:51:32.760 do a new version of don't stop thinking
02:51:35.340 about tomorrow
02:51:36.859 because that's what we need to be doing
02:51:39.359 here and as Senator Hawley has pointed
02:51:42.000 out Congress doesn't always move at the
02:51:45.240 pace of technology and that may be a
02:51:48.540 reason why we need a new agency but we
02:51:51.359 also need to recognize the rest of the
02:51:53.460 world is going to be moving as well and
02:51:56.040 you've been enormously helpful in
02:51:59.460 focusing us and Illuminating some of
02:52:01.800 these questions and performed a great
02:52:03.660 service by being here today so thank you
02:52:06.180 to everyone of our
02:52:08.939 Witnesses and uh
02:52:11.760 I'm going to close the hearing leave the
02:52:14.580 record open for one week in case anyone
02:52:17.100 wants to submit anything I encourage any
02:52:18.960 of you who have either manuscripts that
02:52:22.319 are going to be published or
02:52:23.779 observations uh from your companies uh
02:52:27.240 to to submit them to us and we look
02:52:30.660 forward to our next hearing this one is
02:52:32.880 closed
02:52:51.540 um
02:53:05.720 I'm here all week if you have any time
02:53:07.920 to talk
02:53:11.220 this is
02:53:16.920 um
02:53:32.359 I'm here all week if you need me
02:53:35.700 you want to have your staff get in touch
02:53:37.260 with me I'm here all week Bloom involved
02:53:39.960 people have been eye contact great great
02:53:42.660 I'm delighted to talk
02:53:48.720 to your plan
02:53:52.819 my dad was marketing management
02:53:58.880 thank you very much so thank you for
02:54:01.319 including me in this story I'd love to
02:54:03.240 get together again your people know me
02:54:05.460 I'm here all week if you want to talk
02:54:06.779 about someone okay I was here until
02:54:08.700 Saturday wonderful things thank you
02:54:10.460 we'll see you at five yeah
02:54:22.100 thank you
02:54:30.240 for the kids
02:54:31.620 oh yeah
02:54:49.260 oh
02:54:54.359 thank you
02:54:58.560 thank you take care
02:55:17.340 of those topics
02:55:59.359 during those days
02:56:02.640 foreign
